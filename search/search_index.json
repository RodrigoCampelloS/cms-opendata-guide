{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CMS Open Data Guide \u00b6 Warning This page is under construction Welcome to the official guide for CMS open data. This page is still under construction. We appreciate your feedback and/or your help building this guide. How to use this site \u00b6 There are three main tabs to help you navigate the site. It starts with the Computing Tools most likely needed to deal with CMS open data. Then, there is a little review of CMSSW , which is the software used by CMS. Finally the Analysis section guides you through the different steps (in the most general order) that you need to follow for performing a particle physics analysis with CMS open data. The site's philosophy \u00b6 This site is thought as a navigation aid. The CMS collaboration has built an extensive amount of documentation over the years. However, given the nature of our rapidly evolving research activities, this documentation is usually scattered around, which makes it difficult to navigate. The main goal of this guide, therefore, is to facilitate the usage of CMS open/legacy data by providing a structured set of instructions that agglutinate those pieces of information already available in other sites. In this sense, we do not pretend to copy every little piece of information and/or code, but to help you get to it and find your way around it. For CMS open data the three main sources of documentation/information are: The CMS public Twiki pages . Particularly the workbook and the software guide Note When accessing the CMS twiki pages we will usually point you to the most recent page. However, historical Twiki documentation, i.e., earlier revision of the pages, may provide more accurate information for open data that is already a few years old. One can access this historical archive by going to the bottom of any Twiki page, clicking on History and exploring the revisions closer to the open data release year. The CERN CMS Open Portal pages. This portal is not exactly meant to archive documentation. It is mainly a repository for our open data. However, it does host important information that is not so easy to find. This guide will point you to the right pages. The CMSSW code . Although less conventional, exploring the CMSSW code could be a really good source of information. For instance, having hundreds of trigger bits, if the information from a specific module used in a specific trigger (with which data was taken) was needed, it would be impossible to document that explicitly in some guide. Instead, one can explore the code and easily find out the needed information. We will try to show you how it is done. How to contribute or contact us \u00b6 Please follow these instructions if you would like to contribute. If you find bugs or have suggestions or recommendations to improve this guide, please fill out an issue or contact us .","title":"Home"},{"location":"#cms-open-data-guide","text":"Warning This page is under construction Welcome to the official guide for CMS open data. This page is still under construction. We appreciate your feedback and/or your help building this guide.","title":"CMS Open Data Guide"},{"location":"#how-to-use-this-site","text":"There are three main tabs to help you navigate the site. It starts with the Computing Tools most likely needed to deal with CMS open data. Then, there is a little review of CMSSW , which is the software used by CMS. Finally the Analysis section guides you through the different steps (in the most general order) that you need to follow for performing a particle physics analysis with CMS open data.","title":"How to use this site"},{"location":"#the-sites-philosophy","text":"This site is thought as a navigation aid. The CMS collaboration has built an extensive amount of documentation over the years. However, given the nature of our rapidly evolving research activities, this documentation is usually scattered around, which makes it difficult to navigate. The main goal of this guide, therefore, is to facilitate the usage of CMS open/legacy data by providing a structured set of instructions that agglutinate those pieces of information already available in other sites. In this sense, we do not pretend to copy every little piece of information and/or code, but to help you get to it and find your way around it. For CMS open data the three main sources of documentation/information are: The CMS public Twiki pages . Particularly the workbook and the software guide Note When accessing the CMS twiki pages we will usually point you to the most recent page. However, historical Twiki documentation, i.e., earlier revision of the pages, may provide more accurate information for open data that is already a few years old. One can access this historical archive by going to the bottom of any Twiki page, clicking on History and exploring the revisions closer to the open data release year. The CERN CMS Open Portal pages. This portal is not exactly meant to archive documentation. It is mainly a repository for our open data. However, it does host important information that is not so easy to find. This guide will point you to the right pages. The CMSSW code . Although less conventional, exploring the CMSSW code could be a really good source of information. For instance, having hundreds of trigger bits, if the information from a specific module used in a specific trigger (with which data was taken) was needed, it would be impossible to document that explicitly in some guide. Instead, one can explore the code and easily find out the needed information. We will try to show you how it is done.","title":"The site's philosophy"},{"location":"#how-to-contribute-or-contact-us","text":"Please follow these instructions if you would like to contribute. If you find bugs or have suggestions or recommendations to improve this guide, please fill out an issue or contact us .","title":"How to contribute or contact us"},{"location":"about/","text":"About \u00b6 This is the offcial guide for CMS open data. All CMS instructional material is made available under the Creative Commons Attribution license . Contributors \u00b6 Matt Bellis Edgar Carrera Kati Lassila-Perini Tibor \u0160imko Marco Vidal Garc\u00eda Audrius Mecionis Allan Jales Contact \u00b6 cms-data-preservation@[Eraseme]cern.ch","title":"About"},{"location":"about/#about","text":"This is the offcial guide for CMS open data. All CMS instructional material is made available under the Creative Commons Attribution license .","title":"About"},{"location":"about/#contributors","text":"Matt Bellis Edgar Carrera Kati Lassila-Perini Tibor \u0160imko Marco Vidal Garc\u00eda Audrius Mecionis Allan Jales","title":"Contributors"},{"location":"about/#contact","text":"cms-data-preservation@[Eraseme]cern.ch","title":"Contact"},{"location":"faq/","text":"FAQ \u00b6 Warning This page is under construction Frequently Asked Questions and other problems and issues that have come up. Possible subsections below High-level questions \u00b6 Why would I choose VirtualBox over docker? Why would I choose docker over VirtualBox? \u00b6 Great question! Anyone? Docker \u00b6 Docker downloads container but never launches environment \u00b6 This is an issue with newer OSs on your local laptop/desktop running older OSs in the container. For example, suppose you are following the Running CMS analysis code using Docker tutorial. If you run docker run --name opendata -it cmsopendata/cmssw_5_3_32 /bin/bash and the container downloads but you don't find yourself in the CMSSW_5_3_32 environment, then... Data \u00b6 CMSSW \u00b6","title":"FAQ"},{"location":"faq/#faq","text":"Warning This page is under construction Frequently Asked Questions and other problems and issues that have come up. Possible subsections below","title":"FAQ"},{"location":"faq/#high-level-questions","text":"","title":"High-level questions"},{"location":"faq/#why-would-i-choose-virtualbox-over-docker-why-would-i-choose-docker-over-virtualbox","text":"Great question! Anyone?","title":"Why would I choose VirtualBox over docker? Why would I choose docker over VirtualBox?"},{"location":"faq/#docker","text":"","title":"Docker"},{"location":"faq/#docker-downloads-container-but-never-launches-environment","text":"This is an issue with newer OSs on your local laptop/desktop running older OSs in the container. For example, suppose you are following the Running CMS analysis code using Docker tutorial. If you run docker run --name opendata -it cmsopendata/cmssw_5_3_32 /bin/bash and the container downloads but you don't find yourself in the CMSSW_5_3_32 environment, then...","title":"Docker downloads container but never launches environment"},{"location":"faq/#data","text":"","title":"Data"},{"location":"faq/#cmssw","text":"","title":"CMSSW"},{"location":"analysis/backgrounds/qcdestimation/","text":"QCD Estimation \u00b6 Warning This page is under construction","title":"QCD Estimation"},{"location":"analysis/backgrounds/qcdestimation/#qcd-estimation","text":"Warning This page is under construction","title":"QCD Estimation"},{"location":"analysis/backgrounds/techniques/","text":"Techniques \u00b6 Warning This page is under construction","title":"Techniques"},{"location":"analysis/backgrounds/techniques/#techniques","text":"Warning This page is under construction","title":"Techniques"},{"location":"analysis/datasim/collisiondata/","text":"Collision Data \u00b6 Warning This page is under construction The CMS collision data is organized in primary datasets (PD). All CMS open data primary datasets can be found with this search . The dataset name consists of three parts separated by \"/\", e.g.: /TauPlusX/Run2011A-12Oct2013-v1/AOD The first part indicates the primary dataset contents ( TauPlusX ), the second part is the data-taking era ( Run2011A ) and reprocessing ( 12Oct2013 ), and the last one indicates the data format ( AOD ). Dataset contents \u00b6 The primary dataset definition is centered around physics objects (SingleMu, Jet, Tau etc). Events triggered by High Level Triggers (HLT) with a similar physics contents or use are mostly directed in the same PD. This guide gives an overview of the CMS trigger system. Besides requirements on the physics content, the organisation of the primary datasets has to satisfy constraints related to the data processing and handling, such as the average event rate approximately uniform across the different PDs, and the event rate more than 10 Hz and less than 200 Hz. (relevant?) Each CMS collision dataset comes with a brief description of the contents, and the full listing of all possible HLT trigger streams included in the dataset. The instructions how to find the exact definitions and parameters of the HLT trigger definitions can be found in Guide to the CMS Trigger System under \" HLT Trigger Path definitions \". Since a given event can pass more than one HLT path, it can be included in more than one primary dataset. There's an overall overlap between the PDs of around 25-35% during Run1 and it must be taken into account when combining events from different datasets in an analysis. Data taking and reprocessing \u00b6 One year of data taking is divided in several \"eras\" indicated as RunA, RunB, etc. According to the CMS data policy, 50% of data is published after the embargo period, completed with the full release within 10 years. Currently available are Run2010A and Run2010B Run2011A Run2012B and Run2012C The data are reprocessed several times, and it is the last complete reprocessing available at the time of the release which is made public. Data format \u00b6 The data format in use for Run1 data is Analysis Object Data (AOD). A brief description of data formats can be found in the introductory About CMS under \" Primary and simulated datasets \". Else (FIXME) \u00b6 To consider: mention json files for validated runs/LS mention condition data and GT Integrated luminosity here or in a separate chapter? Refs G. Franzoni: Dataset definition for CMS operations and physics analyses CR2014_311.pdf","title":"Collision Data"},{"location":"analysis/datasim/collisiondata/#collision-data","text":"Warning This page is under construction The CMS collision data is organized in primary datasets (PD). All CMS open data primary datasets can be found with this search . The dataset name consists of three parts separated by \"/\", e.g.: /TauPlusX/Run2011A-12Oct2013-v1/AOD The first part indicates the primary dataset contents ( TauPlusX ), the second part is the data-taking era ( Run2011A ) and reprocessing ( 12Oct2013 ), and the last one indicates the data format ( AOD ).","title":"Collision Data"},{"location":"analysis/datasim/collisiondata/#dataset-contents","text":"The primary dataset definition is centered around physics objects (SingleMu, Jet, Tau etc). Events triggered by High Level Triggers (HLT) with a similar physics contents or use are mostly directed in the same PD. This guide gives an overview of the CMS trigger system. Besides requirements on the physics content, the organisation of the primary datasets has to satisfy constraints related to the data processing and handling, such as the average event rate approximately uniform across the different PDs, and the event rate more than 10 Hz and less than 200 Hz. (relevant?) Each CMS collision dataset comes with a brief description of the contents, and the full listing of all possible HLT trigger streams included in the dataset. The instructions how to find the exact definitions and parameters of the HLT trigger definitions can be found in Guide to the CMS Trigger System under \" HLT Trigger Path definitions \". Since a given event can pass more than one HLT path, it can be included in more than one primary dataset. There's an overall overlap between the PDs of around 25-35% during Run1 and it must be taken into account when combining events from different datasets in an analysis.","title":"Dataset contents"},{"location":"analysis/datasim/collisiondata/#data-taking-and-reprocessing","text":"One year of data taking is divided in several \"eras\" indicated as RunA, RunB, etc. According to the CMS data policy, 50% of data is published after the embargo period, completed with the full release within 10 years. Currently available are Run2010A and Run2010B Run2011A Run2012B and Run2012C The data are reprocessed several times, and it is the last complete reprocessing available at the time of the release which is made public.","title":"Data taking and reprocessing"},{"location":"analysis/datasim/collisiondata/#data-format","text":"The data format in use for Run1 data is Analysis Object Data (AOD). A brief description of data formats can be found in the introductory About CMS under \" Primary and simulated datasets \".","title":"Data format"},{"location":"analysis/datasim/collisiondata/#else-fixme","text":"To consider: mention json files for validated runs/LS mention condition data and GT Integrated luminosity here or in a separate chapter? Refs G. Franzoni: Dataset definition for CMS operations and physics analyses CR2014_311.pdf","title":"Else (FIXME)"},{"location":"analysis/datasim/eventgeneration/","text":"Event Generation \u00b6 Warning This page is under construction Physical event generation and detector simulation are the first steps in producing Monte Carlo samples suitable for physical analysis. Here we will teach you how to use the CMS datasets in the CERN Open Data Portal and the CMSSW machinery for the generation of events in simple steps: Generation and Simulation: To simulate beam collisions. Triggers: To simulate the effect of the detectors and electronics. Reconstruction: For the reconstruction of the events in the collisions. What you will find here: Virtual machines Dataset name System details Configuration files cmsDriver Generation from Matrix Element (ME) generators LHE Simulation High Level Trigger (HLT) Reconstruction Generation from general-purpose generators Generation and Simulation High Level Trigger (HLT) Reconstruction Example for event generation with 2011 CMSSW machinery Example for event generation with 2012 CMSSW machinery Virtual machines \u00b6 A specific CMS virtual machine includes the ROOT framework and CMSSW. Follow these instructions to configure a CERN virtual machine on your computer to be used with the 2011 and 2012 CMS open data. Dataset name \u00b6 When exploring a simulated dataset on the CERN Open Data Portal , the first thing you will see is the name of the dataset. CMS uses the following naming convention : PROCESS_RANGETYPE-RANGELOWtoRANGEHIGH_FILTER_TUNE_COMMENT_COMENERGY-GENERATOR Take as an example the name of record 12201 : QCD_Pt-15to3000_TuneZ2star_Flat_8TeV_pythia6 System details \u00b6 In the record of each dataset, you can find the recommended global tag and release for analysis (CMSSW is the data analysis library). A global tag stores additional data that is required by the reconstruction and analysis software. Take as an example section System details of record 12201 : Recommended global tag for analysis: START53_V27 Recommended release for analysis: CMSSW_5_3_32 Configuration files \u00b6 The CMS software framework uses a software bus model, where data is stored in the event which is passed to a series of modules. A single executable, cmsRun , is used, and the modules are loaded at runtime. A configuration file defines which modules are loaded, in which order they are run, and with which configurable parameters they are run. You can find the configuration files for the generation of events for each dataset in its respective record within the CERN Open Data Portal . Check, for example, the section How were these data generated? of record 12201 . cmsDriver \u00b6 The cmsDriver is a tool to create production-solid configuration files from minimal command line options. Its code implementation, the cmsDriver.py script, is part of the CMSSW software. A summary of the cmsDriver.py script's options with a detailed message about each one can be visualized by getting the help: cmsDriver.py --help Generation from Matrix Element (ME) generators \u00b6 Generator-level datasets can be produced using a Matrix Element (ME) generator (e.g., Powheg , MadGraph5_aMCatNLO , Alpgen ) to deliver the event at the parton level and then a general-purpose generator to hadronise the event. Here we will reproduce the steps in the generation of record 1352 . Guided by the system details specified in the dataset, you should start by setting up your run time environment: cmsrel CMSSW_5_3_32 cd CMSSW_5_3_32/src/ cmsenv We will create a package according to our dataset: mkdir MyPackage cd MyPackage mkedanlzr MySim LHE \u00b6 The Les Houches Event file format ( LHE ) is an agreement between Monte Carlo event generators and theorists to define Matrix Element level event listings in a common language. The LHE input file that store process and event information can be one generated by you or you can look for examples in /eos/cms/store/lhe/ . Here we will use a file with events generated for record 1352 : cmsDriver.py step1 --filein lhe:10270 --fileout file:LHE.root --mc --eventcontent LHE --datatier GEN --conditions START53_LV6A1::All --step NONE --python_filename LHE.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Run the CMSSW executable: cmsRun LHE.py Simulation \u00b6 The next step is to generate fully hadronised events. We need to use the appropriate configuration file for this purpose. Take as an example the file in Step SIM for the simulation of record 1352 . The configuration file is in this link . We add this file to our local area: curl http://uaf-10.t2.ucsd.edu/~phchang/analysis/generator/genproductions/python/SevenTeV/Hadronizer_TuneZ2_7TeV_generic_LHE_pythia_tauola_cff.py -o MySim/python/mysim.py Compile everything: scram b Execute the cmsDriver command as: cmsDriver.py MyPackage/MySim/python/mysim.py --filein file:LHE.root --fileout file:sim.root --mc --eventcontent RAWSIM --customise SimG4Core/Application/reproc2011_2012_cff.customiseG4,Configuration/DataProcessing/Utils.addMonitoring --datatier GEN-SIM --conditions START53_LV6A1::All --beamspot Realistic7TeV2011CollisionV2 --step GEN,SIM --datamix NODATAMIXER --python_filename sim.py --no_exec -n 3 Run the CMSSW executable: cmsRun sim.py High Level Trigger (HLT) \u00b6 It is a crucial part of the CMS data flow since it is the HLT algorithms and filters which will decide whether an event should be kept for an offline analysis: any offline analysis depends on the outcome of HLT. Execute the cmsDriver command as: cmsDriver.py step1 --filein file:sim.root --fileout file:hlt.root --mc --eventcontent RAWSIM --runsScenarioForMC Run2012_AB_C_D_oneRunPerEra --datatier GEN-RAW --conditions START53_LV6A1::All --step DIGI,L1,DIGI2RAW,HLT:2011 --python_filename hlt.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Now, run the CMSSW executable: cmsRun hlt.py Reconstruction \u00b6 The algorithms that make up the CMS event reconstruction software build physics objects (e.g., muons, electrons, jets) from the raw data recorded by the detector. All events collected by the CMS trigger system are reconstructed by the CMS prompt reconstruction system soon after being collected. Execute the cmsDriver command as: cmsDriver.py step2 --filein file:hlt.root --fileout file:reco.root --mc --eventcontent AODSIM,DQM --datatier AODSIM,DQM --conditions START53_LV6A1::All --step RAW2DIGI,L1Reco,RECO,VALIDATION:validation_prod,DQM:DQMOfflinePOGMC --python_filename reco.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Now, run the CMSSW executable: cmsRun reco.py You can start ROOT and type TBrowser t to explore the files that were created. Generation from general-purpose generators \u00b6 Generator-level datasets can be produced using a general-purpose generator (e.g., Pythia , Herwig , Tauola ) to simulate the event and the hadronisation. Here we will reproduce the steps in the generation of record 12201 . Guided by the system details specified in the dataset, you should start by setting up your run time environment: cmsrel CMSSW_5_3_32 cd CMSSW_5_3_32/src/ cmsenv We will create a package according to our dataset: mkdir MyPackage cd MyPackage mkedanlzr MyGen Generation and Simulation \u00b6 We need to use the appropriate configuration file. Take as an example the file in Step SIM for the generation and simulation of record 12201 . The configuration file is in this link . We add this file to our local area: curl https://raw.githubusercontent.com/cms-sw/genproductions/master/python/EightTeV/QCD_Pt/QCD_Pt_15to3000_TuneZ2star_Flat_8TeV_pythia6_cff.py -o MyGen/python/mygen.py Compile everything: scram b Execute the cmsDriver command as: cmsDriver.py MyPackage/MyGen/python/mygen.py --fileout file:gen.root --mc --eventcontent RAWSIM --pileup NoPileUp --customise Configuration/StandardSequences/SimWithCastor_cff.customise,Configuration/DataProcessing/Utils.addMonitoring --datatier GEN-SIM --conditions START50_V13::All --beamspot Realistic8TeVCollision --step GEN,SIM --datamix NODATAMIXER --python_filename gen.py --no_exec -n 3 Run the CMSSW executable: cmsRun gen.py High Level Trigger (HLT) \u00b6 Execute the cmsDriver command as: cmsDriver.py step1 --filein file:gen.root --fileout file:hlt.root --pileup_input dbs:/MinBias_TuneZ2star_8TeV-pythia6/Summer12-START50_V13-v3/GEN-SIM --mc --eventcontent RAWSIM --runsScenarioForMC Run2012_AB_C_D_oneRunPerEra --pileup fromDB --datatier GEN-SIM-RAW --conditions START53_V7N::All --step DIGI,L1,DIGI2RAW,HLT:7E33v2 --python_filename hlt.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 In section How were these data generated? of the record, you can find the pile-up dataset. Additionally, you can manually add ROOT files to the hlt.py file for the pile-up configuration by looking at the list of ROOT files that were used in the Step HLT configuration file of the record you are studying. This involves, for instance, opening file hlt.py and replacing the line process.mix.input.fileNames = cms.untracked.vstring([]) with process.mix.input.fileNames = cms.untracked.vstring([ 'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/005825F1-F260-E111-BD97-003048C692DA.root', 'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/003EEBD4-8061-E111-9A23-003048D437F2.root', 'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/0005E496-3661-E111-B31E-003048F0E426.root']) Now, run the CMSSW executable: cmsRun hlt.py Reconstruction \u00b6 Execute the cmsDriver command as: cmsDriver.py step2 --filein file:hlt.root --fileout file:reco.root --mc --eventcontent AODSIM,DQM --datatier AODSIM,DQM --conditions START53_V7N::All --step RAW2DIGI,L1Reco,RECO,VALIDATION:validation_prod,DQM:DQMOfflinePOGMC --python_filename reco.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Now, run the CMSSW executable: cmsRun reco.py You can start ROOT and type TBrowser t to explore the files that were created. Example for event generation with 2011 CMSSW machinery \u00b6 In this example , you will learn how to generate 2011 MC Drell-Yan events from scratch. A Drell-Yan process occurs when a quark and an antiquark annihilate, creating a virtual photon or Z boson, which then decays into a pair of oppositely charged leptons. Example for event generation with 2012 CMSSW machinery \u00b6 In this example , you will learn how to generate 2012 MC QCD events, which involve the strong interaction between quarks and gluons. Additionally, you will know what are the steps to extract the tracking information of these events.","title":"Event Generation"},{"location":"analysis/datasim/eventgeneration/#event-generation","text":"Warning This page is under construction Physical event generation and detector simulation are the first steps in producing Monte Carlo samples suitable for physical analysis. Here we will teach you how to use the CMS datasets in the CERN Open Data Portal and the CMSSW machinery for the generation of events in simple steps: Generation and Simulation: To simulate beam collisions. Triggers: To simulate the effect of the detectors and electronics. Reconstruction: For the reconstruction of the events in the collisions. What you will find here: Virtual machines Dataset name System details Configuration files cmsDriver Generation from Matrix Element (ME) generators LHE Simulation High Level Trigger (HLT) Reconstruction Generation from general-purpose generators Generation and Simulation High Level Trigger (HLT) Reconstruction Example for event generation with 2011 CMSSW machinery Example for event generation with 2012 CMSSW machinery","title":"Event Generation"},{"location":"analysis/datasim/eventgeneration/#virtual-machines","text":"A specific CMS virtual machine includes the ROOT framework and CMSSW. Follow these instructions to configure a CERN virtual machine on your computer to be used with the 2011 and 2012 CMS open data.","title":"Virtual machines"},{"location":"analysis/datasim/eventgeneration/#dataset-name","text":"When exploring a simulated dataset on the CERN Open Data Portal , the first thing you will see is the name of the dataset. CMS uses the following naming convention : PROCESS_RANGETYPE-RANGELOWtoRANGEHIGH_FILTER_TUNE_COMMENT_COMENERGY-GENERATOR Take as an example the name of record 12201 : QCD_Pt-15to3000_TuneZ2star_Flat_8TeV_pythia6","title":"Dataset name"},{"location":"analysis/datasim/eventgeneration/#system-details","text":"In the record of each dataset, you can find the recommended global tag and release for analysis (CMSSW is the data analysis library). A global tag stores additional data that is required by the reconstruction and analysis software. Take as an example section System details of record 12201 : Recommended global tag for analysis: START53_V27 Recommended release for analysis: CMSSW_5_3_32","title":"System details"},{"location":"analysis/datasim/eventgeneration/#configuration-files","text":"The CMS software framework uses a software bus model, where data is stored in the event which is passed to a series of modules. A single executable, cmsRun , is used, and the modules are loaded at runtime. A configuration file defines which modules are loaded, in which order they are run, and with which configurable parameters they are run. You can find the configuration files for the generation of events for each dataset in its respective record within the CERN Open Data Portal . Check, for example, the section How were these data generated? of record 12201 .","title":"Configuration files"},{"location":"analysis/datasim/eventgeneration/#cmsdriver","text":"The cmsDriver is a tool to create production-solid configuration files from minimal command line options. Its code implementation, the cmsDriver.py script, is part of the CMSSW software. A summary of the cmsDriver.py script's options with a detailed message about each one can be visualized by getting the help: cmsDriver.py --help","title":"cmsDriver"},{"location":"analysis/datasim/eventgeneration/#generation-from-matrix-element-me-generators","text":"Generator-level datasets can be produced using a Matrix Element (ME) generator (e.g., Powheg , MadGraph5_aMCatNLO , Alpgen ) to deliver the event at the parton level and then a general-purpose generator to hadronise the event. Here we will reproduce the steps in the generation of record 1352 . Guided by the system details specified in the dataset, you should start by setting up your run time environment: cmsrel CMSSW_5_3_32 cd CMSSW_5_3_32/src/ cmsenv We will create a package according to our dataset: mkdir MyPackage cd MyPackage mkedanlzr MySim","title":"Generation from Matrix Element (ME) generators"},{"location":"analysis/datasim/eventgeneration/#lhe","text":"The Les Houches Event file format ( LHE ) is an agreement between Monte Carlo event generators and theorists to define Matrix Element level event listings in a common language. The LHE input file that store process and event information can be one generated by you or you can look for examples in /eos/cms/store/lhe/ . Here we will use a file with events generated for record 1352 : cmsDriver.py step1 --filein lhe:10270 --fileout file:LHE.root --mc --eventcontent LHE --datatier GEN --conditions START53_LV6A1::All --step NONE --python_filename LHE.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Run the CMSSW executable: cmsRun LHE.py","title":"LHE"},{"location":"analysis/datasim/eventgeneration/#simulation","text":"The next step is to generate fully hadronised events. We need to use the appropriate configuration file for this purpose. Take as an example the file in Step SIM for the simulation of record 1352 . The configuration file is in this link . We add this file to our local area: curl http://uaf-10.t2.ucsd.edu/~phchang/analysis/generator/genproductions/python/SevenTeV/Hadronizer_TuneZ2_7TeV_generic_LHE_pythia_tauola_cff.py -o MySim/python/mysim.py Compile everything: scram b Execute the cmsDriver command as: cmsDriver.py MyPackage/MySim/python/mysim.py --filein file:LHE.root --fileout file:sim.root --mc --eventcontent RAWSIM --customise SimG4Core/Application/reproc2011_2012_cff.customiseG4,Configuration/DataProcessing/Utils.addMonitoring --datatier GEN-SIM --conditions START53_LV6A1::All --beamspot Realistic7TeV2011CollisionV2 --step GEN,SIM --datamix NODATAMIXER --python_filename sim.py --no_exec -n 3 Run the CMSSW executable: cmsRun sim.py","title":"Simulation"},{"location":"analysis/datasim/eventgeneration/#high-level-trigger-hlt","text":"It is a crucial part of the CMS data flow since it is the HLT algorithms and filters which will decide whether an event should be kept for an offline analysis: any offline analysis depends on the outcome of HLT. Execute the cmsDriver command as: cmsDriver.py step1 --filein file:sim.root --fileout file:hlt.root --mc --eventcontent RAWSIM --runsScenarioForMC Run2012_AB_C_D_oneRunPerEra --datatier GEN-RAW --conditions START53_LV6A1::All --step DIGI,L1,DIGI2RAW,HLT:2011 --python_filename hlt.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Now, run the CMSSW executable: cmsRun hlt.py","title":"High Level Trigger (HLT)"},{"location":"analysis/datasim/eventgeneration/#reconstruction","text":"The algorithms that make up the CMS event reconstruction software build physics objects (e.g., muons, electrons, jets) from the raw data recorded by the detector. All events collected by the CMS trigger system are reconstructed by the CMS prompt reconstruction system soon after being collected. Execute the cmsDriver command as: cmsDriver.py step2 --filein file:hlt.root --fileout file:reco.root --mc --eventcontent AODSIM,DQM --datatier AODSIM,DQM --conditions START53_LV6A1::All --step RAW2DIGI,L1Reco,RECO,VALIDATION:validation_prod,DQM:DQMOfflinePOGMC --python_filename reco.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Now, run the CMSSW executable: cmsRun reco.py You can start ROOT and type TBrowser t to explore the files that were created.","title":"Reconstruction"},{"location":"analysis/datasim/eventgeneration/#generation-from-general-purpose-generators","text":"Generator-level datasets can be produced using a general-purpose generator (e.g., Pythia , Herwig , Tauola ) to simulate the event and the hadronisation. Here we will reproduce the steps in the generation of record 12201 . Guided by the system details specified in the dataset, you should start by setting up your run time environment: cmsrel CMSSW_5_3_32 cd CMSSW_5_3_32/src/ cmsenv We will create a package according to our dataset: mkdir MyPackage cd MyPackage mkedanlzr MyGen","title":"Generation from general-purpose generators"},{"location":"analysis/datasim/eventgeneration/#generation-and-simulation","text":"We need to use the appropriate configuration file. Take as an example the file in Step SIM for the generation and simulation of record 12201 . The configuration file is in this link . We add this file to our local area: curl https://raw.githubusercontent.com/cms-sw/genproductions/master/python/EightTeV/QCD_Pt/QCD_Pt_15to3000_TuneZ2star_Flat_8TeV_pythia6_cff.py -o MyGen/python/mygen.py Compile everything: scram b Execute the cmsDriver command as: cmsDriver.py MyPackage/MyGen/python/mygen.py --fileout file:gen.root --mc --eventcontent RAWSIM --pileup NoPileUp --customise Configuration/StandardSequences/SimWithCastor_cff.customise,Configuration/DataProcessing/Utils.addMonitoring --datatier GEN-SIM --conditions START50_V13::All --beamspot Realistic8TeVCollision --step GEN,SIM --datamix NODATAMIXER --python_filename gen.py --no_exec -n 3 Run the CMSSW executable: cmsRun gen.py","title":"Generation and Simulation"},{"location":"analysis/datasim/eventgeneration/#high-level-trigger-hlt_1","text":"Execute the cmsDriver command as: cmsDriver.py step1 --filein file:gen.root --fileout file:hlt.root --pileup_input dbs:/MinBias_TuneZ2star_8TeV-pythia6/Summer12-START50_V13-v3/GEN-SIM --mc --eventcontent RAWSIM --runsScenarioForMC Run2012_AB_C_D_oneRunPerEra --pileup fromDB --datatier GEN-SIM-RAW --conditions START53_V7N::All --step DIGI,L1,DIGI2RAW,HLT:7E33v2 --python_filename hlt.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 In section How were these data generated? of the record, you can find the pile-up dataset. Additionally, you can manually add ROOT files to the hlt.py file for the pile-up configuration by looking at the list of ROOT files that were used in the Step HLT configuration file of the record you are studying. This involves, for instance, opening file hlt.py and replacing the line process.mix.input.fileNames = cms.untracked.vstring([]) with process.mix.input.fileNames = cms.untracked.vstring([ 'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/005825F1-F260-E111-BD97-003048C692DA.root', 'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/003EEBD4-8061-E111-9A23-003048D437F2.root', 'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/0005E496-3661-E111-B31E-003048F0E426.root']) Now, run the CMSSW executable: cmsRun hlt.py","title":"High Level Trigger (HLT)"},{"location":"analysis/datasim/eventgeneration/#reconstruction_1","text":"Execute the cmsDriver command as: cmsDriver.py step2 --filein file:hlt.root --fileout file:reco.root --mc --eventcontent AODSIM,DQM --datatier AODSIM,DQM --conditions START53_V7N::All --step RAW2DIGI,L1Reco,RECO,VALIDATION:validation_prod,DQM:DQMOfflinePOGMC --python_filename reco.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Now, run the CMSSW executable: cmsRun reco.py You can start ROOT and type TBrowser t to explore the files that were created.","title":"Reconstruction"},{"location":"analysis/datasim/eventgeneration/#example-for-event-generation-with-2011-cmssw-machinery","text":"In this example , you will learn how to generate 2011 MC Drell-Yan events from scratch. A Drell-Yan process occurs when a quark and an antiquark annihilate, creating a virtual photon or Z boson, which then decays into a pair of oppositely charged leptons.","title":"Example for event generation with 2011 CMSSW machinery"},{"location":"analysis/datasim/eventgeneration/#example-for-event-generation-with-2012-cmssw-machinery","text":"In this example , you will learn how to generate 2012 MC QCD events, which involve the strong interaction between quarks and gluons. Additionally, you will know what are the steps to extract the tracking information of these events.","title":"Example for event generation with 2012 CMSSW machinery"},{"location":"analysis/datasim/mcsimulations/","text":"Monte Carlo Simulations \u00b6 Warning This page is under construction A set of simulated data (Monte Carlo - MC) corresponding to the collision data is made available. All directly available MC datasets can be found with this search . Furthermore, large amount of MC, thought to be of less frequent use, is available on demand and included in search results if \" include on-demand datasets \" option is selected. MC dataset are searchable by categories , which can be found under \"Filter by category\" on the left bar of the search page. The dataset name consists of three parts separated by / e.g.: /DYToMuMu_M-15To50_Tune4C_8TeV-pythia8/Summer12_DR53X-PU_S10_START53_V19-v1/AODSIM The first part indicates the simulated physics process ( DYToMuMu ), some of the production parameters ( M-15To50_Tune4C ), collision energy ( 8TeV ), and the event generator used in the processing chain. CMS simulated datasets names gives more details in the naming. The second part is the production campaign ( Summer12_DR53X ), pile-up profile ( PU_S10 ) and processing conditions ( START53_V19 ), and the last one indicates the data format ( AODSIM ). Dataset contents \u00b6 The dataset naming reflects the contents of the dataset, and the actual generator parameters with which the dataset contents have been defined can be found as explained under \" Finding the generator parameters \" in the CMS Monte Carlo production overview . Processing \u00b6 CMS Monte Carlo production overview briefly describes the steps in the MC production chain. Data format \u00b6 The data format in use for Run1 MC data is Analysis Object Data (AODSIM). A brief description of data formats can be found in the introductory About CMS under \" Primary and simulated datasets \".","title":"MC Simulations"},{"location":"analysis/datasim/mcsimulations/#monte-carlo-simulations","text":"Warning This page is under construction A set of simulated data (Monte Carlo - MC) corresponding to the collision data is made available. All directly available MC datasets can be found with this search . Furthermore, large amount of MC, thought to be of less frequent use, is available on demand and included in search results if \" include on-demand datasets \" option is selected. MC dataset are searchable by categories , which can be found under \"Filter by category\" on the left bar of the search page. The dataset name consists of three parts separated by / e.g.: /DYToMuMu_M-15To50_Tune4C_8TeV-pythia8/Summer12_DR53X-PU_S10_START53_V19-v1/AODSIM The first part indicates the simulated physics process ( DYToMuMu ), some of the production parameters ( M-15To50_Tune4C ), collision energy ( 8TeV ), and the event generator used in the processing chain. CMS simulated datasets names gives more details in the naming. The second part is the production campaign ( Summer12_DR53X ), pile-up profile ( PU_S10 ) and processing conditions ( START53_V19 ), and the last one indicates the data format ( AODSIM ).","title":"Monte Carlo Simulations"},{"location":"analysis/datasim/mcsimulations/#dataset-contents","text":"The dataset naming reflects the contents of the dataset, and the actual generator parameters with which the dataset contents have been defined can be found as explained under \" Finding the generator parameters \" in the CMS Monte Carlo production overview .","title":"Dataset contents"},{"location":"analysis/datasim/mcsimulations/#processing","text":"CMS Monte Carlo production overview briefly describes the steps in the MC production chain.","title":"Processing"},{"location":"analysis/datasim/mcsimulations/#data-format","text":"The data format in use for Run1 MC data is Analysis Object Data (AODSIM). A brief description of data formats can be found in the introductory About CMS under \" Primary and simulated datasets \".","title":"Data format"},{"location":"analysis/interpretation/limits/","text":"Upper-limit calculations \u00b6 Warning This page is under construction","title":"Upper-limit Calculations"},{"location":"analysis/interpretation/limits/#upper-limit-calculations","text":"Warning This page is under construction","title":"Upper-limit calculations"},{"location":"analysis/interpretation/stats/","text":"Statistics \u00b6 Warning This page is under construction","title":"Statistics"},{"location":"analysis/interpretation/stats/#statistics","text":"Warning This page is under construction","title":"Statistics"},{"location":"analysis/luminosity/lumi/","text":"Luminosity \u00b6 Warning This page is under construction","title":"Luminosity"},{"location":"analysis/luminosity/lumi/#luminosity","text":"Warning This page is under construction","title":"Luminosity"},{"location":"analysis/selection/objectid/","text":"Object ID \u00b6 Warning This page is under construction","title":"Physics Objects ID"},{"location":"analysis/selection/objectid/#object-id","text":"Warning This page is under construction","title":"Object ID"},{"location":"analysis/selection/objects/","text":"Physics Objects \u00b6 Warning This page is under construction Description \u00b6 The CMS is a giant detector that acts like a camera that \"photographs\" particle collisions, allowing us to interpret their nature. Certainly we cannot directly observe all the particles created in the collisions because some of them decay very quickly or simply do not interact with our detector. However, we can infer their presence. If they decay to other stable particles and interact with the apparatus, they leave signals in the CMS subdetectors. These signals are used to reconstruct the decay products or infer their presence; we call these physics objects . These objects could be electrons, muons, jets, missing energy, etc., but also lower level objects like tracks. For the current releases of open data, we store them in ROOT files following the EDM data model in AOD format. In the CERN Open Portal site one can find a more detailed description of these physical objects and a list of them corresponding to 2010 and 2011/2012 releases of open data. DataFormats \u00b6 As one can see in those guides, these physical objects are usually stored in specific collections . For instance, muons are most commonly obtained from the reco::Muon collection. The AOD Data Format Table gives a good description of the different collections (or data formats) for the AOD tier. Unfortunately, the links for the containers column got broken after CMSSW was moved to Github. Those links would have pointed us to the corresponding CMSSW C++ classes associated with those containers. This is important because one needs to know which CMSSW class matches a given collection of objects to include the headers of those classes in the header of your analyzer code. But let that not let us down. Fortunately, the names of the collections containers actually match the name of its associated CMSSW classes. These classes (data format classes) live under the DataFormats directory in CMSSW. If we browse through, we find the MuonReco package. In its interface area we find the DataFormats/MuonReco/interface/Muon.h class header, which is the one we would need to incorporate in our analyzer. This is corroborated by this Muon Analysis Twiki section . Remember When accessing a specific piece of code in the CMSSW github repository, and want to explore its methods, variables, etc., make sure you select the right git branch. E.g., CMSSW_5_3_X for 2011/2012 open data. In addition to this base class, sometimes it is necessary to invoke other auxiliary classes. For instance, DataFormats/MuonReco/interface/MuonFwd.h , which can be found in the same interface area. So, in the context of this example, in order to support muons information, at the top of your EDAnalyzer you should include the following lines: //classes to extract Muon information #include \"DataFormats/MuonReco/interface/Muon.h\" #include \"DataFormats/MuonReco/interface/MuonFwd.h\" Access methods \u00b6 In the Event methods for data access section of the Getting Data From an Event Twiki page, one can find a complete description of the different methods available for Event data access. Remember When accessing the CMS twiki pages we will usually point you to the most recent page. However, historical Twiki documentation, i.e., earlier revision of the pages, may provide more accurate information for open data that is already a few years old. One can access this historical archive by going to the bottom of any Twiki page, clicking on History and exploring the revisions closer to the open data release year. As indicated in that page, all Event data access methods use the edm::Handle<T> , where T is the C++ type of the requested object, to hold the result of an access. As an example, during Run 1, the recommended method was the getByLabel one. This method needed an InputTag . This can also be extracted from the AOD Data Format Table . The first column indicate the InputTag: Therefore, in the context of this muon example, in the analyze method of your EDAnalyzer you should include the following lines: Handle < reco :: MuonCollection > mymuons ; iEvent . getByLabel ( \"muons\" , mymuons ); If you required cosmic muons, for some reason, you would need instead: Handle < reco :: MuonCollection > mymuons ; iEvent . getByLabel ( \"muonsFromCosmics\" , mymuons ); From configuration \u00b6 Alternatively, it would be also possible to retrieve the InputTag name from configuration . In that case, in your configuration file you would need something like: process . demo = cms . EDAnalyzer ( 'MuonAnalyzer' , InputCollection = cms . InputTag ( \"muons\" ) ) In this case, you would need to declare the appropriate input tag in your EDAnalyzer class: //declare the input tag for MuonCollection edm :: InputTag muonInput ; Extract it from the ParameterSet in the constructor MuonAnalyzer :: MuonAnalyzer ( const edm :: ParameterSet & iConfig ) { //now do what ever initialization is needed muonInput = iConfig . getParameter < edm :: InputTag > ( \"InputCollection\" ); } and use in the analyze routine: Handle < reco :: MuonCollection > mymuons ; iEvent . getByLabel ( muonInput , mymuons ); Additional information for accessing CMS physics objects \u00b6 In Chapter 7 of the CMS Workbook one can find Analysis pages that provide additional information, which can be useful to check on top of the general strategy for accessing objects that was discussed above.","title":"Physics Objects"},{"location":"analysis/selection/objects/#physics-objects","text":"Warning This page is under construction","title":"Physics Objects"},{"location":"analysis/selection/objects/#description","text":"The CMS is a giant detector that acts like a camera that \"photographs\" particle collisions, allowing us to interpret their nature. Certainly we cannot directly observe all the particles created in the collisions because some of them decay very quickly or simply do not interact with our detector. However, we can infer their presence. If they decay to other stable particles and interact with the apparatus, they leave signals in the CMS subdetectors. These signals are used to reconstruct the decay products or infer their presence; we call these physics objects . These objects could be electrons, muons, jets, missing energy, etc., but also lower level objects like tracks. For the current releases of open data, we store them in ROOT files following the EDM data model in AOD format. In the CERN Open Portal site one can find a more detailed description of these physical objects and a list of them corresponding to 2010 and 2011/2012 releases of open data.","title":"Description"},{"location":"analysis/selection/objects/#dataformats","text":"As one can see in those guides, these physical objects are usually stored in specific collections . For instance, muons are most commonly obtained from the reco::Muon collection. The AOD Data Format Table gives a good description of the different collections (or data formats) for the AOD tier. Unfortunately, the links for the containers column got broken after CMSSW was moved to Github. Those links would have pointed us to the corresponding CMSSW C++ classes associated with those containers. This is important because one needs to know which CMSSW class matches a given collection of objects to include the headers of those classes in the header of your analyzer code. But let that not let us down. Fortunately, the names of the collections containers actually match the name of its associated CMSSW classes. These classes (data format classes) live under the DataFormats directory in CMSSW. If we browse through, we find the MuonReco package. In its interface area we find the DataFormats/MuonReco/interface/Muon.h class header, which is the one we would need to incorporate in our analyzer. This is corroborated by this Muon Analysis Twiki section . Remember When accessing a specific piece of code in the CMSSW github repository, and want to explore its methods, variables, etc., make sure you select the right git branch. E.g., CMSSW_5_3_X for 2011/2012 open data. In addition to this base class, sometimes it is necessary to invoke other auxiliary classes. For instance, DataFormats/MuonReco/interface/MuonFwd.h , which can be found in the same interface area. So, in the context of this example, in order to support muons information, at the top of your EDAnalyzer you should include the following lines: //classes to extract Muon information #include \"DataFormats/MuonReco/interface/Muon.h\" #include \"DataFormats/MuonReco/interface/MuonFwd.h\"","title":"DataFormats"},{"location":"analysis/selection/objects/#access-methods","text":"In the Event methods for data access section of the Getting Data From an Event Twiki page, one can find a complete description of the different methods available for Event data access. Remember When accessing the CMS twiki pages we will usually point you to the most recent page. However, historical Twiki documentation, i.e., earlier revision of the pages, may provide more accurate information for open data that is already a few years old. One can access this historical archive by going to the bottom of any Twiki page, clicking on History and exploring the revisions closer to the open data release year. As indicated in that page, all Event data access methods use the edm::Handle<T> , where T is the C++ type of the requested object, to hold the result of an access. As an example, during Run 1, the recommended method was the getByLabel one. This method needed an InputTag . This can also be extracted from the AOD Data Format Table . The first column indicate the InputTag: Therefore, in the context of this muon example, in the analyze method of your EDAnalyzer you should include the following lines: Handle < reco :: MuonCollection > mymuons ; iEvent . getByLabel ( \"muons\" , mymuons ); If you required cosmic muons, for some reason, you would need instead: Handle < reco :: MuonCollection > mymuons ; iEvent . getByLabel ( \"muonsFromCosmics\" , mymuons );","title":"Access methods"},{"location":"analysis/selection/objects/#from-configuration","text":"Alternatively, it would be also possible to retrieve the InputTag name from configuration . In that case, in your configuration file you would need something like: process . demo = cms . EDAnalyzer ( 'MuonAnalyzer' , InputCollection = cms . InputTag ( \"muons\" ) ) In this case, you would need to declare the appropriate input tag in your EDAnalyzer class: //declare the input tag for MuonCollection edm :: InputTag muonInput ; Extract it from the ParameterSet in the constructor MuonAnalyzer :: MuonAnalyzer ( const edm :: ParameterSet & iConfig ) { //now do what ever initialization is needed muonInput = iConfig . getParameter < edm :: InputTag > ( \"InputCollection\" ); } and use in the analyze routine: Handle < reco :: MuonCollection > mymuons ; iEvent . getByLabel ( muonInput , mymuons );","title":"From configuration"},{"location":"analysis/selection/objects/#additional-information-for-accessing-cms-physics-objects","text":"In Chapter 7 of the CMS Workbook one can find Analysis pages that provide additional information, which can be useful to check on top of the general strategy for accessing objects that was discussed above.","title":"Additional information for accessing CMS physics objects"},{"location":"analysis/selection/triggers/","text":"Triggers \u00b6 Warning This page is under construction","title":"Triggers"},{"location":"analysis/selection/triggers/#triggers","text":"Warning This page is under construction","title":"Triggers"},{"location":"analysis/selection/efficiencyidstudies/signalextraction/","text":"Signal Extraction \u00b6 Warning This page is under construction Detector reconstruction efficiencies are calculated using signal muons, that is, only true candidates decaying to dimuons. This is achieved in this study by extracting signal from the data by the usage of some methods. Here it is presented two: sideband subtraction and fitting. Sideband subtraction method \u00b6 The sideband subtraction method involves choosing sideband and signal regions in invariant mass distribution for each tag+probe pair. The signal region is selected by finding the ressonance position and defining a region around it. While the signal region contains both signal and background, the sideband region is chosen such as to have only background, with a distance from signal region. A example of those regions selection can be seen below for the J/psi ressonance. For each event category (i.e. Pass and All), and for a given variable of interest (e.g., the probe pT), two distributions are obtained, one for each region (Signal and Sideband). In order to obtain the variable distribution for the signal only, we proceed by subtracting the Background distribution (Sideband region) from the Signal+Background one (Signal region): Where the normalization \u03b1 factor quantifies the quantity of background present in the signal region: And for the uncertainty: Fitting method \u00b6 In this method, the signal is extracted not by histogram manipulation but by likelihood fitting. The procedure is applied after splitting the data in sub-samples, corresponding to bins of the kinematic variable of interest of the probe objects. As such, the efficiency will be measured as a function of that variable. Each sub-sample contains signal and background events; the signal is accessed by fitting the invariant mass spectra The fit for each bin allows to statistically discriminate between signal and background. In particular, the fit yields the number of signal events. The efficiency is finally obtained by simply forming the ratio of the signal yield from the fit to the passing category by the signal yield from the fit of the inclusive all category. This approach is illustrated below.","title":"Signal extraction"},{"location":"analysis/selection/efficiencyidstudies/signalextraction/#signal-extraction","text":"Warning This page is under construction Detector reconstruction efficiencies are calculated using signal muons, that is, only true candidates decaying to dimuons. This is achieved in this study by extracting signal from the data by the usage of some methods. Here it is presented two: sideband subtraction and fitting.","title":"Signal Extraction"},{"location":"analysis/selection/efficiencyidstudies/signalextraction/#sideband-subtraction-method","text":"The sideband subtraction method involves choosing sideband and signal regions in invariant mass distribution for each tag+probe pair. The signal region is selected by finding the ressonance position and defining a region around it. While the signal region contains both signal and background, the sideband region is chosen such as to have only background, with a distance from signal region. A example of those regions selection can be seen below for the J/psi ressonance. For each event category (i.e. Pass and All), and for a given variable of interest (e.g., the probe pT), two distributions are obtained, one for each region (Signal and Sideband). In order to obtain the variable distribution for the signal only, we proceed by subtracting the Background distribution (Sideband region) from the Signal+Background one (Signal region): Where the normalization \u03b1 factor quantifies the quantity of background present in the signal region: And for the uncertainty:","title":"Sideband subtraction method"},{"location":"analysis/selection/efficiencyidstudies/signalextraction/#fitting-method","text":"In this method, the signal is extracted not by histogram manipulation but by likelihood fitting. The procedure is applied after splitting the data in sub-samples, corresponding to bins of the kinematic variable of interest of the probe objects. As such, the efficiency will be measured as a function of that variable. Each sub-sample contains signal and background events; the signal is accessed by fitting the invariant mass spectra The fit for each bin allows to statistically discriminate between signal and background. In particular, the fit yields the number of signal events. The efficiency is finally obtained by simply forming the ratio of the signal yield from the fit to the passing category by the signal yield from the fit of the inclusive all category. This approach is illustrated below.","title":"Fitting method"},{"location":"analysis/selection/efficiencyidstudies/tagandprobe/","text":"Tag and Probe \u00b6 Warning This page is under construction The Tag and Probe method is an experimental procedure commonly used in particle physics that allows to measure a process\u2019 efficiency directly from data. The procedure provides an unbiased sample of probe objects that can be then used to measure the efficiency of a particular selection criteria. Tag and Probe method \u00b6 This method is a data-driven technique and it is based on decays of known ressonances in pair of particles. The decaying muons are labeled according to the following criteria: Tag muon : well identified, triggered muon (tight selection criteria). Probe muon : unbiased set of muon candidates (very loose selection criteria), either passing or failing the criteria for which the eciency is to be measured. Tag muon are employed to trigger the presence of a resonance decay while probe muons, paired to tag muons, will be used for getting efficiency due its' unbiased characteristic. CMS Efficiency \u00b6 The efficiency will be given by the fraction of probe muons that pass a given criteria (in this case, the Muon ID which is explained below): The denominator corresponds to the number of resonance candidates (tag+probe pairs) reconstructed in the dataset. The numerator corresponds to the subset for which the probe passes the criteria. CMS Muon identification and reconstruction \u00b6 In the standard CMS reconstruction for proton-proton collisions, tracks are first reconstructed independently in the inner tracker and in the muon system. Based on these objects, three reconstruction approaches are used: Tracker Muon reconstruction: all tracker tracks with pT > 0.5 GeV/c and total momentum p > 2.5 GeV/c are considered as possible muon candidates, and are extrapolated to the muon system taking into account the magnetic field; Standalone Muon reconstruction: all tracks of the segments reconstructed in the muon chambers (performed using segments and hits from Drift Tubes in the barrel region, Cathode Strip Chambers and Resistive Plates Chambers in the endcaps) are used to generate \u201cseeds\u201d consisting of position and direction vectors and an estimate of the muon transverse momentum; Global Muon reconstruction: starts from a Standalone reconstructed muon track and extrapolates its trajectory from the innermost muon station through the coil and both calorimeters to the outer tracker surface. These are illustrated below: Note You can find more details concerning CMS Muon Identification and reconstruction in this paper JINST 7 (2012) P10002 .","title":"Tag and Probe"},{"location":"analysis/selection/efficiencyidstudies/tagandprobe/#tag-and-probe","text":"Warning This page is under construction The Tag and Probe method is an experimental procedure commonly used in particle physics that allows to measure a process\u2019 efficiency directly from data. The procedure provides an unbiased sample of probe objects that can be then used to measure the efficiency of a particular selection criteria.","title":"Tag and Probe"},{"location":"analysis/selection/efficiencyidstudies/tagandprobe/#tag-and-probe-method","text":"This method is a data-driven technique and it is based on decays of known ressonances in pair of particles. The decaying muons are labeled according to the following criteria: Tag muon : well identified, triggered muon (tight selection criteria). Probe muon : unbiased set of muon candidates (very loose selection criteria), either passing or failing the criteria for which the eciency is to be measured. Tag muon are employed to trigger the presence of a resonance decay while probe muons, paired to tag muons, will be used for getting efficiency due its' unbiased characteristic.","title":"Tag and Probe method"},{"location":"analysis/selection/efficiencyidstudies/tagandprobe/#cms-efficiency","text":"The efficiency will be given by the fraction of probe muons that pass a given criteria (in this case, the Muon ID which is explained below): The denominator corresponds to the number of resonance candidates (tag+probe pairs) reconstructed in the dataset. The numerator corresponds to the subset for which the probe passes the criteria.","title":"CMS Efficiency"},{"location":"analysis/selection/efficiencyidstudies/tagandprobe/#cms-muon-identification-and-reconstruction","text":"In the standard CMS reconstruction for proton-proton collisions, tracks are first reconstructed independently in the inner tracker and in the muon system. Based on these objects, three reconstruction approaches are used: Tracker Muon reconstruction: all tracker tracks with pT > 0.5 GeV/c and total momentum p > 2.5 GeV/c are considered as possible muon candidates, and are extrapolated to the muon system taking into account the magnetic field; Standalone Muon reconstruction: all tracks of the segments reconstructed in the muon chambers (performed using segments and hits from Drift Tubes in the barrel region, Cathode Strip Chambers and Resistive Plates Chambers in the endcaps) are used to generate \u201cseeds\u201d consisting of position and direction vectors and an estimate of the muon transverse momentum; Global Muon reconstruction: starts from a Standalone reconstructed muon track and extrapolates its trajectory from the innermost muon station through the coil and both calorimeters to the outer tracker surface. These are illustrated below: Note You can find more details concerning CMS Muon Identification and reconstruction in this paper JINST 7 (2012) P10002 .","title":"CMS Muon identification and reconstruction"},{"location":"analysis/systematics/lumiuncertain/","text":"Luminosity Uncertainty \u00b6 Warning This page is under construction","title":"Luminosity Uncertainties"},{"location":"analysis/systematics/lumiuncertain/#luminosity-uncertainty","text":"Warning This page is under construction","title":"Luminosity Uncertainty"},{"location":"analysis/systematics/mcuncertain/","text":"MC Uncertainty \u00b6 Warning This page is under construction","title":"MC Uncertainty"},{"location":"analysis/systematics/mcuncertain/#mc-uncertainty","text":"Warning This page is under construction","title":"MC Uncertainty"},{"location":"analysis/systematics/objectsuncertain/","text":"Object Uncertainty \u00b6 Warning This page is under construction","title":"Object Uncertainty"},{"location":"analysis/systematics/objectsuncertain/#object-uncertainty","text":"Warning This page is under construction","title":"Object Uncertainty"},{"location":"analysis/systematics/pileupuncertain/","text":"Pileup Uncertainty \u00b6 Warning This page is under construction","title":"Pileup Uncertainty"},{"location":"analysis/systematics/pileupuncertain/#pileup-uncertainty","text":"Warning This page is under construction","title":"Pileup Uncertainty"},{"location":"cmssw/cmsswanalyzers/","text":"Analyzers \u00b6 First, a few general words about analysis in the CMSSW framework. Physics analysis proceeds via a series of subsequent steps. Building blocks are identified and more complex objects are built on top of them. How to write a Framework Module and run the job with the cmsRun can be found here . When setting up code for the new EDM (such as creating a new EDProducer) there is a fair amount of 'boiler plate' code that you must write. To make writing such code easier CMS provides a series of scripts that will generate the necessary directory structure and files needed so that all you need to do is write your actual algorithms. CMSSW distiguishes the following module types : EDAnalyzer: takes input from the event and processes the input without writing information back to the event EDProducer: takes input from the event and produces new output which is saved in the event EDFilter: decides if processing the event can be stopped and continued EventSetup: external service not bound to the event structure which provides information useable by all modules (e.g. Geometry, Magnetic Field, etc.) In order to generate above modules: mkedanlzr : makes a skeleton of a package containing an EDAnalyzer mkedprod : makes a skeleton of a package containing an EDProducer mkedfltr : makes a skeleton of a package containing an EDFilter mkrecord : makes a complete implementation of a Record used by the EventSetup More generators are available and you can find them here Warning This page is under construction","title":"Analyzers"},{"location":"cmssw/cmsswanalyzers/#analyzers","text":"First, a few general words about analysis in the CMSSW framework. Physics analysis proceeds via a series of subsequent steps. Building blocks are identified and more complex objects are built on top of them. How to write a Framework Module and run the job with the cmsRun can be found here . When setting up code for the new EDM (such as creating a new EDProducer) there is a fair amount of 'boiler plate' code that you must write. To make writing such code easier CMS provides a series of scripts that will generate the necessary directory structure and files needed so that all you need to do is write your actual algorithms. CMSSW distiguishes the following module types : EDAnalyzer: takes input from the event and processes the input without writing information back to the event EDProducer: takes input from the event and produces new output which is saved in the event EDFilter: decides if processing the event can be stopped and continued EventSetup: external service not bound to the event structure which provides information useable by all modules (e.g. Geometry, Magnetic Field, etc.) In order to generate above modules: mkedanlzr : makes a skeleton of a package containing an EDAnalyzer mkedprod : makes a skeleton of a package containing an EDProducer mkedfltr : makes a skeleton of a package containing an EDFilter mkrecord : makes a complete implementation of a Record used by the EventSetup More generators are available and you can find them here Warning This page is under construction","title":"Analyzers"},{"location":"cmssw/cmsswconditions/","text":"Conditions \u00b6 This page explains the use of global tags and the condition database with the CMS Open Data. All information was taken from here . A Global Tag is a coherent collection of records of additional data needed by the reconstruction and analysis software. The Global Tag is defined for each data-taking period, separately for collision and simulated data. These records are stored in the condition database. Condition data include non-event-related information (Alignment, Calibration, Temperature, etc.) and parameters for the simulation/reconstruction/analysis software. For CMS Open Data, the condition data are provided as sqlite files in the /cvmfs/cms-opendata-conddb.cern.ch/ directory, which is accessible through the CMS Open Data VM. Most physics objects such as electrons , muons , photons in the CMS Open Data are already calibrated and ready-to-use, and no additional corrections are needed other than selection and identification criteria, which will be applied in the analysis code. Therefore, simple analyses do not need to access the condition database. For example you can check the Higgs analysis example . However, access to the condition database is necessary, for example, for jet energy corrections and trigger configuration information. Examples of such analyses are for the PAT object production or the top quark pair production . Note that when you need to access the condition database, the first time you run the job on the CMS Open Data VM, it will download the condition data from the /cvmfs area. It will take time (an example run of a 10 Mbps line took 45 mins), but it will only happen once as the files will be cached on your VM. The job will not produce any output during this time, but you can check the ongoing processes with the command 'top' and you can monitor the progress of reading the condition data to the local cache with the command 'df'. Collision data and Monte Carlo data sets can be found at http://opendata.cern.ch/docs/cms-guide-for-condition-database for years 2010, 2011 and 2012. Warning This page is under construction","title":"Conditions Data"},{"location":"cmssw/cmsswconditions/#conditions","text":"This page explains the use of global tags and the condition database with the CMS Open Data. All information was taken from here . A Global Tag is a coherent collection of records of additional data needed by the reconstruction and analysis software. The Global Tag is defined for each data-taking period, separately for collision and simulated data. These records are stored in the condition database. Condition data include non-event-related information (Alignment, Calibration, Temperature, etc.) and parameters for the simulation/reconstruction/analysis software. For CMS Open Data, the condition data are provided as sqlite files in the /cvmfs/cms-opendata-conddb.cern.ch/ directory, which is accessible through the CMS Open Data VM. Most physics objects such as electrons , muons , photons in the CMS Open Data are already calibrated and ready-to-use, and no additional corrections are needed other than selection and identification criteria, which will be applied in the analysis code. Therefore, simple analyses do not need to access the condition database. For example you can check the Higgs analysis example . However, access to the condition database is necessary, for example, for jet energy corrections and trigger configuration information. Examples of such analyses are for the PAT object production or the top quark pair production . Note that when you need to access the condition database, the first time you run the job on the CMS Open Data VM, it will download the condition data from the /cvmfs area. It will take time (an example run of a 10 Mbps line took 45 mins), but it will only happen once as the files will be cached on your VM. The job will not produce any output during this time, but you can check the ongoing processes with the command 'top' and you can monitor the progress of reading the condition data to the local cache with the command 'df'. Collision data and Monte Carlo data sets can be found at http://opendata.cern.ch/docs/cms-guide-for-condition-database for years 2010, 2011 and 2012. Warning This page is under construction","title":"Conditions"},{"location":"cmssw/cmsswconfigure/","text":"Configuration \u00b6 A configuration document, written using the Python language, is used to configure the cmsRun executable. A Python configuration program specifies which modules, inputs, outputs and services are to be loaded during execution, how to configure these modules and services, and in what order to execute them. All information can be found at twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideAboutPythonConfigFile .","title":"Configuration"},{"location":"cmssw/cmsswconfigure/#configuration","text":"A configuration document, written using the Python language, is used to configure the cmsRun executable. A Python configuration program specifies which modules, inputs, outputs and services are to be loaded during execution, how to configure these modules and services, and in what order to execute them. All information can be found at twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideAboutPythonConfigFile .","title":"Configuration"},{"location":"cmssw/cmsswdatamodel/","text":"Data Model \u00b6 The CMS Event Data Model (EDM) is centered around the concept of an Event . Physically, an event is the result of a single readout of the detector electronics and the signals that will (in general) have been generated by particles, tracks, energy deposits, present in a number of bunch crossings. In software terms, an Event starts as a collection of the RAW data from a detector or MC event, stored as a single entity in memory, a C++ type-safe container called edm::Event . An Event is a C++ object container for all RAW and reconstructed data related to a particular collision. During processing, data are passed from one module to the next via the Event, and are accessed only through the Event. All objects in the Event may be individually or collectively stored in ROOT files, and are thus directly browsable in ROOT. More and detailed information can be found here . The CMS Data Hierarchy \u00b6 CMS Data is arranged into a hierarchy of data tiers. Each physics event is written into each data tier, where the tiers each contain different levels of information about the event. The different tiers each have different uses. The three main data tiers written in CMS are: RAW: full event information from the Tier-0 (i.e. from CERN), containing 'raw' detector information (detector element hits, etc) RAW is not used directly for analysis RECO (\"RECOnstructed data\"): the output from first-pass processing by the Tier-0. This layer contains reconstructed physics objects, but it's still very detailed. RECO can be used for analysis, but is too big for frequent or heavy use when CMS has collected a substantial data sample. RECO Data Format Table AOD (\"Analysis Object Data\"): this is a \"distilled\" version of the RECO event information, and is expected to be used for most analyses. AOD provides a trade-off between event size and complexity of the available information to optimize flexibility and speed for analyses. AOD Data Format Table The data tiers are described in more detail in a dedicated WorkBook chapter on Data Formats and Tiers .","title":"Data Model"},{"location":"cmssw/cmsswdatamodel/#data-model","text":"The CMS Event Data Model (EDM) is centered around the concept of an Event . Physically, an event is the result of a single readout of the detector electronics and the signals that will (in general) have been generated by particles, tracks, energy deposits, present in a number of bunch crossings. In software terms, an Event starts as a collection of the RAW data from a detector or MC event, stored as a single entity in memory, a C++ type-safe container called edm::Event . An Event is a C++ object container for all RAW and reconstructed data related to a particular collision. During processing, data are passed from one module to the next via the Event, and are accessed only through the Event. All objects in the Event may be individually or collectively stored in ROOT files, and are thus directly browsable in ROOT. More and detailed information can be found here .","title":"Data Model"},{"location":"cmssw/cmsswdatamodel/#the-cms-data-hierarchy","text":"CMS Data is arranged into a hierarchy of data tiers. Each physics event is written into each data tier, where the tiers each contain different levels of information about the event. The different tiers each have different uses. The three main data tiers written in CMS are: RAW: full event information from the Tier-0 (i.e. from CERN), containing 'raw' detector information (detector element hits, etc) RAW is not used directly for analysis RECO (\"RECOnstructed data\"): the output from first-pass processing by the Tier-0. This layer contains reconstructed physics objects, but it's still very detailed. RECO can be used for analysis, but is too big for frequent or heavy use when CMS has collected a substantial data sample. RECO Data Format Table AOD (\"Analysis Object Data\"): this is a \"distilled\" version of the RECO event information, and is expected to be used for most analyses. AOD provides a trade-off between event size and complexity of the available information to optimize flexibility and speed for analyses. AOD Data Format Table The data tiers are described in more detail in a dedicated WorkBook chapter on Data Formats and Tiers .","title":"The CMS Data Hierarchy"},{"location":"cmssw/cmsswoverview/","text":"Overview \u00b6 The overall collection of software, referred to as CMS Software (CMSSW), is built around a Framework, an Event Data Model (EDM), and Services needed by the simulation, calibration and alignment, and reconstruction modules that process event data so that physicists can perform analysis. The primary goal of the Framework and EDM is to facilitate the development and deployment of reconstruction and analysis software. The CMSSW event processing model consists of one executable, called cmsRun , and many plug-in modules which are managed by the Framework. All the code needed in the event processing (calibration, reconstruction algorithms, etc.) is contained in the modules. The same executable is used for both detector and Monte Carlo data. More and detailed information can be found here .","title":"Overview"},{"location":"cmssw/cmsswoverview/#overview","text":"The overall collection of software, referred to as CMS Software (CMSSW), is built around a Framework, an Event Data Model (EDM), and Services needed by the simulation, calibration and alignment, and reconstruction modules that process event data so that physicists can perform analysis. The primary goal of the Framework and EDM is to facilitate the development and deployment of reconstruction and analysis software. The CMSSW event processing model consists of one executable, called cmsRun , and many plug-in modules which are managed by the Framework. All the code needed in the event processing (calibration, reconstruction algorithms, etc.) is contained in the modules. The same executable is used for both detector and Monte Carlo data. More and detailed information can be found here .","title":"Overview"},{"location":"tools/cernportal/","text":"The CERN Open Data Portal \u00b6 Warning This page is under construction","title":"CERN Open Data Portal"},{"location":"tools/cernportal/#the-cern-open-data-portal","text":"Warning This page is under construction","title":"The CERN Open Data Portal"},{"location":"tools/cmsopendata/","text":"CMS Open Data \u00b6 Warning This page is under construction","title":"CMS Open Data"},{"location":"tools/cmsopendata/#cms-open-data","text":"Warning This page is under construction","title":"CMS Open Data"},{"location":"tools/cmstwiki/","text":"The CMS Twiki \u00b6 Warning This page is under construction","title":"CMS Twiki"},{"location":"tools/cmstwiki/#the-cms-twiki","text":"Warning This page is under construction","title":"The CMS Twiki"},{"location":"tools/cppandpython/","text":"C++ and python \u00b6 Warning This page is under construction","title":"C++ and Python"},{"location":"tools/cppandpython/#c-and-python","text":"Warning This page is under construction","title":"C++ and python"},{"location":"tools/docker/","text":"Docker \u00b6 Warning This page is under construction Docker is a commercial implementation of a container , a way to package up a snapshot of everything needed to run some particular version of software (OS, libraries, compilers, etc.). It is a very effective way of interfacing with the CMS open data as it gives you the proper environment you need to analyze these data. To learn more about Docker in general, from a HEP perspective, you may want to check out this Introduction to Docker , from Matthew Feickert. You can also jump right in with a tutorial on running CMS analysis code using Docker .","title":"Docker"},{"location":"tools/docker/#docker","text":"Warning This page is under construction Docker is a commercial implementation of a container , a way to package up a snapshot of everything needed to run some particular version of software (OS, libraries, compilers, etc.). It is a very effective way of interfacing with the CMS open data as it gives you the proper environment you need to analyze these data. To learn more about Docker in general, from a HEP perspective, you may want to check out this Introduction to Docker , from Matthew Feickert. You can also jump right in with a tutorial on running CMS analysis code using Docker .","title":"Docker"},{"location":"tools/git/","text":"Git \u00b6 Warning This page is under construction Here are some helpful links to learn how to use git.","title":"Git"},{"location":"tools/git/#git","text":"Warning This page is under construction Here are some helpful links to learn how to use git.","title":"Git"},{"location":"tools/root/","text":"ROOT \u00b6 Warning This page is under construction From ROOT's webpage A modular scientific software toolkit. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualisation and storage. It is mainly written in C++ but integrated with other languages such as Python and R. It is the primary toolkit for many experimental analysis and while you are free to analyze these datasets however you like, some familiarity with ROOT will serve you well when accessing the data. Many ROOT examples can be found here . If you don't know where to start, we would recommend Example 1 Example 2 Example 3 Python has become the language of choice for many analysts and most of the examples you'll see make use of the PyROOT module, callable from python. You can go through a number of the examples here . If you don't know where to start, we would recommend Example 1 Example 2 Example 3","title":"ROOT"},{"location":"tools/root/#root","text":"Warning This page is under construction From ROOT's webpage A modular scientific software toolkit. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualisation and storage. It is mainly written in C++ but integrated with other languages such as Python and R. It is the primary toolkit for many experimental analysis and while you are free to analyze these datasets however you like, some familiarity with ROOT will serve you well when accessing the data. Many ROOT examples can be found here . If you don't know where to start, we would recommend Example 1 Example 2 Example 3 Python has become the language of choice for many analysts and most of the examples you'll see make use of the PyROOT module, callable from python. You can go through a number of the examples here . If you don't know where to start, we would recommend Example 1 Example 2 Example 3","title":"ROOT"},{"location":"tools/unix/","text":"Unix \u00b6 Warning This page is under construction Useful tips on basic unix environments","title":"UNIX"},{"location":"tools/unix/#unix","text":"Warning This page is under construction Useful tips on basic unix environments","title":"Unix"},{"location":"tools/virtualmachines/","text":"Virtual machines \u00b6 CMS open data and legacy data, even though still exciting and full of potential, are already a few years old. Because of the rapidly evolving technolgies, the computing environments that were used to analyze these data are already ancient compared to the current, bleeding edge ones. Therefore, in order to mantain our ability to study these data, we have to rely on technologies that help us preserve adequate computer environments. One way of doing this is by using virtual machines. In simple words, a virtual machine is an emulation of a computer system that can run within another system. The latter is usually known as the host . Open data releases, CMSSW versions and operating systems \u00b6 CMS open data from our 2010 release can be studied using CMSSW_4_2_8, a version of the CMSSW software that used to run under Scientific Linux CERN 5 (slc5) operating system. Likewise, open data from our 2011/2012 release used CMSSW_5_3_32 under Scientific Linux CERN 6 (slc6). The virtual machines that are used to analyze these data, therefore, need to consider all these compatibility subtleties. Virtual machine images \u00b6 In practical terms, a virtual machine image is a computer file that has all the right ingredients to create a virtual computer inside a given host. This file, however, needs to be decoded by a virtual machine interpreter, usually known as hypervisor , which runs on the host machine. One of the most famous hypervisors is Oracle's VirtualBox . CMS virtual images \u00b6 The most current images for CMS open data usage are described separately in the CERN Open Portal site for 2010 and 2011/2012 . They come equiped with the ROOT framework, CMSSW and CVMFS access. Remember When installing a CMS virtual machine (following the instructions below), always use the latest image file available for 2010 or 2011/2012 data. Installation \u00b6 Detailed instructions on how to install the CERN virtual machines can be found in the 2010 and 2011/2012 virtual machine installation guides from the CERN Open Portal. Choose the one to follow depending on the data release you will be working on. In summary, the basic steps are as follows: Download and install the latest (or even better, the latest tested) version of VirtualBox . Note that it is available for an ample range of platforms. Download the latest CMS virtual image file. Choose between 2010 or 2011/2012 , depending on the data release of interest. Once downloaded, import the image file into VirtualBox. Remember Always use the latest image file available for 2010 or 2011/2012 . Older ones are usually deprecated. Test the environment; again, 2010 or 2011/2012 , depending on the release. Finally, check for any known issues or limitations ( 2010 , 2011/2012 .)","title":"Virtual Machines"},{"location":"tools/virtualmachines/#virtual-machines","text":"CMS open data and legacy data, even though still exciting and full of potential, are already a few years old. Because of the rapidly evolving technolgies, the computing environments that were used to analyze these data are already ancient compared to the current, bleeding edge ones. Therefore, in order to mantain our ability to study these data, we have to rely on technologies that help us preserve adequate computer environments. One way of doing this is by using virtual machines. In simple words, a virtual machine is an emulation of a computer system that can run within another system. The latter is usually known as the host .","title":"Virtual machines"},{"location":"tools/virtualmachines/#open-data-releases-cmssw-versions-and-operating-systems","text":"CMS open data from our 2010 release can be studied using CMSSW_4_2_8, a version of the CMSSW software that used to run under Scientific Linux CERN 5 (slc5) operating system. Likewise, open data from our 2011/2012 release used CMSSW_5_3_32 under Scientific Linux CERN 6 (slc6). The virtual machines that are used to analyze these data, therefore, need to consider all these compatibility subtleties.","title":"Open data releases, CMSSW versions and operating systems"},{"location":"tools/virtualmachines/#virtual-machine-images","text":"In practical terms, a virtual machine image is a computer file that has all the right ingredients to create a virtual computer inside a given host. This file, however, needs to be decoded by a virtual machine interpreter, usually known as hypervisor , which runs on the host machine. One of the most famous hypervisors is Oracle's VirtualBox .","title":"Virtual machine images"},{"location":"tools/virtualmachines/#cms-virtual-images","text":"The most current images for CMS open data usage are described separately in the CERN Open Portal site for 2010 and 2011/2012 . They come equiped with the ROOT framework, CMSSW and CVMFS access. Remember When installing a CMS virtual machine (following the instructions below), always use the latest image file available for 2010 or 2011/2012 data.","title":"CMS virtual images"},{"location":"tools/virtualmachines/#installation","text":"Detailed instructions on how to install the CERN virtual machines can be found in the 2010 and 2011/2012 virtual machine installation guides from the CERN Open Portal. Choose the one to follow depending on the data release you will be working on. In summary, the basic steps are as follows: Download and install the latest (or even better, the latest tested) version of VirtualBox . Note that it is available for an ample range of platforms. Download the latest CMS virtual image file. Choose between 2010 or 2011/2012 , depending on the data release of interest. Once downloaded, import the image file into VirtualBox. Remember Always use the latest image file available for 2010 or 2011/2012 . Older ones are usually deprecated. Test the environment; again, 2010 or 2011/2012 , depending on the release. Finally, check for any known issues or limitations ( 2010 , 2011/2012 .)","title":"Installation"}]}