{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CMS Open Data Guide","text":"<p>Warning</p> <p>This guide is under construction</p> <p>Welcome to the guide for CMS open data.  This guide is brought to you by the CMS open data group, on a best-effort basis. All software and instructions are provided \"as is\", without warranty of any kind. This is ongoing work and we appreciate your feedback and/or your help building this guide.</p>"},{"location":"#how-to-use-this-site","title":"How to use this site","text":"<p>The lefthand tabs will help you navigate the site.  If you click on each tab, it will expand to show further subsections.  The sections will guide you through the main topics you will need to become familiar with to conduct an analysis using CMS Open Data.  You'll learn about the computing tools needed to deal with CMS open data and about CMSSW, which is the software used by CMS.  You'll also learn how to conduct a particle physics analysis.</p>"},{"location":"#the-sites-philosophy","title":"The site's philosophy","text":"<p>This site is thought as a navigation aid.  The CMS Collaboration has built an extensive amount of documentation over the years.  However, given the nature of our rapidly evolving research activities, this documentation is usually scattered around, which makes it difficult to navigate.  The main goal of this guide, therefore, is to facilitate the usage of CMS open/legacy data by providing a structured set of instructions that agglutinate those pieces of information already available in other sites.  In this sense, we do not pretend to copy every little piece of information and/or code, but to help you get to it and find your way around it.</p> <p>For CMS open data the three main sources of documentation/information are:</p> <ul> <li> <p>The CMS public Twiki pages.  Particularly the workbook and the software guide</p> <p>Note</p> <p>When accessing the CMS twiki pages we will usually point you to the most recent page.  However, historical Twiki documentation, i.e., earlier revision of the pages, may provide more accurate information for open data that is already a few years old. One can access this historical archive by going to the bottom of any Twiki page, clicking on History and exploring the revisions closer to the open data release year.</p> </li> <li> <p>The CERN CMS Open Portal pages.  This portal is not exactly meant to archive documentation.  It is mainly a repository for our open data.  However, it does host important information that is not so easy to find.  This guide will point you to the right pages.</p> </li> <li> <p>The CMSSW code.  Although less conventional, exploring the CMSSW code could be a really good source of information.  For instance, having hundreds of trigger bits, if the information from a specific module used in a specific trigger (with which data was taken) was needed, it would be impossible to document that explicitly in some guide. Instead, one can explore the code and easily find out the needed information. We will try to show you how it is done.</p> </li> </ul>"},{"location":"#how-to-get-help","title":"How to get help","text":"<p>The best way to get additional help is to visit our open data forum.</p>"},{"location":"#how-to-contribute-or-contact-us","title":"How to contribute or contact us","text":"<p>Please follow these instructions if you would like to contribute.</p> <p>If you find bugs or have suggestions or recommendations to improve this guide, please fill out an issue or contact us.</p>"},{"location":"about/","title":"About","text":"<p>This is the guide for CMS open data.  All CMS instructional material is made available under the Creative Commons Attribution license. This guide is brought to you by the CMS open data group, on a best-effort basis. All software and instructions are provided \"as is\", without warranty of any kind. This is ongoing work and we appreciate your feedback and/or your help building this guide.</p>"},{"location":"about/#contributors","title":"Contributors","text":"<ul> <li>Matt Bellis</li> <li>Edgar Carrera</li> <li>Kati Lassila-Perini</li> <li>Tibor \u0160imko</li> <li>Marco Vidal Garc\u00eda</li> <li>Audrius Mecionis</li> <li>Anniina Kinnunen</li> <li>Allan Jales</li> </ul>"},{"location":"about/#contact","title":"Contact","text":"<p>Please contact us here.</p>"},{"location":"faq/","title":"FAQ","text":"<p>Warning</p> <p>This page is under construction</p> <p>Frequently Asked Questions and other problems and issues that have come up.</p> <p>Possible subsections below</p>"},{"location":"faq/#high-level-questions","title":"High-level questions","text":""},{"location":"faq/#why-would-i-choose-virtualbox-over-docker-why-would-i-choose-docker-over-virtualbox","title":"Why would I choose VirtualBox over docker? Why would I choose docker over VirtualBox?","text":"<p>Great question! Anyone?</p>"},{"location":"faq/#docker","title":"Docker","text":""},{"location":"faq/#docker-downloads-container-but-never-launches-environment","title":"Docker downloads container but never launches environment","text":"<p>This is an issue with newer OSs on your local laptop/desktop running older OSs in the container.</p> <p>For example, suppose you are following the Running CMS analysis code using Docker tutorial. If you run</p> <pre><code>docker run --name opendata -it cmsopendata/cmssw_5_3_32 /bin/bash\n</code></pre> <p>and the container downloads but you don't find yourself in the <code>CMSSW_5_3_32</code> environment, then...</p>"},{"location":"faq/#data","title":"Data","text":""},{"location":"faq/#cmssw","title":"CMSSW","text":""},{"location":"analysis/backgrounds/","title":"Background Modelling Techniques","text":"<p>Warning</p> <p>This page is under construction</p> <p>Accurate modeling of SM background processes is essential to most searches and measurements in high energy physics. The dominant background processes depend strongly on the selected objects, particularly leptons, missing transverse momentum, and b quark jets or jets from boosted particles. Background estimation strategies are always tailored to an individual analysis, typically as a variation of one or more of the following common methods.</p>"},{"location":"analysis/backgrounds/#simulation","title":"Simulation","text":"<p>Many SM processes are simulated at NLO, which provides a strong basis for background estimation. For processes such as W/Z+jets production that are often simulated at LO, k-factor calculations allows one to weight simulated events to reproduce predictions of distributions at NLO. In final states with charged leptons, for which QCD multijet production is unlikely to be a significant background, simulation is a common choice. Additionally, the majority of searches utilize simulation to model the signal process under consideration.</p> <p>Simulated events are weighted so that the efficiencies of certain selections in simulation match those observed in data. These corrections are referred to as ``scale factors''. Common scale factors in searches at the CMS experiment correct for differences in :</p> <ul> <li>the number of pileup interactions</li> <li>the efficiencies of trigger selections</li> <li>the efficiencies of charged lepton identification and isolation selection criteria</li> <li>the efficiencies of various jet identification selection criteria, such as heavy-flavor tagging.</li> </ul> <p>A detailed set of corrections for the jet energy scale and resolution are computed for simulated events so that the response of the jet reconstruction algorithms is consistent between observed data and simulation. Searches may also develop correction formulas to correct for observed mismodeling of data by simulation in certain distributions of interest. A common correction of this type is to reweight the reconstructed top quark \\pt spectrum, since the NLO top quark pair simulations tend to overpredict the rate of high-\\pt top quark pairs. Each correction applied to simulation carries an uncertainty that should be taken into account in the statistical methods of signal extraction.</p>"},{"location":"analysis/backgrounds/#tight-loose-or-matrix-methods","title":"Tight / loose or \"Matrix\" methods","text":"<p>Searches that select multiple charged leptons often have considerable background from events in which nonprompt leptons are selected. Nonprompt leptons are usually charged leptons that arise from sources other than the hard scatter or decays of massive particles produced in the hard scatter.</p> <p>One method to estimate contributions from these events is to measure how often known prompt leptons, typically from the decay of Z bosons, and known nonprompt leptons, typically from a sample of QCD multijet events, pass a certain set of lepton selection criteria. A Z boson sample is created in data by selecting events with two same-flavor opposite-sign leptons whose mass lies very close to the Z boson mass. One lepton, known as the tag, is selected using very high-purity selection criteria, giving confidence that the other probe lepton is indeed a prompt lepton. The efficiency for the probe lepton to pass any criteria of interest can then be measured in this sample (learn more about this calculation on the tag and probe page). In the context of this background estimation method, the efficiency of the analysis selection criteria is referred to as the prompt rate, \\(p\\).</p> <p>A QCD multijet sample can be created by selecting events that pass a low-momentum, low-purity, single-lepton trigger, but otherwise exhibit no strong signs of the lepton arising from a SM boson decay. The rate at which these leptons pass the analysis selection criteria can be measured, and is referred to as the nonprompt rate (or colloquially, fake rate), \\(f\\). Both of these rates describe how often either prompt or nonprompt leptons that pass some baseline loose selection also pass the tight selection criteria used in the analysis.</p> <p>For searches that probe final states with two charged leptons, the probabilities for any prompt or nonprompt lepton to enter the sample must be considered together to develop a background distribution. The number of events with leptons passing the tight and/or loose criteria may be observed, in particular the number of events with two tight leptons, \\(N_{tt}\\); one tight and one loose lepton, \\(N_{tl}\\); and two loose leptons, \\(N_{ll}\\). The prompt and nonprompt rates may then be used to convert those observations into numbers of events with two prompt leptons, \\(N_{pp}\\); one prompt and one nonprompt lepton, \\(N_{pf}\\); and two nonprompt leptons, \\(N_{ff}\\).</p> \\[ \\begin{pmatrix}   N_{tt} \\\\   N_{tl} \\\\   N_{ll} \\end{pmatrix} = \\left( \\begin{array}{ccc}   p^2 &amp; pf &amp; f^2 \\\\   2p(1-p) &amp; f(1-p) + p(1-f) &amp; 2f(1-f) \\\\   (1-p)^2 &amp; (1-p)(1-f) &amp; (1-f)^2 \\end{array} \\right) \\begin{pmatrix}   N_{pp}\\\\   N_{pf}\\\\   N_{ff} \\end{pmatrix} \\] <p>A matrix inversion provides formulas to calculate \\(N_{pf}\\) and \\(N_{ff}\\) from the observed number of events with leptons of varying quality. For a search selecting two tight leptons, the background from events with nonprompt leptons will be given by \\(N_{\\mathrm{bkg}} = pfN_{pf} + f^2N_{ff}\\). This method can be extended to searches targeting final states more than two charged leptons by expanding the probability matrix.</p> <p>A good reference for this method, built on earlier uses within CMS, is the 2022 doctoral thesis of Wing Yan Wong.</p>"},{"location":"analysis/backgrounds/#transfer-factors","title":"Transfer factors","text":"<p>In many searches, one important selection criterion is the primary dividing line between a background-dominated control region (CR) and a region with good signal sensitivity, called the signal region (SR). A transfer factor or transfer function that describes the efficiency of this principle selection criteria can be derived and applied to the observed data in the CR in order to estimate the background present in the SR.</p>"},{"location":"analysis/backgrounds/#alpha-ratio-method","title":"Alpha-ratio method","text":"<p>The transfer function can be computed in multiple ways. Some searches use simulation for this purpose, in which case the method is often called the alpha-ratio method. The number of background events in the SR, \\(N_{\\mathrm{SR}}^{bkg}\\), is calculated as:</p> \\[ N_{\\mathrm{SR}}^{bkg} = N_\\mathrm{CR}^{data} \\times \\frac{N_{\\mathrm{SR}}^{sim}}{N_{\\mathrm{CR}}^{sim}}, \\] <p>where \\(N_\\mathrm{CR}^{data}\\) is the number of observed collision events in the CR, \\(N_{\\mathrm{SR}}^{sim}\\) is the number of simulated events in the SR, and \\(N_{\\mathrm{CR}}^{sim}\\) is the number of simulated events in the CR. The transfer factor from simulation can be computed in any bin of an observable, so the shape as well as the rate of background in the SR may be obtained.</p>"},{"location":"analysis/backgrounds/#abcd-method","title":"ABCD method","text":"<p>Other searches measure transfer factors using observed data in selection regions that are distinct from the primary SR and CR, in which case the method might be referred to as the ABCD method. This method is particularly popular for dealing with multijet backgrounds that are not typically modelled well by simulation.</p> <p>Four selection regions in the observed data are involved, formed by events either passing or failing either of two selection criteria, as shown in the graphic below. The number of background events in the SR (region C), \\(N_\\mathrm{C}\\), is calculated from observations in regions A, B, and D as \\(N_\\mathrm{D} \\times (N_\\mathrm{B} / N_\\mathrm{A})\\). This method may also be used in any bin of an observable to obtain a shape-based prediction for the background. In general, the ABCD method requires that the selection criteria are statistically independent in order to produce unbiased predictions.</p> <p></p> <p>If some background sources are well-modelled by simulation, these contributions may be subtracted from the observed data in each region before computing or applying the transfer function. More than four regions can be used to incorporate a method for validation into the procedure, as shown in the second graphic. The number of background events in the validation region X is estimated from the observations in regions A, D, and Y as \\(N_\\mathrm{D} \\times (N_\\mathrm{Y} / N_\\mathrm{A})\\), and if region X has a suitably low rate of expected signal events the observed data in this region could be compared to the background prediction, to test the validity of the prediction method.</p> <p></p>"},{"location":"analysis/backgrounds/#sideband-fits","title":"Sideband fits","text":"<p>In many searches, the observable most sensitive to the signal is a reconstructed mass or jet mass distribution, in which the signal is expected to be resonant while the dominant background processes are non-resonant. The shape of the background distribution may then be predicted by fitting a smooth functional form to the observed data on either side of the region in which the signal distribution is expected to peak. This method may be used in multiple dimensions for signals that feature more than one resonance.</p> <p>When multiple functional forms offer adequate fits to the observed data, an F-statistic may be used to compare the residual sums of squares for two formulas and determine whether a formula with more parameters provides a significantly better fit than an alternate formula with fewer parameters (known as the Fischer \\(\\mathcal{F}\\)-test).</p>"},{"location":"analysis/lumi/","title":"Luminosity","text":"<p>Warning</p> <p>This page is under construction</p>"},{"location":"analysis/stats/","title":"Statistics","text":"<p>Warning</p> <p>This page is under construction</p>"},{"location":"analysis/stats/#overview-of-cms-techniques","title":"Overview of CMS techniques","text":"<p>CMS searches typically determine an observable or set of observables that is used to measure the potential presence of signal events. This can be any observable, preferably highlighting unique features of the signal process. Signal extraction is based on maximum likelihood fits that compare ``data'' (either collision data or pseudodata sampled from a test distribution) to the signal (\\(s\\)) and background (\\(b\\)) predictions, with signal scaled by some unknown ratio \\(\\mu\\). The likelihood is assumed to follow a Poisson distribution, and all predictions are subject to various nuisance parameters, \\(\\theta\\), that are given default values \\(\\tilde{\\theta}\\) and assigned probability density functions (\\(p\\)). The likelihood function can be written as:</p> \\[ \\mathcal{L}(\\mathrm{data}\\vert \\mu,\\theta) = \\mathrm{Poisson}(\\mathrm{data}\\vert \\mu\\cdot s(\\theta) + b(\\theta))\\cdot p(\\tilde{\\theta}\\vert\\theta). \\] <p>Systematic uncertainties are incorporated into the fit as nuisance parameters. Lognormal probability distributions are assigned to uncertainties that affect only the normalization of a histogram or rate of a predicted event yield, and Gaussian probability distributions are typically assigned to uncertainties provided as histograms that affect the shape of a distribution. You can learn about several typical sources of uncertainty in CMS analyses in the Systematics section of the Guide.</p> <p>Observed and expected limits on the signal ratio \\(\\mu\\) are extracted by comparing the compatibility of the observed data with a background-only (\\(\\mu = 0\\)) hypothesis as well as with a signal+background hypothesis. The most common statistical method within CMS is the CLs method (Read, 2002 and Junk, 1999), which can be used to obtain a limit at the 95% confidence level using the profile likelihood test statistic (Cowan, 2010) with the asymptotic limit approximation.</p> <p>The \"Higgs Combine\" software framework used by the CMS experiment to compute limits is built on the RooFit and RooStats packages and implements statistical procedures developed for combining ATLAS and CMS Higgs boson measurements.</p>"},{"location":"analysis/stats/#tutorials","title":"Tutorials","text":"<p>Many tutorials and lectures on statistical interpretation of LHC data are available online. Some selected highlights are listed here.</p> <ul> <li> <p>Practical Statistics for LHC Physicists, a set of three lectures by Prof. Harrison Prosper, 2015. Slides and videos are available for each lecture:</p> <ul> <li>Descriptive Statistics, Probability and Likelihood</li> <li>Frequentist Inference</li> <li>Bayesian Inference</li> </ul> </li> <li> <p>Higgs Combine tutorial on the main features of Combine.</p> <ul> <li>Solutions are available</li> <li>Note: some links within this tutorial point to CMS internal resources.</li> </ul> </li> <li> <p>Open Data Workshop Simplified Run 2 Analysis lesson</p> <ul> <li>Lessons from the Open Data Workshop series use the docker container environment recommended for processing Open Data.</li> <li>The overall lesson offers tools for analysis of files in the NanoAOD or PhysObjectExtractorTool format.</li> <li>Specifically, the final page of the lesson (5: Systematics and Statistics) introduces the python-based tool pyhf for performing statistical inference without any ROOT software.</li> </ul> </li> </ul>"},{"location":"analysis/datasim/collisiondata/","title":"Collision Data","text":"<p>Warning</p> <p>This page is under construction</p> <p>The CMS collision data is organized in primary datasets (PD). All CMS open data primary datasets can be found with this search.</p> <p>The dataset name consists of three parts separated by \"/\", e.g.:</p> <p><code>/TauPlusX/Run2011A-12Oct2013-v1/AOD</code></p> <p>The first part indicates the primary dataset contents (<code>TauPlusX</code>), the second part is the data-taking era (<code>Run2011A</code>) and reprocessing (<code>12Oct2013</code>), and the last one indicates the data format (<code>AOD</code>).</p>"},{"location":"analysis/datasim/collisiondata/#dataset-contents","title":"Dataset contents","text":"<p>The primary dataset definition is centered around physics objects (SingleMu, Jet, Tau etc). Events triggered by High Level Triggers  (HLT) with a similar physics contents or use are mostly directed in the same PD. This guide gives an overview of the CMS trigger system. Besides requirements on the physics content, the organisation of the primary datasets has to satisfy constraints related to the data processing and handling, such as the average event rate approximately uniform across the different PDs, and the event rate within a certain range.</p> <p>Each CMS collision dataset comes with a brief description of the contents, and the full listing of all possible HLT trigger streams included in the dataset. The instructions how to find the exact definitions and parameters of the HLT trigger definitions can be found in Guide to the CMS Trigger System under \"HLT Trigger Path definitions\".</p> <p>Since a given event can pass more than one HLT path, it can be included in more than one primary dataset. There's an overall overlap between the PDs of around 25-35% during Run1 and it must be taken into account when combining events from different datasets in an analysis.</p>"},{"location":"analysis/datasim/collisiondata/#data-taking-and-reprocessing","title":"Data taking and reprocessing","text":"<p>One year of data taking is divided in several \"eras\" indicated as RunA, RunB, etc. According to the CMS data policy, 50% of data is published after the embargo period, completed with the full release within 10 years. For proton-proton data, currently available are</p> <ul> <li>Run2010A and Run2010B</li> <li>Run2011A and Run2011B</li> <li>Run2012A, Run2012B, Run2012C and Run2012D</li> <li>Run2015D</li> </ul> <p>In addition, heavy-ion data from HIRun2010 and HIRun2011 are available.</p> <p>The data are reprocessed several times, and it is the last complete reprocessing available at the time of the release which is made public.</p>"},{"location":"analysis/datasim/collisiondata/#data-format","title":"Data format","text":"<p>The data format in use for Run1 data is Analysis Object Data (AOD). Starting from Run2, a slimmer version of this format called MINIAOD is used. A brief description of data formats can be found in the introductory About CMS under \"Primary and simulated datasets\".</p>"},{"location":"analysis/datasim/collisiondata/#references","title":"References","text":"<p>G. Franzoni: Dataset definition for CMS operations and physics analyses CR2014_311.pdf</p>"},{"location":"analysis/datasim/eventgeneration/","title":"Event Generation","text":"<p>Warning</p> <p>This page is under construction</p> <p>Physical event generation and detector simulation are the first steps in producing Monte Carlo samples suitable for physical analysis. Here we will teach you how to use the CMS datasets in the CERN Open Data Portal and the CMSSW machinery for the generation of events in simple steps:</p> <ol> <li>Generation and Simulation: To simulate beam collisions.</li> <li>Triggers: To simulate the effect of the detectors and electronics.</li> <li>Reconstruction: For the reconstruction of the events in the collisions.</li> </ol> <p>What you will find here:</p> <ul> <li>Virtual machines </li> <li>Dataset name</li> <li>System details</li> <li>Configuration files</li> <li>cmsDriver</li> <li>Generation from Matrix Element (ME) generators<ul> <li>LHE</li> <li>Simulation</li> <li>High Level Trigger (HLT)</li> <li>Reconstruction</li> </ul> </li> <li>Generation from general-purpose generators<ul> <li>Generation and Simulation</li> <li>High Level Trigger (HLT)</li> <li>Reconstruction</li> </ul> </li> <li>Example for event generation with 2011 CMSSW machinery</li> <li>Example for event generation with 2012 CMSSW machinery</li> </ul> <p></p>"},{"location":"analysis/datasim/eventgeneration/#virtual-machines","title":"Virtual machines","text":"<p>A specific CMS virtual machine includes the ROOT framework and CMSSW. Follow these instructions to configure a CERN virtual machine on your computer to be used with the 2011 and 2012 CMS open data.</p> <p></p>"},{"location":"analysis/datasim/eventgeneration/#dataset-name","title":"Dataset name","text":"<p>When exploring a simulated dataset on the CERN Open Data Portal, the first thing you will see is the name of the dataset. CMS uses the following naming convention: <pre><code>PROCESS_RANGETYPE-RANGELOWtoRANGEHIGH_FILTER_TUNE_COMMENT_COMENERGY-GENERATOR\n</code></pre></p> <p>Take as an example the name of record 12201: <pre><code>QCD_Pt-15to3000_TuneZ2star_Flat_8TeV_pythia6\n</code></pre></p> <p></p>"},{"location":"analysis/datasim/eventgeneration/#system-details","title":"System details","text":"<p>In the record of each dataset, you can find the recommended global tag and release for analysis (CMSSW is the data analysis library). A global tag stores additional data that is required by the reconstruction and analysis software. Take as an example section System details of record 12201:</p> <pre><code>Recommended global tag for analysis: START53_V27\nRecommended release for analysis: CMSSW_5_3_32\n</code></pre> <p></p>"},{"location":"analysis/datasim/eventgeneration/#configuration-files","title":"Configuration files","text":"<p>The CMS software framework uses a software bus model, where data is stored in the event which is passed to a series of modules. A single executable, <code>cmsRun</code>, is used, and the modules are loaded at runtime. A configuration file defines which modules are loaded, in which order they are run, and with which configurable parameters they are run.</p> <p>You can find the configuration files for the generation of events for each dataset in its respective record within the CERN Open Data Portal. Check, for example, the section How were these data generated? of record 12201.</p> <p></p>"},{"location":"analysis/datasim/eventgeneration/#cmsdriver","title":"cmsDriver","text":"<p>The cmsDriver is a tool to create production-solid configuration files from minimal command line options.  Its code implementation, the cmsDriver.py script, is part of the CMSSW software.  </p> <p>A summary of the <code>cmsDriver.py</code> script's options with a detailed message about each one can be visualized by getting the help: <pre><code>cmsDriver.py --help\n</code></pre></p> <p></p>"},{"location":"analysis/datasim/eventgeneration/#generation-from-matrix-element-me-generators","title":"Generation from Matrix Element (ME) generators","text":"<p>Generator-level datasets can be produced using a Matrix Element (ME) generator (e.g., Powheg, MadGraph5_aMCatNLO, Alpgen) to deliver the event at the parton level and then a general-purpose generator to hadronise the event.</p> <p>Here we will reproduce the steps in the generation of record 1352.</p> <p>Guided by the system details specified in the dataset, you should start by setting up your run time environment: <pre><code>cmsrel CMSSW_5_3_32\ncd CMSSW_5_3_32/src/\ncmsenv\n</code></pre></p> <p>We will create a package according to our dataset: <pre><code>mkdir MyPackage\ncd MyPackage\nmkedanlzr MySim\n</code></pre></p> <p></p>"},{"location":"analysis/datasim/eventgeneration/#lhe","title":"LHE","text":"<p>The Les Houches Event file format (LHE) is an agreement between Monte Carlo event generators and theorists to define Matrix Element level event listings in a common language.</p> <p>The LHE input file that store process and event information can be one generated by you or you can look for examples in <code>/eos/cms/store/lhe/</code>. Here we will use a file with events generated for record 1352:</p> <pre><code>cmsDriver.py step1 --filein lhe:10270 --fileout file:LHE.root --mc --eventcontent LHE --datatier GEN --conditions START53_LV6A1::All --step NONE --python_filename LHE.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3\n</code></pre> <p>Run the CMSSW executable: <pre><code>cmsRun LHE.py\n</code></pre></p> <p></p>"},{"location":"analysis/datasim/eventgeneration/#simulation","title":"Simulation","text":"<p>The next step is to generate fully hadronised events. We need to use the appropriate configuration file for this purpose. Take as an example the file in Step SIM for the simulation of record 1352. The configuration file is in this link.</p> <p>We add this file to our local area: <pre><code>curl http://uaf-10.t2.ucsd.edu/~phchang/analysis/generator/genproductions/python/SevenTeV/Hadronizer_TuneZ2_7TeV_generic_LHE_pythia_tauola_cff.py -o MySim/python/mysim.py\n</code></pre></p> <p>Compile everything: <pre><code>scram b\n</code></pre></p> <p>Execute the <code>cmsDriver</code> command as: <pre><code>cmsDriver.py MyPackage/MySim/python/mysim.py --filein file:LHE.root --fileout file:sim.root --mc --eventcontent RAWSIM --customise SimG4Core/Application/reproc2011_2012_cff.customiseG4,Configuration/DataProcessing/Utils.addMonitoring --datatier GEN-SIM --conditions START53_LV6A1::All --beamspot Realistic7TeV2011CollisionV2  --step GEN,SIM --datamix NODATAMIXER --python_filename sim.py --no_exec -n 3\n</code></pre></p> <p>Run the CMSSW executable: <pre><code>cmsRun sim.py\n</code></pre></p> <p></p>"},{"location":"analysis/datasim/eventgeneration/#high-level-trigger-hlt","title":"High Level Trigger (HLT)","text":"<p>It is a crucial part of the CMS data flow since it is the HLT algorithms and filters which will decide whether an event should be kept for an offline analysis: any offline analysis depends on the outcome of HLT.</p> <p>Execute the <code>cmsDriver</code> command as: <pre><code>cmsDriver.py step1 --filein file:sim.root --fileout file:hlt.root --mc --eventcontent RAWSIM --runsScenarioForMC Run2012_AB_C_D_oneRunPerEra --datatier GEN-RAW --conditions START53_LV6A1::All --step DIGI,L1,DIGI2RAW,HLT:2011 --python_filename hlt.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3\n</code></pre></p> <p>Now, run the CMSSW executable: <pre><code>cmsRun hlt.py\n</code></pre></p> <p></p>"},{"location":"analysis/datasim/eventgeneration/#reconstruction","title":"Reconstruction","text":"<p>The algorithms that make up the CMS event reconstruction software build physics objects (e.g., muons, electrons, jets) from the raw data recorded by the detector. All events collected by the CMS trigger system are reconstructed by the CMS prompt reconstruction system soon after being collected.</p> <p>Execute the <code>cmsDriver</code> command as: <pre><code>cmsDriver.py step2 --filein file:hlt.root --fileout file:reco.root --mc --eventcontent AODSIM,DQM --datatier AODSIM,DQM --conditions START53_LV6A1::All --step RAW2DIGI,L1Reco,RECO,VALIDATION:validation_prod,DQM:DQMOfflinePOGMC --python_filename reco.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3\n</code></pre></p> <p>Now, run the CMSSW executable: <pre><code>cmsRun reco.py\n</code></pre></p> <p>You can start ROOT and type <code>TBrowser t</code> to explore the files that were created.</p> <p></p>"},{"location":"analysis/datasim/eventgeneration/#generation-from-general-purpose-generators","title":"Generation from general-purpose generators","text":"<p>Generator-level datasets can be produced using a general-purpose generator (e.g., Pythia, Herwig, Tauola) to simulate the event and the hadronisation.</p> <p>Here we will reproduce the steps in the generation of record 12201.</p> <p>Guided by the system details specified in the dataset, you should start by setting up your run time environment: <pre><code>cmsrel CMSSW_5_3_32\ncd CMSSW_5_3_32/src/\ncmsenv\n</code></pre></p> <p>We will create a package according to our dataset: <pre><code>mkdir MyPackage\ncd MyPackage\nmkedanlzr MyGen\n</code></pre></p> <p></p>"},{"location":"analysis/datasim/eventgeneration/#generation-and-simulation","title":"Generation and Simulation","text":"<p>We need to use the appropriate configuration file. Take as an example the file in Step SIM for the generation and simulation of record 12201. The configuration file is in this link.</p> <p>We add this file to our local area: <pre><code>curl https://raw.githubusercontent.com/cms-sw/genproductions/master/python/EightTeV/QCD_Pt/QCD_Pt_15to3000_TuneZ2star_Flat_8TeV_pythia6_cff.py -o MyGen/python/mygen.py\n</code></pre></p> <p>Compile everything: <pre><code>scram b\n</code></pre></p> <p>Execute the <code>cmsDriver</code> command as: <pre><code>cmsDriver.py MyPackage/MyGen/python/mygen.py --fileout file:gen.root --mc --eventcontent RAWSIM --pileup NoPileUp --customise Configuration/StandardSequences/SimWithCastor_cff.customise,Configuration/DataProcessing/Utils.addMonitoring --datatier GEN-SIM --conditions START50_V13::All --beamspot Realistic8TeVCollision --step GEN,SIM --datamix NODATAMIXER --python_filename gen.py --no_exec -n 3\n</code></pre></p> <p>Run the CMSSW executable: <pre><code>cmsRun gen.py\n</code></pre></p> <p></p>"},{"location":"analysis/datasim/eventgeneration/#high-level-trigger-hlt_1","title":"High Level Trigger (HLT)","text":"<p>Execute the <code>cmsDriver</code> command as: <pre><code>cmsDriver.py step1 --filein file:gen.root --fileout file:hlt.root --pileup_input dbs:/MinBias_TuneZ2star_8TeV-pythia6/Summer12-START50_V13-v3/GEN-SIM --mc --eventcontent RAWSIM --runsScenarioForMC Run2012_AB_C_D_oneRunPerEra --pileup fromDB --datatier GEN-SIM-RAW --conditions START53_V7N::All --step DIGI,L1,DIGI2RAW,HLT:7E33v2 --python_filename hlt.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3\n</code></pre></p> <p>In section How were these data generated? of the record, you can find the pile-up dataset. Additionally, you can manually add ROOT files to the <code>hlt.py</code>  file for the pile-up configuration by looking at the list of ROOT files that were used in the Step HLT configuration file of the record you are studying. This involves, for instance, opening file <code>hlt.py</code> and replacing the line</p> <p><pre><code>process.mix.input.fileNames = cms.untracked.vstring([])\n</code></pre> with <pre><code>process.mix.input.fileNames = cms.untracked.vstring([\n'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/005825F1-F260-E111-BD97-003048C692DA.root',\n'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/003EEBD4-8061-E111-9A23-003048D437F2.root', \n'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/0005E496-3661-E111-B31E-003048F0E426.root'])\n</code></pre></p> <p>Now, run the CMSSW executable: <pre><code>cmsRun hlt.py\n</code></pre></p> <p></p>"},{"location":"analysis/datasim/eventgeneration/#reconstruction_1","title":"Reconstruction","text":"<p>Execute the <code>cmsDriver</code> command as: <pre><code>cmsDriver.py step2 --filein file:hlt.root --fileout file:reco.root --mc --eventcontent AODSIM,DQM --datatier AODSIM,DQM --conditions START53_V7N::All --step RAW2DIGI,L1Reco,RECO,VALIDATION:validation_prod,DQM:DQMOfflinePOGMC --python_filename reco.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3\n</code></pre></p> <p>Now, run the CMSSW executable: <pre><code>cmsRun reco.py\n</code></pre></p> <p>You can start ROOT and type <code>TBrowser t</code> to explore the files that were created.</p> <p></p>"},{"location":"analysis/datasim/eventgeneration/#example-for-event-generation-with-2011-cmssw-machinery","title":"Example for event generation with 2011 CMSSW machinery","text":"<p>In this example, you will learn how to generate 2011 MC Drell-Yan events from scratch. A Drell-Yan process occurs when a quark and an antiquark annihilate, creating a virtual photon or Z boson, which then decays into a pair of oppositely charged leptons.</p> <p></p>"},{"location":"analysis/datasim/eventgeneration/#example-for-event-generation-with-2012-cmssw-machinery","title":"Example for event generation with 2012 CMSSW machinery","text":"<p>In this example, you will learn how to generate 2012 MC QCD events, which involve the strong interaction between quarks and gluons. Additionally, you will know what are the steps to extract the tracking information of these events.</p>"},{"location":"analysis/datasim/mcsimulations/","title":"Monte Carlo Simulations","text":"<p>A set of simulated data (Monte Carlo - MC) corresponding to the collision data is made available. All directly available MC datasets can be found with this search. For 2012 data taking, large amount of MC, thought to be of less frequent use, is available on demand and included in search results if \"include on-demand datasets\" option is selected.</p> <p>MC dataset are searchable by categories, which can be found under \"Filter by category\" on the left bar of the search page.</p> <p>The dataset name consists of three parts separated by <code>/</code> e.g.:</p> <p><code>/DYToMuMu_M-15To50_Tune4C_8TeV-pythia8/Summer12_DR53X-PU_S10_START53_V19-v1/AODSIM</code></p> <p>The first part indicates the simulated physics process (<code>DYToMuMu</code>), some of the production parameters (<code>M-15To50_Tune4C</code>), collision energy (<code>8TeV</code>),  and the event generator used in the processing chain. CMS simulated datasets names  gives more details in the naming.  The second part is the production campaign (<code>Summer12_DR53X</code>), pile-up  profile (<code>PU_S10</code>) and processing conditions (<code>START53_V19</code>),  and the last one indicates the data format (<code>AODSIM</code>).</p>"},{"location":"analysis/datasim/mcsimulations/#dataset-contents","title":"Dataset contents","text":"<p>The dataset naming reflects the contents of the dataset, and the actual generator parameters with which the dataset contents have been defined can be found as explained under \"Finding the generator parameters\" in the CMS Monte Carlo production overview.</p>"},{"location":"analysis/datasim/mcsimulations/#processing","title":"Processing","text":"<p>CMS Monte Carlo production overview briefly describes the steps in the MC production chain.</p>"},{"location":"analysis/datasim/mcsimulations/#data-format","title":"Data format","text":"<p>The data format in use for Run1 MC data is Analysis Object Data (AODSIM). Starting from Run2, a slimmer version of this format called MINIAODSIM is used. A brief description of data formats can be found in the introductory About CMS under \"Primary and simulated datasets\".</p>"},{"location":"analysis/datasim/mcsimulations/#cross-section-calculation","title":"Cross section calculation","text":"<p>Cross sections can be calculated for MC samples.</p> <p>Caveat: The cross-sections found with this tool are those predicted by the respective generators. There may be better estimates, coming from dedicated task forces, theory papers etc.</p> <p>To account for the different running conditions in Run 1 vs Run 2, click the appropriate tab below for Run 1 vs Run 2 data.</p> Run 1 DataRun 2 Data <ul> <li>This page is under construction</li> </ul> <ul> <li>First, fetch a CMSSW image and start a container. You can find a list of Docker container images available for CMS open data in the guide page for CMS open data containers. A tutorial on working with docker is at CMS open data containers. After you start your container, you will need the file ana.py, which you can access by curl</li> </ul> <pre><code>curl https://raw.githubusercontent.com/cms-sw/genproductions/master/Utilities/calculateXSectionAndFilterEfficiency/genXsec_cfg.py -o ana.py\n</code></pre> <ul> <li> <p>Next, you'll calculate a cross-section for a root file. You can identify the address of your root file by navigating to the CERN open data record. When you click the Download button at the bottom of the page, you'll get a printout of the path to your file. For example, a simulated dataset is available at Simulated dataset TGJets_TuneCUETP8M1_13TeV_amcatnlo_madspin_pythia8 in MINIAODSIM format for 2015 collision data. After clicking the Download button at the button of the page, you will be brought here, and you can copy the path of your file/files of interest to use to compute your cross-section.</p> </li> <li> <p>To compute a cross-section using one of the above files, type</p> </li> </ul> <pre><code>cmsRun ana.py inputFiles=\"root://eospublic.cern.ch//eos/opendata/cms/mc/RunIIFall15MiniAODv2/TGJets_TuneCUETP8M1_13TeV_amcatnlo_madspin_pythia8/MINIAODSIM/PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1/00000/1A454199-F8B8-E511-A55D-7845C4FC374C.root\" maxEvents=-1\n</code></pre> <ul> <li> <p>Caveat: in the above method, you directly access the file from your container. If instead your files are on your local system and you plan to copy them to your container to use them, note that you must modify the syntax to: <code>cmsRun ana.py inputFiles=\"file:xxxx.root\" maxEvents=-1</code> You must use the syntax \"file:\" before your root file name. For example, if your root file is called ttbar.root, you would type <code>cmsRun ana.py inputFiles=\"file:ttbar.root\" maxEvents=-1</code></p> </li> <li> <p>After running the above commands, you will get a log file. A sample printout is below and also available here: </p> </li> <li> <p>A cross-section summary will be printed out. The definition of each quantity is:</p> <ul> <li>Before matching: the cross section before jet matching and any filter</li> <li>After matching: the cross section after jet matching BUT before any filter</li> <li>Filter efficiency: the efficiency of the any filter.</li> <li>After filter: the cross section after jet matching and additional filter are applied. This is your final cross section.</li> </ul> </li> </ul>"},{"location":"analysis/interpretation/crossSections/","title":"Cross section calculation","text":"<p>Warning</p> <p>This page is under construction</p> <p>Cross sections can be calculated for MC samples.</p> <p>To account for the different running conditions in Run 1 vs Run 2, click the appropriate tab below for Run 1 vs Run 2 data.</p> Run 1 DataRun 2 Data <ul> <li>This page is under construction</li> </ul> <ul> <li> <p>You can calculate a cross section using the GenXSecAnalyzer. To use it, you will need the file ana.py, which you can get by <code>curl https://raw.githubusercontent.com/cms-sw/genproductions/master/Utilities/calculateXSectionAndFilterEfficiency/genXsec_cfg.py -o ana.py</code></p> </li> <li> <p>Next, fetch a CMSSW image and start a container. You can find a list of Docker container images available for CMS open data in the guide page for CMS open data containers. A tutorial on working with docker is at CMS open data containers. If you named your container <code>my_od</code>, you can fetch and start it by</p> </li> </ul> <pre><code>docker start -i my_od\n\ncd CMSSW_7_6_7/src\n</code></pre> <ul> <li>Then, copy the file ana.py to your container. </li> </ul> <pre><code>curl https://raw.githubusercontent.com/cms-sw/genproductions/master/Utilities/calculateXSectionAndFilterEfficiency/genXsec_cfg.py -o ana.py\n</code></pre> <ul> <li>To compute the cross-section, type <code>cmsRun ana.py inputFiles=\"file:xxxx.root\" maxEvents=-1</code> in case you have copied one file locally or <code>cmsRun ana.py inputFiles=\"root://eospublic.cern.ch//eos/opendata/cms/mc/[....].root\" maxEvents=-1</code> if you access the file through xrootd protocol from the CERN Open data portal. For example:</li> </ul> <pre><code>cmsRun ana.py inputFiles=\"root://eospublic.cern.ch//eos/opendata/cms/mc/RunIIFall15MiniAODv2/TGJets_TuneCUETP8M1_13TeV_amcatnlo_madspin_pythia8/MINIAODSIM/PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1/00000/1A454199-F8B8-E511-A55D-7845C4FC374C.root\" maxEvents=-1\n</code></pre> <ul> <li> <p>After running the above commands, you will get a log file.</p> </li> <li> <p>A cross-section summary will be printed out. The definition of each quantity is:</p> <ul> <li>Before matching: the cross section before jet matching and any filter</li> <li>After matching: the cross section after jet matching BUT before any filter</li> <li>Filter efficiency: the efficiency of the any filter.</li> <li>After filter: the cross section after jet matching and additional filter are applied. This is your final cross section.</li> </ul> </li> <li> <p>You can use any CMS Run2 Open Data MC sample for testing, the example above is from Simulated dataset TGJets_TuneCUETP8M1_13TeV_amcatnlo_madspin_pythia8 in MINIAODSIM format for 2015 collision data.</p> </li> </ul>"},{"location":"analysis/interpretation/limits/","title":"Upper-limit calculations","text":"<p>Warning</p> <p>This page is under construction</p>"},{"location":"analysis/selection/triggers/","title":"Triggers","text":"<p>Warning</p> <p>This page is under construction</p>"},{"location":"analysis/selection/validatedRuns/","title":"Validated Runs","text":"<p>Data recorded by CMS go through a validation process and are certified as good for physics analysis if all subdetectors, trigger, lumi and physics objects (tracking, electron, muon, photon, jet and MET) show the expected performance.</p> <p>Lists of validated runs and luminosity sections (the smallest unit of data taking, 23 seconds) are provided on the CERN open data portal.</p> <p>They are of format</p> <pre><code>{\n\"&lt;run number&gt;\":\n     [\n[ &lt;first certified luminosity section in a range&gt;, &lt;last certified luminosity section in a range&gt;\n       ],\n       ..\n</code></pre> <p>for example:</p> <pre><code>{\"190645\": [[10, 110]], \"190646\": [[1, 111]], \"190659\": [[33, 167]], \"190679\": [[1, 55]],\n \"190688\": [[69, 249]], \"190702\": [[51, 53], [55, 122], [124, 169]], \"190703\": [[1, 252]],\n \"190704\": [[1, 3]], ...\n</code></pre> <p>Each CMS open data record has a link to the corresponding list of validated runs, and it must be applied to all analyses. Most code examples expect that this list is downloaded to the working directory. In a CMSSW job, the filtering based on this list is applied by adding the following lines in the configuration file of the job</p> <pre><code>   import FWCore.ParameterSet.Config as cms\n   import FWCore.PythonUtilities.LumiList as LumiList\n   goodJSON = '&lt;file name here&gt;'\n   myLumis = LumiList.LumiList(filename = goodJSON).getCMSSWString().split(',') \n</code></pre> <p>and by adding these two lines after the <code>process.source</code> input file definition:</p> <pre><code>   process.source.lumisToProcess = cms.untracked.VLuminosityBlockRange()\n   process.source.lumisToProcess.extend(myLumis)\n</code></pre> <p>This list should also be used as an input to the luminosity calculation.</p>"},{"location":"analysis/selection/idefficiencystudy/signalextraction/","title":"Signal Extraction","text":"<p>Detector reconstruction efficiencies are calculated using signal muons, that is, only true candidates decaying to dimuons. This is achieved in this study by extracting signal from  the data by the usage of some methods. Here it is presented two: sideband subtraction and fitting.</p>"},{"location":"analysis/selection/idefficiencystudy/signalextraction/#sideband-subtraction-method","title":"Sideband subtraction method","text":"<p>The sideband subtraction method involves choosing sideband and signal regions in invariant mass distribution for each tag+probe pair. The signal region is selected by finding the ressonance position and defining a region around it. While the signal region contains both signal and background, the sideband region is chosen such as to have only background, with a distance from signal region. A example of those regions selection can be seen below for the J/\u03c8 ressonance.</p> <p></p> <p>For each event category (i.e. Pass and All), and for a given variable of interest (e.g., the probe pT), two distributions are obtained, one for each region (Signal and Sideband). In order to obtain the variable distribution for the signal only, we proceed by subtracting the Background distribution (Sideband region) from the Signal+Background one (Signal region):</p> <p></p> <p>Where the normalization \u03b1 factor quantifies the quantity of background present in the signal region:</p> <p></p> <p>And for the uncertainty:</p> <p></p> <p>Applying those equations we get histograms like this:</p> <p></p> <ul> <li>Solid blue line (Total) = particles in signal region;</li> <li>Dashed blue line (Background) = particles in sideband regions;</li> <li>Solid magenta line (signal) = signal histogram subtracted.</li> </ul>"},{"location":"analysis/selection/idefficiencystudy/signalextraction/#fitting-method","title":"Fitting method","text":"<p>In this method, the signal is extracted not by histogram manipulation but by likelihood fitting. The procedure is applied after splitting the data in sub-samples, corresponding to bins of the kinematic variable of interest of the probe objects. As such, the efficiency will be measured as a function of that variable. Each sub-sample contains signal and background events; the signal is accessed by fitting the invariant mass spectra</p> <p>The fit for each bin allows to statistically discriminate between signal and background. In particular, the fit yields the number of signal events. The efficiency is finally obtained by simply forming the ratio of the signal yield from the fit to the passing category by the signal yield from the fit of the inclusive all category. This approach is illustrated below.</p> <p></p>"},{"location":"analysis/selection/idefficiencystudy/tagandprobe/","title":"Tag and Probe","text":"<p>The Tag and Probe method is an experimental procedure commonly used in particle physics that allows to measure a process\u2019 efficiency directly from data. The procedure provides an unbiased sample of probe objects that can be then used to measure the efficiency of a particular selection criteria.</p>"},{"location":"analysis/selection/idefficiencystudy/tagandprobe/#tag-and-probe-method","title":"Tag and Probe method","text":"<p>This method is a data-driven technique and it is based on decays of known ressonances in pair of particles. The decaying muons are labeled according to the following criteria:</p> <ul> <li>Tag muon: well identified, triggered muon (tight selection criteria).</li> <li>Probe muon: unbiased set of muon candidates (very loose selection criteria), either passing or failing the criteria for which the eciency is to be measured.</li> </ul> <p>Tag muon are employed to trigger the presence of a resonance decay while probe muons, paired to tag muons, will be used for getting efficiency due its' unbiased characteristic.</p>"},{"location":"analysis/selection/idefficiencystudy/tagandprobe/#cms-efficiency","title":"CMS Efficiency","text":"<p>The efficiency will be given by the fraction of probe muons that pass a given criteria (in this case, the Muon ID which is explained below):</p> <p></p> <p>The denominator corresponds to the number of resonance candidates (tag+probe pairs) reconstructed in the dataset. The numerator corresponds to the subset for which the probe passes the criteria.</p>"},{"location":"analysis/selection/idefficiencystudy/tagandprobe/#cms-muon-identification-and-reconstruction","title":"CMS Muon identification and reconstruction","text":"<p>In the standard CMS reconstruction for proton-proton collisions, tracks are first reconstructed independently in the inner tracker and in the muon system. Based on these objects, three reconstruction approaches are used, Tracker Muon, Standalone Muon, Global Muon:</p> <p>These are illustrated below:</p> <p></p> <p>Note</p> <p>You can find more details concerning CMS Muon Identification and reconstruction in this paper JINST 7 (2012) P10002.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/","title":"Overview of fitting method","text":"<p>The fitting method folder is structured in folders and main files. Main files are those ones that are used to run the most important codes. Below is a list of folders presented in this method and the files encontered here. It is important to note that this code has been tested on root 6.22/00.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#fitting-method-folder-structure","title":"Fitting method folder structure","text":"<p>The folders contained in fitting method are described below.</p> Folder Purpose \ud83d\udcc2 DATA Where <code>.root</code> with data should be placed for measuring efficiency \ud83d\udcc2 src Where important files related to main code are keeped \u2514 \ud83d\udcc2 dofits Here it keeps files that are responsible to do the fitting over invariant masses histograms \ud83d\udcc2 tests Some teste made during the development of this tool \ud83d\udcc2 results This folder stores the results output and it is created when any code finnish running"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#main-files","title":"Main files","text":"<p>There are six main files in the fitting method. For simple results like the ones obtained in sideband subtraction method the file used is efficiency.cpp.</p> <p>Main files are explained below.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#simplify_datacpp","title":"\ud83d\udcc4 simplify_data.cpp","text":"<p>The <code>simplify_data.cpp</code> file, as the name sugest, simplify a DATA file obtained from this Tag and Probe tool. It is necessary to simplify due RooFit limitations where fitting method codes here used are based on.</p> <p>There are two lines responsable for input and output file:</p> <pre><code>TFile *file0  = TFile::Open(\"INPUT_FILE_PATH.root\");\n</code></pre> <pre><code>TFile *fileIO = TFile::Open(\"OUTPUT_FILE_PATH.root\",\"RECREATE\");\n</code></pre> <p>Every user should run this code firstly to simplify <code>.root</code> files in order to use it in fitting method.</p> <p>The input files here are provenient from the main Tag and Probe tool in this repository. If you want to get a new ntuple, you should run it.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#efficiencycpp","title":"\ud83d\udcc4 efficiency.cpp","text":"<p>This file is responsible to measure the efficiency simple by fitting method as described in this fitting method section.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#choosing-ressonance","title":"Choosing ressonance","text":"<p>Here it include the file that is responsible to fit the ressonance and return the yield obtained with error.</p> <pre><code>//Change if you need\n#include \"src/dofits/DoFit_Jpsi_Run.h\"\n</code></pre> <p>By default out tool keeps all ressonance fit in the folder <code>src/dofits</code>. There are some example there for specific ressonances and fits.</p> <p>There are two main parameters to control this code.</p> <pre><code>//Which Muon Id do you want to study?\nstring MuonId   = \"trackerMuon\";\n</code></pre> <p>The <code>string MuonId</code> supports <code>\"trackerMuon\"</code>, <code>\"standaloneMuon\"</code> and <code>\"globalMuon\"</code> values.</p> <pre><code>//Which quantity do you want to use?\nstring quantity = \"Pt\";     double bins[] = {0., 2.0, 3.4, 4.0, 4.4, 4.7, 5.0, 5.6, 5.8, 6.0, 6.2, 6.4, 6.6, 6.8, 7.3, 9.5, 13.0, 17.0, 40.};\n</code></pre> <p><code>string quantity</code> supports <code>\"Pt\"</code>, <code>\"Eta\"</code> and <code>\"Phi\"</code> values.</p> <p><code>double bins[]</code> is used to set histogram bins limits. In the example above, the first bin is [0., 2.), the second is [2., 4.) and so on.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#output","title":"Output","text":"<p>There are two output folders in this file by default. They are defined in those lines of code:</p> <pre><code>//Path where is going to save results png for every bin \nconst char* path_bins_fit_folder = \"results/bins_fit/efficiency/\";\n</code></pre> <p><code>path_bins_fit_folder</code> refers to the path where each individual fit of bins will be stored as <code>.png</code>. In this folder you can find every fit made in this method.</p> <pre><code>//Path where is going to save efficiency \nstring directoryToSave = string(\"results/efficiencies/efficiency/\") + output_folder_name + string(\"/\");\n</code></pre> <p>The <code>directoryToSave</code> stores the path to save the efficiency result. It is saved as a <code>.root</code> file containing passing and total histograms as well the efficiency result histogram.</p> <p>Informations about the output is printed at end of running.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#loop_over_efficienciescpp","title":"\ud83d\udcc4 loop_over_efficiencies.cpp","text":"<p>The purpose of this code is rerun the <code>efficiency.cpp</code> for differents configurations. This code is not recommended for systematic calculations indeed and it was firstly created for systematic studies only.</p> <p>The importants variables to keep in mind are listed below</p> Type Name Purpose double default_min the minimum invariant mass window postion double default_max the maximum invariant mass window postion bool should_loop_muon_id if true, it loops over all muons id (tracking, standalone, global) bool should_loop_settings if true, it loops over all settings presented in set_settings() function int setting if should_loop_settings is false, it uses only this setting number bool exactly This only affect the name of output plots inside <code>.root</code>. Its recommended to keep it set to false"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#set_settings","title":"set_settings(...)","text":"<p>It is one of four functions presented in this code. Its is called by:</p> <pre><code>void set_settings(int index, bool exactly = false)\n</code></pre> <p>Inside this function are preset settings that this file runs over. Each setting is associated with a number here named as index. This function is responsible to set the index configuration to the efficiency for running the <code>efficiency.cpp</code> file.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#loop_settings","title":"loop_settings()","text":"<pre><code>void loop_settings()\n</code></pre> <p>If <code>should_loop_muon_id</code> is true, this function is called. It loops over all muon ids: tracking, standalone, global.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#loop_muon_id","title":"loop_muon_id()","text":"<pre><code>void loop_muon_id()\n</code></pre> <p>If <code>should_loop_settings</code> is true, this function is called. It loops over all settings preset in <code>set_settings(...)</code> function.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#loop_over_efficiencies","title":"loop_over_efficiencies()","text":"<pre><code>void loop_over_efficiencies()\n</code></pre> <p>It is the main function of this file. It is the function which calls every other function when it is needed.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#plot_sys_efficiencycpp","title":"\ud83d\udcc4 plot_sys_efficiency.cpp","text":"<p>The <code>plot_sys_efficiency.cpp</code> code creates a single <code>.root</code> with variations made. Unlike the previous code, the <code>loop_over_efficiencies.cpp</code>, that makes each source of uncertainty be in a separate .root, this one puts all of them in a single <code>.root</code>. This code has been further optimized than his precursor and also as a differential it already calculates the systematic uncertainty. Below it is specified main variables used in this code.</p> <pre><code>//Which Muon Id do you want to study?\nstring MuonId   = \"trackerMuon\";\n</code></pre> <p>The <code>string MuonId</code> supports <code>\"trackerMuon\"</code>, <code>\"standaloneMuon\"</code> and <code>\"globalMuon\"</code> values.</p> <pre><code>//Which quantity do you want to use?\nstring quantity = \"Pt\";     double bins[] = {0., 2.0, 3.4, 4.0, 4.4, 4.7, 5.0, 5.6, 5.8, 6.0, 6.2, 6.4, 6.6, 6.8, 7.3, 9.5, 13.0, 17.0, 40.};\n</code></pre> <p><code>string quantity</code> supports <code>\"Pt\"</code>, <code>\"Eta\"</code> and <code>\"Phi\"</code> values.</p> <p><code>double bins[]</code> is used to set histogram bins limits. In the example above, the first bin is [0., 2.), the second is [2., 4.) and so on.</p> <p>Inside <code>plot_sys_efficiency()</code>, there is some useful variables too:</p> Type Name Purpose string path_bins_fit_folder Stores the path to the output folder where <code>.png</code> of fit for each bin made will be string directoryToSave Stores the path to output file"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#overplot_efficienciescpp","title":"\ud83d\udcc4 overplot_efficiencies.cpp","text":"<p>The <code>overplot_efficiencies.cpp</code> code will take the results of the previous topic and make a single graph containing all its variations and will output a <code>.png</code> containing the graph.</p> <p>All main variables in this file are in <code>overplot_efficiencies()</code> function</p> Type Name Purpose const char* input_folder_name Stores the path to input folder where <code>.root</code> is const char* output_folder_name Stores the path to output folder string MuonId It accepts values of <code>\"trackerMuon\"</code>, <code>\"standaloneMuon\"</code> and <code>\"globalMuon\"</code> string quantity It accepts values of <code>\"Pt\"</code>, <code>\"Eta\"</code> and <code>\"Phi\"</code> <p>Remeber when selecting MuonId and quantity to run <code>plot_sys_efficiency.cpp</code> before with same configurations.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#plot_sys_efficiency_2dcpp","title":"\ud83d\udcc4 plot_sys_efficiency_2d.cpp","text":"<p>In order to calculate systematic uncertainties in 2D, it was necessary to create another code: the <code>plot_sys_efficiency_2d.cpp</code>. It has a <code>.root</code> output containing the efficiency histograms that can be viewed through the <code>new TBrowser</code> on root command.</p> <p>The variables in this file is shown below:</p> Type Name Purpose string MuonId It accepts values of <code>\"trackerMuon\"</code>, <code>\"standaloneMuon\"</code> and <code>\"globalMuon\"</code> string xquantity It accepts values of <code>\"Pt\"</code>, <code>\"Eta\"</code> and <code>\"Phi\"</code> for horizontal axis double[] xbins is used to set histogram bins limits for horizontal axis string yquantity It accepts values of <code>\"Pt\"</code>, <code>\"Eta\"</code> and <code>\"Phi\"</code> for vertical axis double[] ybins is used to set histogram bins limits for vertical axis string path_bins_fit_folder Stores the path folder where is going to save fit results png for every bin const char* output_folder_name Stores the path to output folder where is going to save the 2D efficiency result"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/","title":"Src files","text":"<p>In this section there is a brief explanation for each file in <code>src/</code> folder. In general, this files are headers called by main files defined on the section Overview.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#create_th2dh","title":"\ud83d\udcc4 create_TH2D.h","text":"<pre><code>TH2D* create_TH2D(const char* name, const char* title, string xquantity, string yquantity, int nbinsx, int nbinsy,\ndouble* xbins, double* ybins)\n</code></pre> <p>Create a empty TH2D histogram according <code>xquantity</code> and <code>yquantity</code> variables. these varibles supports <code>\"Pt\"</code>, <code>\"Eta\"</code> and <code>\"Phi\"</code> values.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#create_folderh","title":"\ud83d\udcc4 create_folder.h","text":"<pre><code>void create_folder(const char* folderPath, bool deleteOld = false)\n</code></pre> <p>This function creates folder path recursively. If <code>deleteOld</code> is true, it deleted the old folder if the path already exists.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#get_efficiencyh","title":"\ud83d\udcc4 get_efficiency.h","text":"<pre><code>TEfficiency* get_efficiency(TH1D* all, TH1D* pass, string quantity, string MuonId, string prefix_name = \"\", bool shouldWrite = false)\n</code></pre> <p>Function used to calculate the efficiency. The <code>MuonId</code>, <code>quantity</code> and <code>prefix_name</code> are used to set the name and title of <code>TEfficiency*</code>. If <code>shouldWrite</code> is true, it writes the result in any root file opened.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#get_efficiency_2dh","title":"\ud83d\udcc4 get_efficiency_2D.h","text":"<pre><code>TEfficiency* get_efficiency_2D(TH2D* all, TH2D* pass, string xquantity, string yquantity, string MuonId, string prefix_name = \"\", bool shouldWrite = false)\n</code></pre> <p>Function used to calculate the 2D efficiency. The <code>MuonId</code>, <code>xquantity</code>, <code>yquantity</code> and <code>prefix_name</code> are used to set the name and title of <code>TEfficiency*</code>. If <code>shouldWrite</code> is true, it writes the result in any root file opened.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#get_efficiency_th2dh","title":"\ud83d\udcc4 get_efficiency_TH2D.h","text":"<pre><code>TH2D* get_efficiency_TH2D(TH2D* hall, TH2D* hpass, string xquantity, string yquantity, string MuonId, string prefix_name = \"\")\n</code></pre> <p>Function used to calculate the 2D efficiency. The <code>MuonId</code>, <code>xquantity</code>, <code>yquantity</code> and <code>prefix_name</code> are used to set the name and title of <code>TEfficiency*</code>. If <code>shouldWrite</code> is true, it writes the result in any root file opened.</p> <p>Same function idea as <code>TEfficiency* get_efficiency_2D(...)</code>, but it creates a <code>TH2D</code> objects instead which allows better control of uncertainty calculus.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#make_th1dh","title":"\ud83d\udcc4 make_TH1D.h","text":"<pre><code>TH1D* make_TH1D(string name, double** values, int index, double* bins, int nbins, string quantity = \"\", bool draw = false)\n</code></pre> <p>Creates TH1D* histogram direclty from <code>values</code> which stores <code>doFit</code>'s outputs.</p> <ul> <li><code>int index</code> is related with the information above: <code>0</code> means all histogram and <code>1</code> means pass histogram. Choose the number due the histogram you are looking to make.</li> <li><code>double* bins</code> is used to set histogram bins limits.</li> <li><code>int nbins</code> represents the number of bins in <code>double* bins</code>.</li> <li><code>string quantity</code> supports <code>\"Pt\"</code>, <code>\"Eta\"</code> and <code>\"Phi\"</code> values.</li> <li>If <code>bool draw</code> it draws the plot on screen.</li> </ul>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#yields_n_errs_to_th2ds_binh","title":"\ud83d\udcc4 yields_n_errs_to_TH2Ds_bin.h","text":"<pre><code>void yields_n_errs_to_TH2Ds_bin(TH2D* hist2d_all, TH2D* hist2d_pass, int x, int y, double* yields_n_errs)\n</code></pre> <p>This function fills <code>hist2d_all</code> and <code>hist2d_pass</code> histogram in cell (x,y) with <code>yields_n_errs</code> which is a output from <code>doFit</code> functions.</p>"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#dofits","title":"\ud83d\udcc2 dofits","text":"<p>Here is stored functions that measures the yields and errors from each bin fit.</p> <p>The return from each function follows this structure: <code>[yield_all, yield_pass, error_all, error_pass]</code>.</p> <p>Functions in this files are defined by:</p> <pre><code>double* doFit(string condition, string MuonId, const char* savePath = NULL)\n</code></pre> <ul> <li><code>string condition</code> selects the bin conditions.</li> <li><code>string MuonId</code> supports <code>\"trackerMuon\"</code>, <code>\"standaloneMuon\"</code> and <code>\"globalMuon\"</code> values.</li> <li><code>const char* savePath</code> where the fit output file from the fit will be saved for further checks.</li> </ul>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/","title":"class FitFunctions","text":"<p>This class hold all fit functions for histograms.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#class-fitfunctionsprimary","title":"class FitFunctions::Primary","text":"<p>This class is holding primary fit functions for histograms.</p> <p>Content list</p> <ul> <li> <p>double Gaus(...)</p> </li> <li> <p>double Pol1(...)</p> </li> <li> <p>double Exp(...)</p> </li> <li> <p>double CrystalBall(...)</p> </li> </ul> <p>Functions details</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#gaus","title":"Gaus(...)","text":"<pre><code>static double Gaus(double *x, double *par)\n</code></pre> <p>Parameters:</p> <pre><code>par = [height, position, sigma]\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#pol1","title":"Pol1(...)","text":"<pre><code>static double Pol1(double *x, double *par)\n</code></pre> <p>Parameters:</p> <pre><code>par = [b, a]\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#pol3","title":"Pol3(...)","text":"<pre><code>static double Pol3(double *x, double *par)\n</code></pre> <p>Parameters:</p> <pre><code>par = [d, c, b, a]\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#exp","title":"Exp(...)","text":"<pre><code>static double Exp(double *x, double *par)\n</code></pre> <p>Parameters:</p> <pre><code>par = [height, width]\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#crystalball","title":"CrystalBall(...)","text":"<pre><code>static double CrystalBall(double *x, double *par)\n</code></pre> <p>Parameters:</p> <pre><code>par = [alpha, n, mean, sigma, yield]\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#class-fitfunctionsmerged","title":"class FitFunctions::Merged","text":"<p>This class holds merged fit functions for histograms.</p> <p>Content list</p> <ul> <li> <p>double Jpsi::Signal_InvariantMass()</p> </li> <li> <p>double Jpsi::Background_InvariantMass()</p> </li> <li> <p>double Jpsi::InvariantMass()</p> </li> <li> <p>double Upsilon::Signal_InvariantMass()</p> </li> <li> <p>double Upsilon::Background_InvariantMass()</p> </li> <li> <p>double Upsilon::InvariantMass()</p> </li> </ul> <p>Functions details</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#jpsisignal_invariantmass","title":"Jpsi::Signal_InvariantMass(...)","text":"<pre><code>static double Signal_InvariantMass(double *x, double *par)\n</code></pre> <p>Form:</p> <p>Gaus + CrystalBall</p> <p>Parameters:</p> <pre><code>par = [height, position, sigma, alpha, n, mean, sigma, yield]\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#jpsibackground_invariantmass","title":"Jpsi::Background_InvariantMass(...)","text":"<pre><code>static double Background_InvariantMass(double *x, double *par)\n</code></pre> <p>Form:</p> <p>Exp</p> <p>Parameters:</p> <pre><code>par = [b, a]\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#jpsiinvariantmass","title":"Jpsi::InvariantMass(...)","text":"<pre><code>static double Signal_InvariantMass(double *x, double *par) + Background_InvariantMass(double *x, double *par)\n</code></pre> <p>Form:</p> <p>Gaus + CrystalBall + Exp</p> <p>Parameters:</p> <pre><code>par = [height1, position1, sigma1, alpha2, n2, mean2, sigma2, yield2, b, a]\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#upsilonsignal_invariantmass","title":"Upsilon::Signal_InvariantMass(...)","text":"<pre><code>static double Signal_InvariantMass(double *x, double *par)\n</code></pre> <p>Form:</p> <p>CrystalBall + Gaus + Gaus</p> <p>Parameters:</p> <pre><code>par = [alpha1, n1, mean1, sigma1, yield1, height2, position2, sigma2, height3, position3, sigma3]\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#upsilonbackground_invariantmass","title":"Upsilon::Background_InvariantMass(...)","text":"<pre><code>static double Background_InvariantMass(double *x, double *par)\n</code></pre> <p>Form:</p> <p>Pol3</p> <p>Parameters:</p> <pre><code>par = [d, c, b, a]\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#upsiloninvariantmass","title":"Upsilon::InvariantMass(...)","text":"<pre><code>static double Signal_InvariantMass(double *x, double *par) + Background_InvariantMass(double *x, double *par)\n</code></pre> <p>Form:</p> <p>CrystalBall + Gaus + Gaus + Pol3</p> <p>Parameters:</p> <pre><code>par = [alpha1, n1, mean1, sigma1, yield1, height2, position2, sigma2, height3, position3, sigma3, d, c, b, a]\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/","title":"class InvariantMass","text":"<p>Holds MassValues struct.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#constructor-details","title":"Constructor details","text":"<pre><code>InvariantMass(\nconst char*&amp; resonance,\nconst char*&amp; particleName,\nconst char*&amp; canvasWatermark,\nconst char*&amp; directoryToSave,\nconst char*&amp; particleType)\n: resonance(resonance),\nparticleName(particleName),\ncanvasWatermark(canvasWatermark),\ndirectoryToSave(directoryToSave),\nparticleType(particleType)\n{\nif (strcmp(resonance, \"Jpsi\") == 0)\n{\nxMin  = 2.9;\nxMax  = 3.3;\nnBins = 160;\n}\n\nif (strcmp(resonance, \"Upsilon\") == 0)\n{\nxMin  = 8.7;\nxMax  = 11.;\nnBins = 60;\n}\n\nif (strcmp(resonance, \"Upsilon1S\") == 0)\n{\nxMin  = 8.7;\nxMax  = 11.;\nnBins = 60;\n}\n\ncreateMassHistogram(Pass.hMass, \"Passing\");\ncreateMassHistogram(All. hMass, \"All\");\n}\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#private-variable-details","title":"Private variable details","text":"<p>Summary</p> Type Name const char*&amp; resonance const char*&amp; particleName const char*&amp; canvasWatermark const char*&amp; directoryToSave const char*&amp; particleType <p>All variables here are reference for public variables in mother class: Type class</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#private-functions-details","title":"Private Functions details","text":""},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#createmasshistogram","title":"createMassHistogram(...)","text":"<pre><code>void createMassHistogram(TH1D* &amp;hMass,\nconst char* PassingOrFailing)\n</code></pre> <p>Create invariant mass histogram with a specific title. The argument <code>hMass</code> is a pointer where the histogram shall be stored.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#drawcanvasquarter","title":"drawCanvasQuarter(...)","text":"<pre><code>void drawCanvasQuarter(TCanvas* &amp;canvas,\nbool drawRegions,\nint quarter,\nMassValues* ObjMassValues,\nint color = kBlue)\n</code></pre> <p>Draw a quarter of whole canvas with invariant mass histogram pointed.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#public-variable-details","title":"Public variable details","text":"<p>Summary</p> Type Name Default value double xMin 0. double xMax 0. int nBins 0 int decimals 3 <p>Constructed objects</p> <ul> <li><code>MassValues Pass</code><ul> <li>Stores information about passing mass histograms.</li> </ul> </li> <li><code>MassValues All</code><ul> <li>Stores information about passing mass histograms.</li> </ul> </li> </ul>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#public-functions-details","title":"Public Functions details","text":""},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#createmasscanvas","title":"createMassCanvas(...)","text":"<pre><code>TCanvas* createMassCanvas(bool drawRegions = false,\nbool shouldWrite = false,\nbool shouldSavePNG = false)\n</code></pre> <p>Create canvas for invariant mass (passing and all muons).</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#definemasshistogramnumbers","title":"defineMassHistogramNumbers()","text":"<pre><code>void defineMassHistogramNumbers(int nBins,\ndouble xMin,\ndouble xMax,\nint decimals = 3)\n</code></pre> <p>Redefine number parameters of mass histograms in Mass object.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#dofit","title":"doFit()","text":"<pre><code>void doFit()\n</code></pre> <p>Apply a fit over invariant mass in MassValues objects.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#fillmasshistograms","title":"fillMassHistograms(...)","text":"<pre><code>void fillMassHistograms(double** quantities,\nint** types)\n</code></pre> <p>Automatically fill masses histograms. Needs to be called in a loop over all dataset.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#updatemassvaluesall","title":"updateMassValuesAll()","text":"<pre><code>void updateMassValuesAll()\n</code></pre> <p>After fill invariant mass histogram, you need to set signal regions and sideband regions. This function will set it for you.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#updatemassvaluesall_1","title":"updateMassValuesAll(...)","text":"<pre><code>void updateMassValuesFor(MassValues* ObjMassValues,\nbool isAll = false)\n</code></pre> <p>After fill invariant mass histograms, you need to set signal regions and sideband regions. This function will set it for you.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#writemasshistogramsonfile","title":"writeMassHistogramsOnFile(...)","text":"<pre><code>void writeMassHistogramsOnFile(bool writehPass,\nbool writehAll)\n</code></pre> <p>Write all mass canvas histograms in a root file. Just need to call this function and all mass histograms will be written.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/","title":"struct MassValues","text":"<p>Holds informations about passing or all particles fit.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#public-variable-details","title":"Public variable details","text":"<p>Summary</p> Type Name Default value TH1D* hMass NULL TF1* fitFunction NULL TF1* fitSignal NULL TF1* fitBackground NULL double sidebandRegion1_x1 0. double sidebandRegion1_x2 0. double signalRegion_x1 0. double signalRegion_x2 0. double sidebandRegion2_x1 0. double sidebandRegion2_x2 0. TFitResultPtr fitResult 0"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#public-functions-details","title":"Public Functions details","text":""},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#createtbox","title":"createTBox(...)","text":"<pre><code>TBox* createTBox(double Ymax,\nint index = 0,\ndouble Ymin = 0.)\n</code></pre> <p>Return TBox of sideband or signal region.</p> <ul> <li>if  <code>index = -1</code> return TBox representing left sideband region.</li> <li>if  <code>index =  0</code> return TBox representing signal region.</li> <li>if  <code>index =  1</code> return TBox representing right sideband region.</li> </ul>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#dofitjpsi","title":"doFitJpsi()","text":"<pre><code>void doFitJpsi()\n</code></pre> <p>Do fit for J/psi resonance.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#dofitupsilon","title":"doFitUpsilon()","text":"<pre><code>void doFitUpsilon()\n</code></pre> <p>Do fit for Upsilon resonance with 3 resonances peaks (1S, 2S, 3S).</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#dofitupsilon1s","title":"doFitUpsilon1S()","text":"<pre><code>void doFitUpsilon1S()\n</code></pre> <p>Do fit for Upsilon (1S) resonance.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#isinsidebandregion","title":"isInSidebandRegion(...)","text":"<pre><code>bool isInSidebandRegion(double InvariantMass)\n</code></pre> <p>Check if InvariantMass is in sideband region.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#isinsignalregion","title":"isInSignalRegion(...)","text":"<pre><code>bool isInSignalRegion(double InvariantMass)\n</code></pre> <p>Check if InvariantMass is in signal region.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#subtractionfactor","title":"subtractionFactor()","text":"<pre><code>double subtractionFactor()\n</code></pre> <p>Get the subtraction factor calculated by the ratio between yield of background particles in signal region by yield of background particles in sideband region. This yield is get by the integral of function stored in fitBackground variable.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/","title":"class PassingFailing","text":"<p>Holds histograms of passing and all particle quantities.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#constructor-details","title":"Constructor details","text":"<pre><code>PassingFailing(\nconst char*&amp; resonance,\nconst char*&amp; particleName,\nconst char*&amp; canvasWatermark,\nconst char*&amp; directoryToSave,\nconst char*&amp; particleType,\nInvariantMass&amp; ObjMass,\nconst char*&amp; tagOrProbe,\nconst char*  passingOrFailing,\nconst char*&amp; quantityName,\nconst char*&amp; xAxisName,\nconst char*&amp; quantityUnit,\nconst char*&amp; extendedQuantityName,\ndouble&amp;      xMin,\ndouble&amp;      xMax,\nint&amp;         nBins,\nint&amp;         decimals)\n: resonance(resonance),\nparticleName(particleName),\ncanvasWatermark(canvasWatermark),\ndirectoryToSave(directoryToSave),\nparticleType(particleType),\nObjMass(ObjMass),\ntagOrProbe(tagOrProbe),\npassingOrFailing(passingOrFailing),\nquantityName(quantityName),\nxAxisName(xAxisName),\nquantityUnit(quantityUnit),\nextendedQuantityName(extendedQuantityName),\nnBins(nBins),\nxMin(xMin),\nxMax(xMax),\ndecimals(decimals)\n{\ncreateHistogram(hSigBack, \"SigBack\");\ncreateHistogram(hSig,     \"Sig\");\ncreateHistogram(hBack,    \"Back\");\n}\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#private-variable-details","title":"Private variable details","text":"<p>Summary</p> Type Name const char*&amp; resonance const char*&amp; particleName const char*&amp; canvasWatermark const char*&amp; directoryToSave const char*&amp; particleType const char*&amp; tagOrProbe InvariantMass&amp; ObjMass const char*&amp; tagOrProbe const char*&amp; xAxisName const char*&amp; quantityUnit const char*&amp; extendedQuantityName double&amp; xMin double&amp; xMax int&amp; nBins int&amp; decimals <p>All variables here are reference for public variables in mother class: PtEtaPhi class.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#private-functions-details","title":"Private Functions details","text":""},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#createhistogram","title":"createHistogram()","text":"<pre><code>void createHistogram()\n</code></pre> <p>Create quantity histogram.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#fillafter","title":"fillAfter()","text":"<pre><code>string fillAfter(string text,\nchar fillWith,\nint targetLength)\n</code></pre> <p>Fill blank space of a string. It is used in consistencyDebugCout().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#public-variable-details","title":"Public variable details","text":"<p>Summary</p> Type Name Default value const char* passingOrFailing NULL TH1D* hSigBack NULL TH1D* hSig NULL TH1D* hBack NULL <p>Details</p> <ul> <li><code>const char* passingOrFailing</code><ul> <li>Set if it is \"Passing\" or \"All\" object.</li> </ul> </li> <li><code>TH1D* hSigBack</code><ul> <li>Stores the histogram for particles in signal region.</li> </ul> </li> <li><code>TH1D* hSig</code><ul> <li>Stores the subtracted histogram.</li> </ul> </li> <li><code>TH1D* hBack</code><ul> <li>Stores the histogram for particles in sideband region.</li> </ul> </li> </ul>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#public-functions-details","title":"Public Functions details","text":""},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#consistencydebugcout","title":"consistencyDebugCout()","text":"<pre><code>void consistencyDebugCout()\n</code></pre> <p>Print on terminal the consistency check after subtractSigHistogram().</p> <p>It is result for this equation:</p> <p></p> <p>Where: alpha = yield of background particles signal region / yield of background particles sideband region</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#createquantitiescanvas","title":"createQuantitiesCanvas(...)","text":"<pre><code>TCanvas* createQuantitiesCanvas(bool shouldWrite = false,\nbool shouldSavePNG = false)\n</code></pre> <p>Create canvas for all quantities after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#fillquantitieshistograms","title":"fillQuantitiesHistograms(...)","text":"<pre><code>void fillQuantitiesHistograms(double&amp; InvariantMass,\nint&amp; isPassing)\n</code></pre> <p>Automatically fill all quantities histograms. Needs to be called in a loop over all dataset.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#normalizehistograms","title":"normalizeHistograms()","text":"<pre><code>void normalizeHistograms()\n</code></pre> <p>Normalize quantities histograms of variable bin after filling it.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#passfailobj","title":"PassFailObj()","text":"<pre><code>MassValues* PassFailObj()\n</code></pre> <p>Get the MassValue object of corresponding MassValue object.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#subtractsighistogram","title":"subtractSigHistogram()","text":"<pre><code>void subtractSigHistogram()\n</code></pre> <p>Apply sideband subtraction over histograms.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#writequantitieshistogramsonfile","title":"writeQuantitiesHistogramsOnFile(...)","text":"<pre><code>void writeQuantitiesHistogramsOnFile(bool hSigBack,\nbool hSig,\nbool hBack)\n</code></pre> <p>Write quantity histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/","title":"class PtEtaPhi","text":"<p>Holds PassingFailing class.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#constructor-details","title":"Constructor details","text":"<pre><code>PtEtaPhi(\nconst char*&amp; resonance,\nconst char*&amp; particleName,\nconst char*&amp; canvasWatermark,\nconst char*&amp; directoryToSave,\nconst char*&amp; particleType,\nInvariantMass&amp; ObjMass,\nconst char*&amp; tagOrProbe,\nconst char*  quantityName,\nconst char*  xAxisName,\nconst char*  quantityUnit,\nconst char*  extendedQuantityName,\nint          nBins,\ndouble       xMin,\ndouble       xMax,\nint          decimals = 3)\n: resonance(resonance),\nparticleName(particleName),\ncanvasWatermark(canvasWatermark),\ndirectoryToSave(directoryToSave),\nparticleType(particleType),\nObjMass(ObjMass),\ntagOrProbe(tagOrProbe),\nquantityName(quantityName),\nxAxisName(xAxisName),\nquantityUnit(quantityUnit),\nextendedQuantityName(extendedQuantityName),\nnBins(nBins),\nxMin(xMin),\nxMax(xMax),\ndecimals(decimals)\n{}\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#private-variable-details","title":"Private variable details","text":"<p>Summary</p> Type Name const char*&amp; resonance const char*&amp; particleName const char*&amp; canvasWatermark const char*&amp; directoryToSave const char*&amp; particleType const char*&amp; tagOrProbe InvariantMass&amp; ObjMass <p>All variables here are reference for public variables in mother class: TagProbe class.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#public-variable-details","title":"Public variable details","text":"<p>Summary</p> Type Name Default value const char* tagOrProbe NULL const char* xAxisName NULL const char* quantityUnit NULL const char* extendedQuantityName NULL double xMin 0. double xMax 0. int nBins 0 int decimals 3 TEfficiency* pEff NULL <p>Details</p> <ul> <li><code>const char* quantityName</code><ul> <li>Stores the quantity name. E.g.: \"pT\".</li> </ul> </li> <li><code>const char* extendedQuantityName</code><ul> <li>Stores the extended quantity name. E.g.: \"Transversal Momentum\".</li> </ul> </li> <li><code>const char* quantityUnit</code><ul> <li>Stores the quantity unit. E.g.: \"GeV/c\".</li> </ul> </li> <li><code>const char* xAxisName</code><ul> <li>Stores the quantity name for histogram horizontal axis in LaTeX form. E.g.: \"p_{t}\".</li> </ul> </li> <li><code>int nBins</code><ul> <li>Stores the number of bins in histograms.</li> </ul> </li> <li><code>int decimals = 3</code><ul> <li>Number of decimals showed in bin width on histogram vertical axis.</li> </ul> </li> <li><code>double xMin</code><ul> <li>Lower horizontal value of histogram.</li> </ul> </li> <li><code>double xMax</code><ul> <li>Higher horizontal value of histogram.</li> </ul> </li> <li><code>TEfficiency* pEff</code><ul> <li>Stores the efficiency plot.</li> </ul> </li> </ul> <p>Constructed objects</p> <ul> <li><code>PassingFailing Pass</code><ul> <li>Stores all informations about invariant masses, including fit and histograms.</li> </ul> </li> <li><code>PassingFailing All</code><ul> <li>Stores all informations about tag muons, incuding quantities histograms and efficiencies.</li> </ul> </li> </ul>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#public-functions-details","title":"Public Functions details","text":""},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#consistencydebugcout","title":"consistencyDebugCout()","text":"<pre><code>void consistencyDebugCout()\n</code></pre> <p>Print on terminal the consistency check after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#createefficiencycanvas","title":"createEfficiencyCanvas(...)","text":"<pre><code>void createEfficiencyCanvas(bool shouldWrite = false,\nbool shouldSavePNG = false)\n</code></pre> <p>Create canvas for all efficiencies calculated. It need to be called after createEfficiencyPlot(...).</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#createefficiencyplot","title":"createEfficiencyPlot(...)","text":"<pre><code>TEfficiency* createEfficiencyPlot(bool shouldWrite = false)\n</code></pre> <p>Create a TEfficiency object with calculated efficiency. It needs do be called after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#createquantitiescanvas","title":"createQuantitiesCanvas(...)","text":"<pre><code>TCanvas* createQuantitiesCanvas(bool shouldWrite = false,\nbool shouldSavePNG = false)\n</code></pre> <p>Create canvas for all quantities after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#fillquantitieshistograms","title":"fillQuantitiesHistograms(...)","text":"<pre><code>void fillQuantitiesHistograms(double&amp; quantity,\ndouble&amp; InvariantMass,\nint&amp; isPassing)\n</code></pre> <p>Automatically fill all quantities histograms. Needs to be called in a loop over all dataset.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#normalizehistograms","title":"normalizeHistograms()","text":"<pre><code>void normalizeHistograms()\n</code></pre> <p>Normalize quantities histograms of variable bin after filling it.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#subtractsighistograms","title":"subtractSigHistograms()","text":"<pre><code>void subtractSigHistograms()\n</code></pre> <p>Apply sideband subtraction over all histograms.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#writequantitieshistogramsonfile","title":"writeQuantitiesHistogramsOnFile(...)","text":"<pre><code>void writeQuantitiesHistogramsOnFile(bool hSigBack,\nbool hSig,\nbool hBack)\n</code></pre> <p>Write all quantities histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/","title":"class SidebandSubtraction","text":"<p>Holds Type class. This is the mother class.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#constructor-details","title":"Constructor details","text":"<pre><code>SidebandSubtraction()\n{}\n</code></pre> <pre><code>SidebandSubtraction(const char* resonance)\n: resonance(resonance)\n{}\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#public-variable-details","title":"Public variable details","text":"<p>Summary</p> Type Name Default value const char* resonance \"Jpsi\" const char* particleName \"Muon\" const char* canvasWatermark \"#bf{CMS Open Data}\" const char* directoryToSave \"../result/\" bool doTracker true bool doStandalone true bool doGlobal true bool doTagMuon true bool doProbeMuon true <p>Details</p> <ul> <li><code>const char* resonance = \"Jpsi\"</code><ul> <li>Supports values <code>\"Jpsi\"</code>, <code>\"Upsilon\"</code> or <code>\"Upsilon(1S)\"</code>.</li> </ul> </li> <li><code>const char* particleName = \"Muon\"</code><ul> <li>Stores the particle name for titles.</li> </ul> </li> <li><code>const char* canvasWatermark = \"#bf{CMS Open Data}\"</code><ul> <li>Stores what watermark will be showed in plots.</li> </ul> </li> <li><code>const char* directoryToSave = \"../result/\"</code><ul> <li>Where all canvas will be stored.</li> </ul> </li> <li><code>bool doTracker    = true</code><ul> <li>If it will compute Tracker muons efficiency.</li> </ul> </li> <li><code>bool doStandalone = true</code><ul> <li>If it will compute Standalone muons efficiency.</li> </ul> </li> <li><code>bool doGlobal     = true</code><ul> <li>If it will compute Global muons efficiency.</li> </ul> </li> </ul> <p>Constructed objects</p> <ul> <li><code>Type Tracker</code><ul> <li>Stores all informations about Tracker muons.</li> </ul> </li> <li><code>Type Standalone</code><ul> <li>Stores all informations about Standalone muons.</li> </ul> </li> <li><code>Type Global</code><ul> <li>Stores all informations about Global muons.</li> </ul> </li> </ul>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#public-functions-details","title":"Public Functions details","text":""},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#consistencydebugcout","title":"consistencyDebugCout()","text":"<pre><code>void consistencyDebugCout()\n</code></pre> <p>Print on terminal the consistency check after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#createefficiencycanvas","title":"createEfficiencyCanvas(...)","text":"<pre><code>void createEfficiencyCanvas(bool shouldWrite = false,\nbool shouldSavePNG = false)\n</code></pre> <p>Create canvas for all efficiencies calculated. It need to be called after createEfficiencyPlot(...).</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#createefficiencyplot","title":"createEfficiencyPlot(...)","text":"<pre><code>void createEfficiencyPlot(bool shouldWrite = false)\n</code></pre> <p>Create a TEfficiency object with calculated efficiency. It needs do be called after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#createmasscanvas","title":"createMassCanvas(...)","text":"<pre><code>void createMassCanvas(bool drawRegions = false,\nbool shouldWrite = false,\nbool shouldSavePNG = false)\n</code></pre> <p>Create canvas for all invariant mass (passing and all muons).</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#createquantitiescanvas","title":"createQuantitiesCanvas(...)","text":"<pre><code>void createQuantitiesCanvas(bool shouldWrite = false,\nbool shouldSavePNG = false)\n</code></pre> <p>Create canvas for all quantities after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#definemasshistogramnumbers","title":"defineMassHistogramNumbers()","text":"<pre><code>void defineMassHistogramNumbers(int nBins,\ndouble xMin,\ndouble xMax,\nint decimals = 3)\n</code></pre> <p>Redefine number parameters of all mass histograms.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#dofit","title":"doFit()","text":"<pre><code>void doFit()\n</code></pre> <p>Apply a fit over all invariant mass stored.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#fillmasshistograms","title":"fillMassHistograms(...)","text":"<pre><code>void fillMassHistograms(double** quantities,\nint** types)\n</code></pre> <p>Automatically fill all masses histograms. Needs to be called in a loop over all dataset.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#fillquantitieshistograms","title":"fillQuantitiesHistograms(...)","text":"<pre><code>void fillQuantitiesHistograms(double** quantities,\nint** types)\n</code></pre> <p>Automatically fill all quantities histograms. Needs to be called in a loop over all dataset.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#normalizehistograms","title":"normalizeHistograms()","text":"<pre><code>void normalizeHistograms()\n</code></pre> <p>Normalize quantities histograms of variable bin after filling it.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#subtractsighistograms","title":"subtractSigHistograms()","text":"<pre><code>void subtractSigHistograms()\n</code></pre> <p>Apply sideband subtraction over all histograms.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#updatemassvaluesall","title":"updateMassValuesAll()","text":"<pre><code>void updateMassValuesAll()\n</code></pre> <p>After fill invariant mass histograms, you need to set signal regions and sideband regions. This function will set it for you.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#writemasshistogramsonfile","title":"writeMassHistogramsOnFile(...)","text":"<pre><code>void writeMassHistogramsOnFile(bool writehPass,\nbool writehAll)\n</code></pre> <p>Write all mass canvas histograms in a root file. Just need to call this function and all mass histograms will be written.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#writequantitieshistogramsonfile","title":"writeQuantitiesHistogramsOnFile(...)","text":"<pre><code>void writeQuantitiesHistogramsOnFile(bool hSigBack,\nbool hSig,\nbool hBack)\n</code></pre> <p>Write all quantities histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/","title":"class TagProbe","text":"<p>Holds TagProbe class and InvariantMass class.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#constructor-details","title":"Constructor details","text":"<pre><code>TagProbe(\nconst char*&amp; resonance,\nconst char*&amp; particleName,\nconst char*&amp; canvasWatermark,\nconst char*&amp; directoryToSave,\nconst char*&amp; particleType,\nInvariantMass&amp; ObjMass,\nconst char*  tagOrProbe)\n: resonance(resonance),\nparticleName(particleName),\ncanvasWatermark(canvasWatermark),\ndirectoryToSave(directoryToSave),\nparticleType(particleType),\nObjMass(ObjMass),\ntagOrProbe(tagOrProbe)\n{}\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#private-variable-details","title":"Private variable details","text":"<p>Summary</p> Type Name const char*&amp; resonance const char*&amp; particleName const char*&amp; canvasWatermark const char*&amp; directoryToSave const char*&amp; particleType InvariantMass&amp; ObjMass <p>All variables here are reference for public variables in mother class: Type class</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#public-variable-details","title":"Public variable details","text":"<p>Summary</p> Type Name Default value const char* tagOrProbe NULL <p>Details</p> <ul> <li><code>const char* tagOrProbe = NULL</code><ul> <li>Set if it is \"Tag\" or \"Probe\" object</li> </ul> </li> </ul> <p>Constructed objects</p> <ul> <li><code>PtEtaPhi Pt</code><ul> <li>Transversal momentum histograms.</li> </ul> </li> <li><code>PtEtaPhi Eta</code><ul> <li>Pseudorapidity histograms.</li> </ul> </li> <li><code>PtEtaPhi Phi</code><ul> <li>Azimutal angle histograms.</li> </ul> </li> </ul>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#public-functions-details","title":"Public Functions details","text":""},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#consistencydebugcout","title":"consistencyDebugCout()","text":"<pre><code>void consistencyDebugCout()\n</code></pre> <p>Print on terminal the consistency check after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#createefficiencycanvas","title":"createEfficiencyCanvas(...)","text":"<pre><code>void createEfficiencyCanvas(bool shouldWrite = false,\nbool shouldSavePNG = false)\n</code></pre> <p>Create canvas for all efficiencies calculated. It need to be called after createEfficiencyPlot(...).</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#createefficiencyplot","title":"createEfficiencyPlot(...)","text":"<pre><code>void createEfficiencyPlot(bool shouldWrite = false)\n</code></pre> <p>Create a TEfficiency object with calculated efficiency. It needs do be called after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#createquantitiescanvas","title":"createQuantitiesCanvas(...)","text":"<pre><code>void createQuantitiesCanvas(bool shouldWrite = false,\nbool shouldSavePNG = false)\n</code></pre> <p>Create canvas for all quantities after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#fillquantitieshistograms","title":"fillQuantitiesHistograms(...)","text":"<pre><code>void fillQuantitiesHistograms(double** quantities,\ndouble&amp; InvariantMass,\nint&amp; isPassing)\n</code></pre> <p>Automatically fill all quantities histograms. Needs to be called in a loop over all dataset.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#normalizehistograms","title":"normalizeHistograms()","text":"<pre><code>void normalizeHistograms()\n</code></pre> <p>Normalize quantities histograms of variable bin after filling it.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#subtractsighistograms","title":"subtractSigHistograms()","text":"<pre><code>void subtractSigHistograms()\n</code></pre> <p>Apply sideband subtraction over all histograms.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#writequantitieshistogramsonfile","title":"writeQuantitiesHistogramsOnFile(...)","text":"<pre><code>void writeQuantitiesHistogramsOnFile(bool hSigBack,\nbool hSig,\nbool hBack)\n</code></pre> <p>Write all quantities histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/","title":"class Type","text":"<p>Holds TagProbe class and InvariantMass class.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#constructor-details","title":"Constructor details","text":"<pre><code>    Type(\nconst char*&amp; resonance,\nconst char*&amp; particleName,\nbool&amp; doTagMuon,\nbool&amp; doProbeMuon,\nconst char*&amp; canvasWatermark,\nconst char*&amp; directoryToSave,\nconst char*  particleType)\n: resonance(resonance),\nparticleName(particleName),\ndoTagMuon(doTagMuon),\ndoProbeMuon(doProbeMuon),\ncanvasWatermark(canvasWatermark),\ndirectoryToSave(directoryToSave),\nparticleType(particleType)\n{}\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#private-variable-details","title":"Private variable details","text":"<p>Summary</p> Type Name const char*&amp; resonance const char*&amp; particleName bool&amp; doTagMuon bool&amp; doProbeMuon const char*&amp; canvasWatermark const char*&amp; directoryToSave <p>All variables here are reference for public variables in mother class: SidebandSubtraction class.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#public-variable-details","title":"Public variable details","text":"<p>Summary</p> Type Name Default value const char* particleType NULL <p>Details</p> <ul> <li><code>const char* particleType = NULL</code><ul> <li>Set the name of particle type.</li> </ul> </li> </ul> <p>Constructed objects</p> <ul> <li><code>InvariantMass Mass</code><ul> <li>Stores all informations about invariant masses, including fit and histograms.</li> </ul> </li> <li><code>TagProbe Tag</code><ul> <li>Stores all informations about tag muons, incuding quantities histograms and efficiencies.</li> </ul> </li> <li><code>TagProbe Probe</code><ul> <li>Stores all informations about probe muons, incuding quantities histograms and efficiencies.</li> </ul> </li> </ul>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#public-functions-details","title":"Public Functions details","text":""},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#consistencydebugcout","title":"consistencyDebugCout()","text":"<pre><code>void consistencyDebugCout()\n</code></pre> <p>Print on terminal the consistency check after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#createefficiencycanvas","title":"createEfficiencyCanvas(...)","text":"<pre><code>void createEfficiencyCanvas(bool shouldWrite = false,\nbool shouldSavePNG = false)\n</code></pre> <p>Create canvas for all efficiencies calculated. It need to be called after createEfficiencyPlot(...).</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#createefficiencyplot","title":"createEfficiencyPlot(...)","text":"<pre><code>void createEfficiencyPlot(bool shouldWrite = false)\n</code></pre> <p>Create a TEfficiency object with calculated efficiency. It needs do be called after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#createmasscanvas","title":"createMassCanvas(...)","text":"<pre><code>void createMassCanvas(bool drawRegions = false,\nbool shouldWrite = false,\nbool shouldSavePNG = false)\n</code></pre> <p>Create canvas for all invariant mass (passing and all muons).</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#createquantitiescanvas","title":"createQuantitiesCanvas(...)","text":"<pre><code>void createQuantitiesCanvas(bool shouldWrite = false,\nbool shouldSavePNG = false)\n</code></pre> <p>Create canvas for all quantities after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#definemasshistogramnumbers","title":"defineMassHistogramNumbers()","text":"<pre><code>void defineMassHistogramNumbers(int nBins,\ndouble xMin,\ndouble xMax,\nint decimals = 3)\n</code></pre> <p>Redefine number parameters of mass histograms in Mass object.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#dofit","title":"doFit()","text":"<pre><code>void doFit()\n</code></pre> <p>Apply a fit over invariant mass in Mass object.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#fillmasshistograms","title":"fillMassHistograms(...)","text":"<pre><code>void fillMassHistograms(double&amp; InvariantMass,\nint&amp; isPassing)\n</code></pre> <p>Automatically fill all masses histograms. Needs to be called in a loop over all dataset.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#fillquantitieshistograms","title":"fillQuantitiesHistograms(...)","text":"<pre><code>void fillQuantitiesHistograms(double** quantities,\nint&amp; isPassing)\n</code></pre> <p>Automatically fill all quantities histograms. Needs to be called in a loop over all dataset.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#normalizehistograms","title":"normalizeHistograms()","text":"<pre><code>void normalizeHistograms()\n</code></pre> <p>Normalize quantities histograms of variable bin after filling it.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#subtractsighistograms","title":"subtractSigHistograms()","text":"<pre><code>void subtractSigHistograms()\n</code></pre> <p>Apply sideband subtraction over all histograms.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#updatemassvaluesall","title":"updateMassValuesAll()","text":"<pre><code>void updateMassValuesAll()\n</code></pre> <p>After fill invariant mass histograms, you need to set signal regions and sideband regions. This function will set it for you.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#writemasshistogramsonfile","title":"writeMassHistogramsOnFile(...)","text":"<pre><code>void writeMassHistogramsOnFile(bool writehPass,\nbool writehAll)\n</code></pre> <p>Write all mass canvas histograms in a root file. Just need to call this function and all mass histograms will be written.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#writequantitieshistogramsonfile","title":"writeQuantitiesHistogramsOnFile(...)","text":"<pre><code>void writeQuantitiesHistogramsOnFile(bool hSigBack,\nbool hSig,\nbool hBack)\n</code></pre> <p>Write all quantities histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/","title":"The Macro","text":"<p>A macro is a code file create to be interpreted by a program. In this case, ROOT program will interpret it. The main code of this tool is in the file <code>macro.ccp</code>. In this section what compose this file is explained in details. It is important to note that this code has been tested on root 6.22/00.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#about-the-code","title":"About the code","text":"<p><code>macro.cpp</code> is a example how to use Sideband Subtraction to get reconstruction efficiencies for a Tag &amp; Probe ntupple. It analyzes J/psi and Upsilon reconstruction efficiency for tracker, standalone and global muons. The file is encountered in folder <code>main</code>. Now, I going to talk about what this function do and how it does in the text below.</p> <p>Dataset</p> <p>The datasets used in this code are obtained with the main code of this Tag an Probe tool.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#classes-list","title":"Classes list","text":"<p>There are some classes in Sideband Subtraction Tag And Probe project and they are distributed in these files with same name:</p> <p>Static functions:</p> <ul> <li>FitFunctions<ul> <li>Primary</li> <li>Merged<ul> <li>Jpsi</li> <li>Upsilon</li> </ul> </li> </ul> </li> </ul> <p>Classes and struct:</p> <ul> <li>SidebandSubtraction<ul> <li>Type<ul> <li>InvariantMass<ul> <li>MassValues</li> </ul> </li> <li>TagProbe<ul> <li>PtEtaPhi<ul> <li>PassingFailing</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>This format shows what nested classes. Classes or structs below slided at right represents they are nested with the class above it.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#sideband-subtraction-code-structure","title":"Sideband Subtraction code structure","text":"<p>The diagram below represents the structure of objects in code. At left we have the structure of objects name. At right we have the correspondent class name of objects in these line.</p> <p></p> <p>Also in Mass object we have:</p> <p></p> <p>Notice that all objects in same line shares the same structure.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#before-macrocpp","title":"Before macro.cpp","text":"<p>There are some files in folder <code>config</code> aside of <code>macro.ccp</code>. The sections below explain about them.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#cutsh","title":"cuts.h","text":"<p>This is it content:</p> <pre><code>//This files holds some functions used in macro.cpp for particle selection\n\n//Return if is a accepted particle or no\nbool applyCuts(double** quantities, int** types)\n{\n//Assign variables for easy visualization\ndouble &amp;ProbeMuon_Pt            = *quantities[0];\ndouble &amp;ProbeMuon_Eta           = *quantities[1];\ndouble &amp;ProbeMuon_Phi           = *quantities[2];\ndouble &amp;TagMuon_Pt              = *quantities[3];\ndouble &amp;TagMuon_Eta             = *quantities[4];\ndouble &amp;TagMuon_Phi             = *quantities[5];\ndouble &amp;InvariantMass           = *quantities[6];\nint &amp;PassingProbeTrackingMuon   = *types[0];\nint &amp;PassingProbeStandAloneMuon = *types[1];\nint &amp;PassingProbeGlobalMuon     = *types[2];\n\n//Apply cuts\nif (TagMuon_Pt &gt;= 7.0 &amp;&amp; fabs(TagMuon_Eta) &lt;= 2.4)\nreturn true;\n\nreturn false;\n}\n</code></pre> <p>It stores the function applyCuts(), where return <code>true</code> for allowed pair of particles and <code>false</code> for not allowed.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#createhistogramh","title":"createHistogram.h","text":"<p>This file is called in PassingFailing.cpp and set quantity histograms bins and create the hitogram. Its default content is shwon bellow:</p> <pre><code>void createHistogram(TH1D* &amp;histo, const char* histoName)\n{\n//Set parameters\nstring hName          = string(particleType) + string(passingOrFailing) + string(tagOrProbe) + string(particleName) + \"_\" + string(quantityName) + string(histoName);\nstring hTitle         = string(passingOrFailing) + \" in \" + string(particleType) + \" \" + string(tagOrProbe);\nstring xAxisTitle     = string(xAxisName);\nstring yAxisTitleForm = \"Events\";\n\n//Add unit if has\nif (strcmp(quantityUnit, \"\") != 0)\nxAxisTitle += \" [\" + string(quantityUnit) + \"]\";\n\n//Change title is passing\nif (strcmp(passingOrFailing, \"Passing\") == 0)\nhTitle = string(particleType) + \" \" + string(particleName) + \" \" + string(tagOrProbe);\n\nif (strcmp(passingOrFailing, \"All\") == 0)\nhTitle = \"All \" + string(particleName) + \" \" + string(tagOrProbe);\n\n\n//Variable bin for pT\nif (strcmp(quantityName, \"Pt\") == 0)\n{\ndouble xbins[] = {0., 2.0, 3.4, 4.0, 4.4, 4.7, 5.0, 5.6, 5.8, 6.0, 6.2, 6.4, 6.6, 6.8, 7.3, 9.5, 13.0, 17.0, 40.};\n\nint nbins = sizeof(xbins)/sizeof(*xbins) - 1;\nhisto = new TH1D(hName.data(), hTitle.data(), nbins, xbins);\n}\n\n//Variable bin for eta\nelse if (strcmp(quantityName, \"Eta\") == 0)\n{\ndouble xbins[] = {-2.4, -1.8, -1.4, -1.2, -1.0, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1.0, 1.2, 1.4, 1.8, 2.4};\n\nint nbins = sizeof(xbins)/sizeof(*xbins) - 1;\nhisto = new TH1D(hName.data(), hTitle.data(), nbins, xbins);\n}\n\n//Bins for phi \nelse\n{\ndouble xbins[] = {-3.0, -1.8, -1.6, -1.2, -1.0, -0.7, -0.4, -0.2, 0, 0.2, 0.4, 0.7, 1.0, 1.2, 1.6, 1.8, 3.0};\n\nint nbins = sizeof(xbins)/sizeof(*xbins) - 1;\nhisto = new TH1D(hName.data(), hTitle.data(), nbins, xbins);\n}\n\n//Edit histogram axis\nhisto-&gt;GetYaxis()-&gt;SetTitle(Form(yAxisTitleForm.data(), histo-&gt;GetBinWidth(0)));\nhisto-&gt;GetXaxis()-&gt;SetTitle(xAxisTitle.data());\n}\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#settingscpp","title":"settings.cpp","text":"<p>It stores many configurations used in <code>macro.cpp</code>:</p> <pre><code>//List of files\nconst char *files[] = {\"../data_histoall.root\",\n\"../Run2011AMuOnia_mergeNtuple.root\",\n\"../JPsiToMuMu_mergeMCNtuple.root\",\n\"../Run2011A_MuOnia_Upsilon.root\",\n\"../Upsilon1SToMuMu_MC_full.root\"};\n\nconst char* directoriesToSave[] = {\"../results/result/\",\n\"../results/Jpsi_Run_2011/\",\n\"../results/Jpsi_MC_2020_sbs/\",\n\"../results/Upsilon_Run_2011/\",\n\"../results/Upsilon_MC_2020_sbs/\"};\n\n//MAIN OPTIONS\n\n//Which file of files (variable above) should use\nint useFile = 4;\n\n//Set the canvasW wtermark\nconst char* canvasWatermark = \"#bf{CMS Open Data}\";\n\n//Path where is going to save results \nconst char* directoryToSave = directoriesToSave[useFile];\n//directoryToSave = \"../result/\";\n\n//Should limit data?\nlong long limitData = 0; //0 -&gt; do not limit\n\n//Canvas drawing\nbool shouldDrawInvariantMassCanvas          = true;\nbool shouldDrawInvariantMassCanvasRegion    = true;\nbool shouldDrawQuantitiesCanvas             = true;\nbool shouldDrawEfficiencyCanvas             = true;\n\n//Muon id anlyse\nbool doTracker    = true;\nbool doStandalone = false;\nbool doGlobal     = false;\n\n//Muon label anlyse\nbool doTagMuon   = false;\nbool doProbeMuon = true;\n\n//ENDED MAIN OPTIONS\n</code></pre> <p>And then there are more automatically set options:</p> <pre><code>//Auto detect resonance due file index\nconst char* resonance = \"Jpsi\";\nif (useFile &gt; 2)\nresonance = \"Upsilon\";\nif (useFile == 4)\nresonance = \"Upsilon1S\";\n\n\n\n//Auto detect limit of data\nif (limitData &gt; 0)\ndirectoryToSave = \"../partial_result/\";\n\n\n\n//Compatibility adjusts on file read (for data_histoall ntupples)\nbool needsRetroCompatibility = false;\n\nif (useFile == 0)\nneedsRetroCompatibility = true;\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#code-explained-in-parts","title":"Code explained in parts","text":"<p><code>macro.cpp</code> is the main file of this program. Its the main code. It is explained in parts below:</p> <pre><code>//Input files, options are set here!\n#include \"config/settings.cpp\"\n</code></pre> <p>It imports configurations about macro.cpp</p> <pre><code>//Check if the name of dir is ok\nif (string(directoryToSave).back() != string(\"/\"))\n{\ncerr &lt;&lt; \"To avoid errors, please end the result directory with a \\\"/\\\"\" &lt;&lt; endl;\nabort();\n}\n\n//Check if dir exists and create\nif (gSystem-&gt;AccessPathName(directoryToSave))\n{\nif (gSystem-&gt;mkdir(directoryToSave, true))\n{\ncerr &lt;&lt; \"\\\"\" &lt;&lt; directoryToSave &lt;&lt; \"\\\" path could not be found and could not be created ERROR\" &lt;&lt; endl;\ncerr &lt;&lt; \"Try to create manually this folder path\" &lt;&lt; endl;\nabort();\n}\nelse\n{\ncout &lt;&lt; \"\\\"\" &lt;&lt; directoryToSave &lt;&lt; \"\\\" directory created OK\" &lt;&lt; endl;\n}\n}\nelse\n{\ncout &lt;&lt; \"\\\"\" &lt;&lt; directoryToSave &lt;&lt; \"\\\" directory OK\" &lt;&lt; endl;\n}\n</code></pre> <p>Check if the <code>directoryToSave</code> (setted in settings.cpp) has a valid name and if exists. If not, the code creates the folder.</p> <pre><code>//Compatibility adjusts on file read (for data_histoall ntupples)\nstring folderName = \"tagandprobe/\";\nif (needsRetroCompatibility)\nfolderName = \"demo/\";\n\n//Open and read files\nTFile *file0  = TFile::Open(files[useFile]);\nTTree *TreePC = (TTree*)file0-&gt;Get((folderName + \"PlotControl\").data());\nTTree *TreeAT = (TTree*)file0-&gt;Get((folderName + \"AnalysisTree\").data());\ncout &lt;&lt; \"Using \\\"\" &lt;&lt; files[useFile] &lt;&lt; \"\\\" ntupple\" &lt;&lt; endl;\n</code></pre> <p>This part is responsible to open the file and do conversions. The first one file is a bit different of the other ones, so it needs compatibiliy besides its not important anymore and is a obsolete file.</p> <pre><code>//Create variables\ndouble ProbeMuon_Pt;\ndouble ProbeMuon_Eta;\ndouble ProbeMuon_Phi;\ndouble TagMuon_Pt;\ndouble TagMuon_Eta;\ndouble TagMuon_Phi;\ndouble InvariantMass;\nint PassingProbeTrackingMuon;\nint PassingProbeStandAloneMuon;\nint PassingProbeGlobalMuon;\n\n//Assign variables\nTreePC-&gt;SetBranchAddress(\"ProbeMuon_Pt\",                &amp;ProbeMuon_Pt);\nTreePC-&gt;SetBranchAddress(\"ProbeMuon_Eta\",               &amp;ProbeMuon_Eta);\nTreePC-&gt;SetBranchAddress(\"ProbeMuon_Phi\",               &amp;ProbeMuon_Phi);\nTreePC-&gt;SetBranchAddress(\"TagMuon_Pt\",                  &amp;TagMuon_Pt);\nTreePC-&gt;SetBranchAddress(\"TagMuon_Eta\",                 &amp;TagMuon_Eta);\nTreePC-&gt;SetBranchAddress(\"TagMuon_Phi\",                 &amp;TagMuon_Phi);\nif (needsRetroCompatibility)\nTreePC-&gt;SetBranchAddress(\"InvariantMass\",               &amp;InvariantMass);\nelse\nTreeAT-&gt;SetBranchAddress(\"InvariantMass\",               &amp;InvariantMass);\nTreeAT-&gt;SetBranchAddress(\"PassingProbeTrackingMuon\",    &amp;PassingProbeTrackingMuon);\nTreeAT-&gt;SetBranchAddress(\"PassingProbeStandAloneMuon\",  &amp;PassingProbeStandAloneMuon);\nTreeAT-&gt;SetBranchAddress(\"PassingProbeGlobalMuon\",      &amp;PassingProbeGlobalMuon);\n\ndouble* quantities[] = {&amp;ProbeMuon_Pt,\n&amp;ProbeMuon_Eta,\n&amp;ProbeMuon_Phi,\n&amp;TagMuon_Pt,\n&amp;TagMuon_Eta,\n&amp;TagMuon_Phi,\n&amp;InvariantMass,\n};\n\nint* types[] = {&amp;PassingProbeTrackingMuon,\n&amp;PassingProbeStandAloneMuon,\n&amp;PassingProbeGlobalMuon\n};\n</code></pre> <p>Now variables are created and linked to branches in ntupple. Then a array of these variables are set.</p> <pre><code>//Create a object and set configs\nSidebandSubtraction SdS{resonance};\nSdS.canvasWatermark = canvasWatermark;\nSdS.directoryToSave = directoryToSave;\nSdS.doTracker       = doTracker;\nSdS.doStandalone    = doStandalone;\nSdS.doGlobal        = doGlobal;\nSdS.doTagMuon       = doTagMuon;\nSdS.doProbeMuon     = doProbeMuon;\n\ncout &lt;&lt; \"resonance: \" &lt;&lt; SdS.resonance &lt;&lt; \"\\n\";\ncout &lt;&lt; \"Using subtraction factor as integral of background fit\\n\";\n</code></pre> <p>The macro.cpp now creates the SdS object and assign variables setted in settings.cpp. At this point, it creates all histograms that you will need such as invariant mass histograms and pT, eta, phi histograms.</p> <pre><code>//Get data size and set data limit if has\nlong long numberEntries = TreePC-&gt;GetEntries();\nif (limitData &gt; 0 &amp;&amp; limitData &lt; numberEntries)\nnumberEntries = limitData;\nprintf(\"Data analysed = %lld of %lld\\n\", numberEntries, TreePC-&gt;GetEntries());\n\n//Prepare for showing progress\nstring progressFormat = \"Progress: %05.2f%% %0\"+to_string(strlen(to_string(numberEntries).data()))+\"lld/%lld\\r\";\nauto lastTime = std::chrono::steady_clock::now();\nauto start    = std::chrono::steady_clock::now();\n</code></pre> <p>Now the code are limiting data if you setted and setting a string for progress information while filling histograms.</p> <pre><code>cout &lt;&lt; \"\\nFilling Invariant Mass Histograms..... (1/2)\\n\";\n\n//Loop between the components\nfor (long long i = 0; i &lt; numberEntries; i++)\n{\n//Select particle pair\nTreePC-&gt;GetEntry(i);\nTreeAT-&gt;GetEntry(i);\n\n//Show progress on screen\nif (chrono::duration_cast&lt;chrono::milliseconds&gt;(chrono::steady_clock::now() - lastTime).count() &gt;= 1000 || i == numberEntries - 1)\n{\nprintf(progressFormat.data(), (float)(i+1)/(float)numberEntries*100, i+1, numberEntries);\nlastTime = chrono::steady_clock::now();\n}\n\n//Fill histograms\nif (applyCuts(quantities, types))\n{\nSdS.fillMassHistograms(quantities, types);\n}\n}\n\ncout &lt;&lt; \"\\nTook \" &lt;&lt; chrono::duration_cast&lt;chrono::milliseconds&gt;(chrono::steady_clock::now() - start).count() &lt;&lt; \" ms\\n\";\n</code></pre> <p>This part of the code fill invariant mass histograms. Cuts are applyied in cuts.h. At this point, macro.cpp separes in passing and all muons.</p> <pre><code>//Do function fit over the histogram\nSdS.doFit();\n\n//Get values for invariant mass and sigma from plot\nSdS.updateMassValuesAll();\n</code></pre> <p>After filling mass histograms, it is necessary to apply the fit function.</p> <p>After doing fit, updateMassValuesAll() get regions for sideband subtraction mostly based in fitting.</p> <pre><code>//-------------------------------------\n// Generate and save files\n//-------------------------------------\n\n//Create file root to store generated files\nTFile* generatedFile = TFile::Open((string(directoryToSave) + \"generated_hist.root\").data(),\"RECREATE\");\ngeneratedFile-&gt;mkdir(\"canvas/\");\ngeneratedFile-&gt;   cd(\"canvas/\");\n\nif (shouldDrawInvariantMassCanvas)\n{\nbool drawRegions    = false;\nbool shouldWrite    = true;\nbool shouldSavePNG  = true;\n\nSdS.createMassCanvas(drawRegions, shouldWrite, shouldSavePNG);\n}\n\nif (shouldDrawInvariantMassCanvasRegion &amp;&amp; !isMC)\n{\nbool drawRegions    = true;\nbool shouldWrite    = true;\nbool shouldSavePNG  = true;\n\nSdS.createMassCanvas(drawRegions, shouldWrite, shouldSavePNG);\n}\n</code></pre> <p>Canvas are drawn and saved in the <code>generated_hist.root</code> file and in the folder as <code>.png</code>.</p> <pre><code>//Prepare for showing progress\nlastTime = std::chrono::steady_clock::now();\nstart    = std::chrono::steady_clock::now();\n\ncout &lt;&lt; \"\\nFilling Quantities Histograms..... (2/2)\\n\";\n\n//Loop between the components again\nfor (long long i = 0; i &lt; numberEntries; i++)\n{\n//Select particle pair\nTreePC-&gt;GetEntry(i);\nTreeAT-&gt;GetEntry(i);\n\n//Show progress on screen\nif (chrono::duration_cast&lt;chrono::milliseconds&gt;(chrono::steady_clock::now() - lastTime).count() &gt;= 1000 || i == numberEntries - 1)\n{\nprintf(progressFormat.data(), (float)(i+1)/(float)numberEntries*100, i+1, numberEntries);\nlastTime = chrono::steady_clock::now();\n}\n\n//Fill histograms\nif (applyCuts(quantities, types))\n{   SdS.fillQuantitiesHistograms(quantities, types);\n}\n}\ncout &lt;&lt; \"\\nTook \" &lt;&lt; chrono::duration_cast&lt;chrono::milliseconds&gt;(chrono::steady_clock::now() - start).count() &lt;&lt; \" ms\\n\";\n</code></pre> <p>At this point of the code, this will separate all histogram in signal + background (signal region) and background (sideband region) due the regions for sideband choosen before.</p> <pre><code>//Normalize Histograms for variable binning\ncout &lt;&lt; \"\\n\";\nSdS.normalizeHistograms();  </code></pre> <p>After folling histograms, as some of them has variable bins, it needs to be normalized. This function does this.</p> <pre><code>//For sideband subtraction\nSdS.subtractSigHistograms();\n</code></pre> <p>Subtract <code>background</code> from <code>signal + background</code> histogram to create signal histogram. This method is what is called sideband subtraction.</p> <pre><code>if (shouldDrawQuantitiesCanvas)\n{\nbool shouldWrite    = true;\nbool shouldSavePNG  = true;\n\ncout &lt;&lt; endl;\nSdS.createQuantitiesCanvas(shouldWrite, shouldSavePNG);\n}\n</code></pre> <p>The code here draw the canvas for all pT, eta and phi quantities it has. Including <code>background</code>, <code>signal</code> and <code>signal + background</code>.</p> <pre><code>//Debug consistency for histograms\nSdS.consistencyDebugCout();\n</code></pre> <p>This is a checker of how consistent is our result values and print on terminal results. For all histograms this calculations should result 0. For more details about how exactly it works, see consistencyDebugCout().</p> <pre><code>//Save histograms\ngeneratedFile-&gt;mkdir(\"histograms/\");\ngeneratedFile-&gt;   cd(\"histograms/\");\n\n//Write quantities histograms on file\n{\nbool writehSigBack  = true;\nbool writehSig      = true;\nbool writehBack     = true;\n\nSdS.writeQuantitiesHistogramsOnFile(writehSigBack, writehSig, writehBack);\n}\n\n//Write mass histograms on file\n{\nbool writehPass = true;\nbool writehAll  = true;\n\nSdS.writeMassHistogramsOnFile(writehPass, writehAll);\n}\n</code></pre> <p>At this point, the code will write all histograms in a folder in the <code>.root</code> generated file. Including mass histograms and quantities histograms.</p> <pre><code>//Save plots\ngeneratedFile-&gt;mkdir(\"efficiency/plots/\");\ngeneratedFile-&gt;cd(\"efficiency/plots/\");\n\n//Creates efficiency plots\n{\nbool shouldWrite    = true;\n\nSdS.createEfficiencyPlot(shouldWrite);\n}\n</code></pre> <p>It calculates the efficiency of the quantities by using TEfficiency class of ROOT. Then saves the plots in another folder inside the <code>.root</code> file.</p> <pre><code>//Saves new histograms and canvas in file\ngeneratedFile-&gt;mkdir(\"efficiency/canvas/\");\ngeneratedFile-&gt;cd(\"efficiency/canvas/\");\n\nif (shouldDrawEfficiencyCanvas)\n{\nbool shouldWrite    = true;\nbool shouldSavePNG  = true;\n\ncout &lt;&lt; \"\\n\";\nSdS.createEfficiencyCanvas(shouldWrite, shouldSavePNG);\n}\n\n//Close files\ngeneratedFile-&gt;Close();\n\ncout &lt;&lt; \"\\nDone. All result files can be found at \\\"\" &lt;&lt; SdS.directoryToSave &lt;&lt; \"\\\"\\n\\n\";\n</code></pre> <p>The end point of this function. It creates a canvas for every efficiency plot calculated above and also saves in the generated file. After this, the task is done.</p>"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#results","title":"Results","text":"<p>All results are saved in a folder setted in <code>directoryToSave</code> variable. The result contains a file <code>.root</code> with all canvas, histograms and plots aside of <code>.png</code> images of all canvas created.</p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/01-introduction/","title":"Introduction","text":""},{"location":"analysis/selection/idefficiencystudy/tutorial/01-introduction/#what-is-the-tag-and-probe-method","title":"What is the tag and probe method?","text":"<p>The tag and probe method is a data-driven technique for measuring particle detection efficiencies. It is based on the decays of known resonances (e.g. J/\u03c8, \u03a5 and Z) to pairs of the particles being studied. In this exercise, these particles are muons, and the J/\u03c8 resonance is nominally used.</p> <p>The determination of the detector efficiency is a critical ingredient in any physics measurement. It accounts for the particles that were produced in the collision but escaped detection (did not reach the detector elements, were missed by the reconstructions algorithms, etc). It can be in general estimated using simulations, but simulations need to be calibrated with data. The T&amp;P method here described provides a useful and elegant mechanism for extracting efficiencies directly from data!.</p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/01-introduction/#what-is-tag-and-probe","title":"What is \"tag\" and \"probe\"?","text":"<p>The resonance, used to calculate the efficiencies, decays to a pair of particles: the tag and the probe.</p> <ul> <li>Tag muon = well identified, triggered muon (tight selection criteria).</li> <li>Probe muon = unbiased set of muon candidates (very loose selection criteria), either passing or failing the criteria for which the efficiency is to be measured.</li> </ul>"},{"location":"analysis/selection/idefficiencystudy/tutorial/01-introduction/#how-do-we-calculate-the-efficiency","title":"How do we calculate the efficiency?","text":"<p>The efficiency is given by the fraction of probe muons that pass a given criteria (in this case, the Muon ID which we explain below):</p> <p></p> <p>The denominator corresponds to the number of resonance candidates (tag+probe pairs) reconstructed in the dataset. The numerator corresponds to the subset for which the probe passes the criteria.</p> <p>The tag+probe invariant mass distribution is used to select only signal, that is, only true J/\u03c8 candidates decaying to dimuons. This is achieved in this exercise by the usage of two methods: fitting and side-band-subtraction.</p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/01-introduction/#cms-muon-identification-and-reconstruction","title":"CMS Muon identification and reconstruction","text":"<p>The final objective in this lesson is to measure the efficiency for identifying reconstructed tracker muons.  We present here a short description of the muon identification and reconstruction employed in the CMS experiment at the LHC.</p> <p></p> <p>In the standard CMS reconstruction for proton-proton collisions, tracks are first reconstructed independently in the inner tracker and in the muon system. Based on these objects, two reconstruction approaches are used:</p> <ul> <li> <p>Tracker Muon reconstruction (red line): In this approach, all tracker tracks with pT &gt; 0.5 GeV/c and total momentum p &gt; 2.5 GeV/c are considered as possible muon candidates and are extrapolated to the muon system taking into account the magnetic field;</p> </li> <li> <p>Standalone Muon reconstruction (green line): they are all tracks of the segments reconstructed in the muon chambers (performed using segments and hits from Drift Tubes - DTs in the barrel region, Cathode strip chambers - CSCs in the endcaps and Resistive Plates Chambers - RPCs for all muon system) are used to generate \"seeds\" consisting of position and direction vec01/tors and an estimate of the muon transverse momentum;</p> </li> <li> <p>Global Muon reconstruction (blue line): For each standalone-muon track, a matching tracker track is found by comparing parameters of the two tracks propagated onto a common surface.</p> </li> </ul> <p>You can find more details concerning CMS Muon Identification and reconstruction in this paper JINST 7 (2012) P10002.</p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/02-sidebandsubtraction/","title":"Sideband","text":""},{"location":"analysis/selection/idefficiencystudy/tutorial/02-sidebandsubtraction/#signal-extraction-sideband-subtraction-method","title":"Signal extraction: sideband subtraction method","text":"<p>The reconstruction efficiency is calculated using only signal muons. In order to measure the efficiency, we need a way to extract signal from the dataset. You've used the fitting method and now you'll meet the sideband subtraction method.</p> <p>This method consists in choosing sideband and signal regions in invariant mass distribution. The sideband regions (shaded in red in the figure) have background particles and the signal region (shared in green in the figure) has background and signal particles.</p> <p></p> <p>Note</p> <p>The background corresponds to candidates that do not correspond to the decay of a genuine resonance; for example, the pair is formed by the tag muon associated to an uncorrelated track produced elsewhere in the collision; the corresponding invariant mass has thus a smooth continuous shape, that is extrapolated from the signal regions into the sideband region.</p> <p>For each event category (i.e. Pass and All), and for a given variable of interest (e.g., the probe pT), two distributions are obtained, one for each region (Signal and Sideband). In order to obtain the variable distribution for the signal only, we proceed by subtracting the Background distribution (Sideband region) from the Signal+Background one (Signal region):</p> <p></p> <p>Where the normalization \u03b1 factor quantifies the quantity of background present in the signal region:</p> <p></p> <p>And for the uncertainty:</p> <p></p> <p>Applying those equations we get histograms like this:</p> <p></p> <ul> <li>Solid blue line (Total) = particles in signal region;</li> <li>Dashed blue line (Background) = particles in sideband regions;</li> <li>Solid magenta line (signal) = signal histogram (background subtracted).</li> </ul> <p>You will see this histogram on this exercise.</p> <p>About this code</p> <p>More info about this code can be found in the reference guide.</p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/02-sidebandsubtraction/#preparing-files","title":"Preparing files","text":"<p>First, from the root folder of our downloaded repository, we need to go sideband subtraction method tutorial:</p> <pre><code>cd efficiency_tools/sideband_subtraction\n</code></pre> <p>To copy the J/\u03c8 dataset of real data file to your machine (requires 3,3 GB), type:</p> <pre><code>wget -O Run2011AMuOnia_mergeNtuple.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?files=Run2011AMuOnia_mergeNtuple.root\"\n</code></pre> <p>Run this code to download the simulation dataset for J/\u03c8 (requires 492 MB):</p> <pre><code>wget -O JPsiToMuMu_mergeMCNtuple.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?files=JPsiToMuMu_mergeMCNtuple.root\"\n</code></pre> <p>Now, check if everything is ok:</p> <pre><code>ls\n</code></pre> <pre><code>JPsiToMuMu_mergeMCNtuple.root  main  README.md  Run2011AMuOnia_mergeNtuple.root\n</code></pre> <p>Your <code>sideband_subtraction</code> folder should have these files:</p> <p></p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/02-sidebandsubtraction/#preparing-code-for-data","title":"Preparing code for Data","text":"<p>Note</p> <p>This tutorial will teach you to manage the files on the terminal, but you can use a graphical file explorer or any other way you are used to.</p> <p>We need to edit some settings. Open settings.cpp:</p> <pre><code>cd main/config\nls\n</code></pre> <pre><code>createHistogram.h  cuts.h  settings.cpp\n</code></pre> <p>There are different ways to open this file. You can try to run:</p> <pre><code>gedit settings.cpp\n</code></pre> <p>Or, if you can not use gedit, try nano:</p> <pre><code>nano settings.cpp\n</code></pre> <p>I do not have nano!</p> <p>You can try to use any text editor, but here is some commands you cant try to use to install it:</p> <ul> <li>Ubuntu/Debian: <code>sudo apt-get -y install nano</code>.</li> <li>RedHat/CentOS/Fedora: <code>sudo yum install nano</code>.</li> <li>Mac OS X: <code>nano is installed by default</code>.</li> </ul> <p>We want to calculate efficiencies of tracker muons. With the settings.cpp file opened, make sure to let the variables like this:</p> <pre><code>//Canvas drawing\nbool shouldDrawInvariantMassCanvas       = true;\nbool shouldDrawInvariantMassCanvasRegion = true;\nbool shouldDrawQuantitiesCanvas          = true;\nbool shouldDrawEfficiencyCanvas          = true;\n\n//Muon id analyse   \nbool doTracker    = true;\nbool doStandalone = false;\nbool doGlobal     = false;\n\n//quantity analyse\nbool doPt  = true;\nbool doEta = true;\nbool doPhi = true;\n</code></pre> <p>We want to calculate the efficiency using specific files that we downloaded. They name are <code>Run2011AMuOnia_mergeNtuple.root</code> and <code>JPsiToMuMu_mergeMCNtuple.root</code> and are listed in <code>const char *files[]</code>. While settings.cpp is open, try to use the variable <code>int useFile</code> to run <code>Run2011AMuOnia_mergeNtuple.root</code>.</p> How to do this <p>Make sure <code>useFile</code> is correct:</p> <pre><code>//List of files\nconst char *files[] = {\"../data_histoall.root\",\n\"../Run2011AMuOnia_mergeNtuple.root\",\"\"\n\"../JPsiToMuMu_mergeMCNtuple.root\",\n\"../Run2011A_MuOnia_Upsilon.root\",\n\"../Upsilon1SToMuMu_MC_full.root\"};\n\nconst char* directoriesToSave[] = {\"../results/result/\",\n\"../results/Jpsi Run 2011/\",\n\"../results/Jpsi MC 2020/\",\n\"../results/Upsilon Run 2011/\",\n\"../results/Upsilon MC 2020/\"};\n\n\n//MAIN OPTIONS\n\n//Which file of files (variable above) should use\nint useFile = 1;\n</code></pre> <p>It will tell which configuration the program will use. So, the macro will run with the ntuple in <code>files[useFile]</code> and the results will be stored in <code>directoriesToSave[useFile]</code>.</p> <p>the first three files won't be used in this exercise.</p> <p>About code</p> <p>Normally we need to set the variable <code>const char* resonance</code>, but at this time it is already done and set automatically for these ntuples' names.</p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/02-sidebandsubtraction/#editting-bins","title":"Editting bins","text":"<p>The code allows to define the binning of the kinematic variable, to ensure each bin is sufficiently populated, for increased robustness. To change the binning, open createHistogram.h that is on same folder that settings.cpp:</p> <pre><code>gedit createHistogram.h\n</code></pre> <p>Search for the <code>createEfficiencyPlot(...)</code> function. You'll find something like this:</p> <pre><code>void createHistogram(TH1D* &amp;histo, const char* histoName)\n{...}\n</code></pre> <p>For each quantity (pT, eta, phi) we used different bins. To change the bins, look inside the <code>createEfficiencyPlot(...)</code> function. In a simpler version, you'll see a structure like this:</p> <pre><code>//Variable bin for pT\nif (strcmp(quantityName, \"Pt\") == 0)\n{\n//Here creates histogram for pT\n}\n\n//Variable bin for eta\nelse if (strcmp(quantityName, \"Eta\") == 0)\n{\n//Here creates histogram for eta\n}\n\n//Bins for phi\nelse\n{\n//Here creates histogram for phi\n}\n</code></pre> See the whole scructure <p>Don't be scared! Code does'nt bite.</p> <pre><code>//Variable bin for pT\nif (strcmp(quantityName, \"Pt\") == 0)\n{\ndouble xbins[] = {0., 2.0, 3.4, 4.0, 4.4, 4.7, 5.0, 5.6, 5.8, 6.0, 6.2, 6.4, 6.6, 6.8, 7.3, 9.5, 13.0, 17.0, 40.};\n\nint nbins = sizeof(xbins)/sizeof(*xbins) - 1;\nhisto = new TH1D(hName.data(), hTitle.data(), nbins, xbins);\n}\n\n//Variable bin for eta\nelse if (strcmp(quantityName, \"Eta\") == 0)\n{\ndouble xbins[] = {-2.4, -1.8, -1.4, -1.2, -1.0, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1.0, 1.2, 1.4, 1.8, 2.4};\n\nint nbins = sizeof(xbins)/sizeof(*xbins) - 1;\nhisto = new TH1D(hName.data(), hTitle.data(), nbins, xbins);\n}\n\n//Bins for phi\nelse\n{\ndouble xbins[] = {-3.0, -1.8, -1.6, -1.2, -1.0, -0.7, -0.4, -0.2, 0, 0.2, 0.4, 0.7, 1.0, 1.2, 1.6, 1.8, 3.0};\n\nint nbins = sizeof(xbins)/sizeof(*xbins) - 1;\nhisto = new TH1D(hName.data(), hTitle.data(), nbins, xbins);\n}\n\n//Edit histogram axis\nhisto-&gt;GetYaxis()-&gt;SetTitle(Form(yAxisTitleForm.data(), histo-&gt;GetBinWidth(0)));\nhisto-&gt;GetXaxis()-&gt;SetTitle(xAxisTitle.data());\n</code></pre> <p>The code that creates the histogram bins is located inside the conditionals and is commented. You can edit this code and uncomment to create histogram bins however you want. Instead of using a function to generate the bins, we can also define them manually.</p> <p>As we intend to compare the results between data and simulation, but also between the sideband and fitting methods. You are advised to employ the same bin choice. Garantee your the code uses same bin as the previous here:</p> <pre><code>    //Variable bin for pT\nif (strcmp(quantityName, \"Pt\") == 0)\n{\ndouble xbins[] = {0., 2.0, 3.4, 4.0, 4.4, 4.7, 5.0, 5.6, 5.8, 6.0, 6.2, 6.4, 6.6, 6.8, 7.3, 9.5, 13.0, 17.0, 40.};\n\nint nbins = sizeof(xbins)/sizeof(*xbins) - 1;\nhisto = new TH1D(hName.data(), hTitle.data(), nbins, xbins);\n}\n\n//Variable bin for eta\nelse if (strcmp(quantityName, \"Eta\") == 0)\n{\ndouble xbins[] = {-2.4, -1.8, -1.4, -1.2, -1.0, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1.0, 1.2, 1.4, 1.8, 2.4};\n\nint nbins = sizeof(xbins)/sizeof(*xbins) - 1;\nhisto = new TH1D(hName.data(), hTitle.data(), nbins, xbins);\n}\n\n//Bins for phi\nelse\n{\ndouble xbins[] = {-3.0, -1.8, -1.6, -1.2, -1.0, -0.7, -0.4, -0.2, 0, 0.2, 0.4, 0.7, 1.0, 1.2, 1.6, 1.8, 3.0};\n\nint nbins = sizeof(xbins)/sizeof(*xbins) - 1;\nhisto = new TH1D(hName.data(), hTitle.data(), nbins, xbins);\n}\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/tutorial/02-sidebandsubtraction/#running-the-code","title":"Running the code","text":"<p>After setting the configurations, it's time to run the code. Go back to the main directory and make sure <code>macro.cpp</code> is there.</p> <pre><code>cd ..\nls\n</code></pre> <pre><code>classes  compare_efficiency.cpp  config  macro.cpp\n</code></pre> <p>Run the macro.cpp:</p> <pre><code>root -l -b -q macro.cpp\n</code></pre> <pre><code>\"../results/Jpsi_Run_2011/\" directory created OK\nUsing \"../Run2011AMuOnia_mergeNtuple.root\" ntupple\nresonance: Jpsi\nUsing subtraction factor as integral of background fit\nData analysed = 5950253 of 5950253\n</code></pre> <p>Note</p> <p>As this dataset is larger, the code will run slowly. It can take several minutes to be completed depending where the code is been running.</p> <p>In this process, more informations will be printed in terminal while plots will be created on a specified folder. The message below tells you that code has finished running:</p> <pre><code>Done. All result files can be found at \"../results/Jpsi_Run_2011/\"\n</code></pre> <p>Common errors</p> <p>If you run the code and your terminal printed some erros like:</p> <pre><code>Error in &lt;ROOT::Math::Cephes::incbi   : Wrong domain for parameter b (must be     0)\n</code></pre> <p>This occurs when the contents of a bin of the pass histogram is greater than the corresponding bin in the total histogram. With sideband subtraction, depending on bins you choose, this can happen and will result in enormous error bars.</p> <p>This issue may be avoided by fine-tuning the binning choice. For now, these messages may be ignored.</p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/02-sidebandsubtraction/#probe-efficiency-results-for-data","title":"Probe Efficiency results for Data","text":"<p>If all went well, your results are going to be like these:</p> <p> </p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/02-sidebandsubtraction/#preparing-and-running-the-code-for-simulation","title":"Preparing and running the code for simulation","text":"<p>Challenge</p> <p>Try to run the same code on the <code>JPsiToMuMu_mergeMCNtuple.root</code> file we downloaded.</p> Tip <p>You will need the redo the steps above, but setting:</p> <pre><code>int useFile = 2;\n</code></pre> <p>in <code>main/config/settings.cpp</code> file.</p> <p>Comparison between real data and simulation</p> <p>We'll do this in the last section of this exercise. So the challenge above is mandatory.</p> <p>Extra challenge</p> <p>If you are looking for an extra exercise, you can try to apply the same logic, changing some variables you saw, in order to get results for the \u03a5 nutpple.</p> <p>To download the \u03a5 real data ntupple (requires 442 MB):</p> <pre><code>wget -O Run2011A_MuOnia_Upsilon.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?files=Run2011A_MuOnia_Upsilon.root\"\n</code></pre> <p>Run this code to download the simulation dataset for \u03a5 (requires 67 MB):</p> <pre><code>wget -O Upsilon1SToMuMu_MC_full.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?files=Upsilon1SToMuMu_MC_full.root\"\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/tutorial/03-fitting/","title":"Fitting method","text":""},{"location":"analysis/selection/idefficiencystudy/tutorial/03-fitting/#signal-extraction-fitting-method","title":"Signal extraction: Fitting method","text":"<p>In this method, the signal is extracted not by histogram manipulation but by likelihood fitting. The procedure is applied after splitting the data in sub-samples, corresponding to bins of the kinematic variable of interest of the probe objects. As such, the efficiency will be measured as a function of that variable. Each sub-sample contains signal and background events; the signal is accessed by fitting the invariant mass spectra</p> <p>The fit for each bin allows to statistically discriminate between signal and background. In particular, the fit yields the number of signal events. The efficiency is finally obtained by simply forming the ratio of the signal yield from the fit to the passing category by the signal yield from the fit of the inclusive all category. This approach is illustrated below.</p> <p></p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/03-fitting/#setting-it-up","title":"Setting it up","text":"<p>First, clone this repository.</p> <pre><code>git clone --branch systematic https://github.com/RodrigoCampelloS/TagAndProbe.git\ncd TagAndProbe/efficiency_tools/fitting\n</code></pre> <p>You will also need to download the TagAndProbe_Z_Run2012.root file using this link.</p> <pre><code>https://cernbox.cern.ch/files/link/public/lqHEasYWJpOZsfq/simplified_datasets_for_fitting_method?items-per-page=100&amp;view-mode=resource-table&amp;tiles-size=1\n</code></pre> <p>For MC data you need to download the TagAndProbe_Z_MC.root using the same link. You can also download the TagAndProbe_Jpsi_Run2011.root and TagAndProbe_Jpsi_MC.root files if you wish to use the Jpsi particle instead.</p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/03-fitting/#editing-the-fit","title":"Editing the Fit","text":"<p>You will have 3 options to edit in order to better fit your needs.</p> <p>1- Which type of data do you want to use?</p> <pre><code>//uncomment the fit you want to do \n#include \"src/dofits/DoFit_Z.h\"\n//#include \"src/dofits/DoFit_Z_MC.h\"\n</code></pre> <p>2- Which Muon Id you want to use?</p> <pre><code>//Which Muon Id do you want to study?\n//string MuonId   = \"trackerMuon\";\n//string MuonId   = \"standaloneMuon\";\nstring MuonId   = \"globalMuon\";\n</code></pre> <p>3- Which quantity you want to use?</p> <pre><code>//Which Quantity?\nstring quantity = \"Pt\";     double bins[] = {15.0,20.0,25.0,30.0,40.0,120.0};\n//string quantity = \"Eta\";    double bins[] = {-2.4, -1.8, -1.4, -1.2, -1.0, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1.0, 1.2, 1.4, 1.8, 2.4};\n//string quantity = \"Phi\";    double bins[] = {-3.0, -1.8, -1.6, -1.2, -1.0, -0.7, -0.4, -0.2, 0, 0.2, 0.4, 0.7, 1.0, 1.2, 1.6, 1.8, 3.0};\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/tutorial/03-fitting/#running-the-code","title":"Running the Code","text":"<p>In order to do the fit you will have to run this code:</p> <pre><code>root efficiency_Z.cpp </code></pre> <p>You should get something like this:</p> <p></p> <p>Note</p> <p>for the Jpsi particle use efficiency_Jpsi.cpp</p> <p>You can always further check the result using the TBrowser</p> <pre><code>root\nnew TBrowser\n</code></pre> <p>Go to results&gt;efficiencies&gt;efficiency&gt;Z_Run&gt;Pt_GlobalMuon.root&gt;globalMuon_Pt_Efficiency and you should get something like this:</p> <p></p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/04-comparison/","title":"How to make a comparison between two previous methods of signal extraction","text":""},{"location":"analysis/selection/idefficiencystudy/tutorial/04-comparison/#how-sideband-subtraction-method-code-stores-its-files","title":"How sideband subtraction method code stores its files","text":"<p>the Sideband subtraction code saves every efficiency plot in <code>efficiency/plots/</code> folder inside a single <code>generated_hist.root</code> file. Lets check it!</p> <p>You're probably on the <code>main</code> directory. Lets go back one directory.</p> <pre><code>cd ..\nls\n</code></pre> <pre><code>JPsiToMuMu_mergeMCNtuple.root  README.md  Run2011AMuOnia_mergeNtuple.root\nmain                           results\n</code></pre> <p>A folder named <code>results</code> showed up on this folder. Lets go check its content.</p> <pre><code>cd results\nls\n</code></pre> <pre><code>Jpsi_MC_2020  Jpsi_Run_2011\n</code></pre> <p>If you did every step of the sideband subtraction on this page lesson, these results should match with the results on your pc. Access one of those folders (except comparison).</p> <pre><code>cd Jpsi_Run_2011\nls\n</code></pre> <pre><code>Efficiency_Tracker_Probe_Eta.png  Tracker_Probe_Eta_All.png\nEfficiency_Tracker_Probe_Phi.png  Tracker_Probe_Eta_Passing.png\nEfficiency_Tracker_Probe_Pt.png   Tracker_Probe_Phi_All.png\ngenerated_hist.root               Tracker_Probe_Phi_Passing.png\nInvariantMass_Tracker.png         Tracker_Probe_Pt_All.png\nInvariantMass_Tracker_region.png  Tracker_Probe_Pt_Passing.png\n</code></pre> <p>Here, all the output plots you saw when running the sideband subtraction method are stored as a <code>.png</code>. Aside from them, there's a <code>generated_hist.root</code> that stores the efficiency in a way that we can manipulate it after. This file is needed to run the comparison between efficiencies for the sideband subtraction method. Lets look inside of this file.</p> <p>Run this command to open <code>generated_hist.root</code> with ROOT:</p> <pre><code>root -l generated_hist.root\n</code></pre> <pre><code>root [0]\nAttaching file generated_hist.root as _file0...\n(TFile *) 0x55dca0f04c50\nroot [1]\n</code></pre> <p>Lets check its content. Type on terminal:</p> <pre><code>new TBrowser\n</code></pre> <p>You should see something like this:</p> <p></p> <p>This is a visual navigator of a <code>.root</code> file. Here you can see the struture of <code>generated_hist.root</code>. Double click the folders to open them and see their content. The Efficiency plots we see are stored in <code>efficiency/plots/</code> folder:</p> <p></p> <p>You can double click each plot to see its content:</p> <p></p> <p>Tip</p> <p>To close this window, click on terminal and press <code>Ctrl</code> + <code>C</code>. This command stops any processes happening in the terminal.</p> <p>Key Point</p> <ul> <li>As you see, the <code>.root</code> file has paths inside of it and the efficiencies plots have a self path inside them as well!</li> </ul>"},{"location":"analysis/selection/idefficiencystudy/tutorial/04-comparison/#comparison-results-between-real-data-and-simulations-for-sideband-method","title":"Comparison results between real data and simulations for sideband method","text":"<p>After runinng the sideband subtraction code, we get a <code>.root</code> with all the efficiencies plots inside it in two different folders:</p> <ul> <li><code>../results/Jpsi_Run_2011/generated_hist.root</code></li> <li><code>../results/Jpsi_MC_2020/generated_hist.root</code></li> </ul> <p>We'll get back to this on the discussion below.</p> <p>Head back to the <code>main</code> folder. Inside of it there is a code for the efficiency plot comparison. Lets check it out. From the <code>sideband_subtraction</code> folder, type:</p> <pre><code>cd main\nls\n</code></pre> <pre><code>classes  compare_efficiency.cpp  config  macro.cpp\n</code></pre> <p>There is it. Now lets open it.</p> <pre><code>gedit compare_efficiency.cpp\n</code></pre> <p>Its easy to prepare it for the sideband subtraction comparison. Our main editing point can be found in this part:</p> <pre><code>//CONFIGS\n\nint useScheme = 0;\n//Jpsi    Sideband Run vs Jpsi    Sideband MC\n//Jpsi    Fitting  Run vs Jpsi    Fitting  MC\n//Jpsi    Sideband Run vs Jpsi    Fitting  Run\n//Upsilon Sideband Run vs Upsilon Sideband MC\n//Upsilon Fitting  Run vs Upsilon Fitting  MC\n//Upsilon Sideband Run vs Upsilon Fitting  Run\n\n//Muon id analyse\nbool doTracker    = true;\nbool doStandalone = true;\nbool doGlobal     = true;\n\n//quantity analyse\nbool doPt  = true;\nbool doEta = true;\nbool doPhi = true;\n</code></pre> <p>Note</p> <p>In the scope above we see:</p> <ul> <li><code>int useScheme</code> represents which comparison you are doing.</li> <li><code>bool doTracker</code> is a variable that allow plots for tracker muons.</li> <li><code>bool doStandalone</code> is a variable that allow plots for standalone muons.</li> <li><code>bool doGlobal</code> is a variable that allow plots for global muons.</li> <li><code>bool doPt</code> is a variable that allow plots for muon pT.</li> <li><code>bool doEta</code> is a variable that allow plots for muon \u03b7.</li> <li><code>bool doPhi</code> is a variable that allow plots for muon \u03c6.</li> </ul> <p>Everything is up to date to compare sideband subtraction's results between real data and simulations, except it is comparing standalone and global muons. As we are  looking for tracker muons efficiencies only, you should switch to false variables for Standalone and Global.</p> <p>Also, you will need to change the <code>useScheme</code> variable to plot what you want to plot. As we want to plot efficiency of real data and simulated data, the value has to be 0.</p> See result scructure <p>If you deleted the right lines, your code now should be like this:</p> <pre><code>//CONFIGS\n\nint useScheme = 0;\n//Jpsi    Sideband Run vs Jpsi    Sideband MC\n//Jpsi    Fitting  Run vs Jpsi    Fitting  MC\n//Jpsi    Sideband Run vs Jpsi    Fitting  Run\n//Upsilon Sideband Run vs Upsilon Sideband MC\n//Upsilon Fitting  Run vs Upsilon Fitting  MC\n//Upsilon Sideband Run vs Upsilon Fitting  Run\n\n//Muon id analyse\nbool doTracker    = true;\nbool doStandalone = false;\nbool doGlobal     = false;\n\n//quantity analyse\nbool doPt  = true;\nbool doEta = true;\nbool doPhi = true;\n</code></pre> <p>Let your variables like this.</p> <p>Now you need to run the code. To do this, save the file and type on your terminal:</p> <pre><code>root -l compare_efficiency.cpp\n</code></pre> <p>If everything went well, the message you'll see in terminal at end of the process is:</p> <pre><code>Use Scheme: 0\nDone. All result files can be found at \"../results/Comparison_Upsilon_Sideband_Run_vs_MC/\"\n</code></pre> <p>Note</p> <p>The command above to run the code will display three new windows on your screen with comparison plots. You can avoid them by running straight the command below.</p> <pre><code>root -l -b -q compare_efficiency.cpp\n</code></pre> <p>In this case, to check it results you are going to need go for result folder (printed on code run) and check images there by yourself. You can try to run <code>new TBrowser</code> again:</p> <pre><code>cd [FOLDER_PATH]\nroot -l\nnew TBrowser\n</code></pre> <p>And as output plots comparison, you get:</p> <p> </p> <p>Now you can type the command below to quit root and close all created windows:</p> <pre><code>.q\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/tutorial/04-comparison/#how-fitting-method-code-stores-its-files","title":"How fitting method code stores its files","text":"<p>To do the next part, first you need to understand how the fitting method code saves. It is not so different than sideband subtraction method. Lets look at how they are saved.</p> <p>If you look inside <code>fitting\\results'</code> folder, where is stored fitting method results, you will see another folder named <code>efficiencies</code>. lets go there by terminal from <code>fitting</code> folder:</p> <pre><code>cd results/efficiencies\nls\n</code></pre> <pre><code>efficiency\n</code></pre> <p>Inside of it there is a unique folder named efficiency. It is necessary because later on, we will work in efficiencies in 2 dimentions. The efficiency folder means it is one-dimensional, which we worked here so on.</p> <pre><code>cd efficiency\nls\n</code></pre> <pre><code>Comparison_Run2011_vs_MC  Jpsi_MC_2020  Jpsi_Run_2011\n</code></pre> <p>Let's go inside one of them:</p> <pre><code>cd Jpsi_Run_2011\nls\n</code></pre> <pre><code>Pt_trackerMuon.root\n</code></pre> <p>For every configuration for a specific dataset, they will create <code>.root</code> files inside its respectively folder. For example, this one folder that we choose will have all calculations for the J/\u03c8 real data dataset.</p> <p>If you go with your terminal to this folder and run this command, you'll see that the result files only have one plot on main folder.</p> <pre><code>root -l Pt_trackerMuon.root\n</code></pre> <pre><code>root [0]\nAttaching file Pt_trackerMuon.root as _file0...\n(TFile *) 0x55efb5f44930\nroot [1]\n</code></pre> <p>Now lets look at its content. Type on terminal:</p> <pre><code>new TBrowser\n</code></pre> <p>It has only one plot, because the others are in different files. Inside the folder <code>histograms</code>, you can find the histograms that created this efficiency result.</p> <p></p> <p>Key Point</p> <ul> <li>There is a <code>.root</code> file for each efficiency plot created with the fitting method.</li> </ul>"},{"location":"analysis/selection/idefficiencystudy/tutorial/04-comparison/#comparison-results-between-real-data-and-simulations-for-fitting-method","title":"Comparison results between real data and simulations for fitting method","text":"<p>Go back to the <code>main</code> folder on <code>sideband_subtraction</code> folder.</p> <pre><code>cd cd TagAndProbe/efficiency_tools/fitting/\n</code></pre> <p>Open compare_efficiency.cpp</p> <p>You will need to choose which particle you will want to use</p> <pre><code>//choose the particle\n\nstring particle =\"Z\";\n//string particle =\"Jpsi\";\n</code></pre> <p>You have to edit which quantity and MuonId you want to use:</p> <pre><code>//string MuonId   = \"trackerMuon\";\n//string MuonId   = \"standaloneMuon\";\nstring MuonId   = \"globalMuon\";\n\nstring quantity = \"Pt\";\n//string quantity = \"Eta\";\n//string quantity = \"Phi\";\n</code></pre> <p>Doing this and running the program with:</p> <pre><code>root compare_efficiency.cpp\n</code></pre> <p>Should get you these results:</p> <p> </p> <p>Challenge</p> <p>As you notice here we presented comparison for \u03b7 and \u03c6. Try to go back to fitting method tutorial and redo commands to get efficiencies for these quantities in order to compare with sideband subctraction method. Do not forget to turn <code>bool doEta</code> and <code>bool doPhi</code> true.</p> <p>Now you can type the command below to quit root and close all created windows:</p> <pre><code>.q\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/tutorial/04-comparison/#comparison-results-between-data-from-the-sideband-and-data-from-the-fitting-method","title":"Comparison results between data from the sideband and data from the fitting method","text":"<p>Challenge</p> <p>Using what you did before, try to mix them and plot a comparison between real data for sideband method and real data for sthe fitting method and get an analysis. Notice that:</p> <ul> <li>Real data = Run 2011</li> <li>Simulations = Monte Carlo = MC</li> </ul> <p>Tip: you just need to change what you saw in this page to do this comparison.</p> <p>Extra challenge</p> <p>As you did with the last 2 extras challenges, try to redo this exercise comparing results between \u03a5 datas.</p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/05-systematic/","title":"Procedures and strategy for estimating the systematics uncertainties","text":""},{"location":"analysis/selection/idefficiencystudy/tutorial/05-systematic/#setting-it-up","title":"Setting it up","text":"<p>First, clone this repository.</p> <pre><code>git clone --branch systematic https://github.com/RodrigoCampelloS/TagAndProbe.git\ncd TagAndProbe/efficiency_tools/fitting\n</code></pre> <p>You will also need to download the TagAndProbe_Z_Run2012.root file using this link.</p> <pre><code>https://cernbox.cern.ch/files/link/public/lqHEasYWJpOZsfq/simplified_datasets_for_fitting_method?items-per-page=100&amp;view-mode=resource-table&amp;tiles-size=1\n</code></pre> <p>For MC data you need to download the TagAndProbe_Z_MC.root using the same link. You can also download the TagAndProbe_Jpsi_Run2011.root and TagAndProbe_Jpsi_MC.root files if you wish to use the Jpsi particle instead.</p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/05-systematic/#estimations-of-systematics-uncertainty-sources","title":"Estimations of systematics uncertainty sources","text":"<p>To estimate the systematic error we will need first to get some uncertainties from the DATA. So, to do that, run the following code.</p> <pre><code>root -l -b -q plot_sys_efficiency_Z.cpp\n</code></pre> <p>By default, this code will estimate the Muon ID efficiency for the Global Muon ID for |\u03b7| distribution, this can be changed by opening the \"plot_sys_efficiency_Z.cpp\" and commenting and uncommenting the Muon ID and quantity of your desire. This process may take several minutes to complete.</p> <p>The systematics uncertainties will be evaluated by making small changes in the fit on the invariant mass distribution of the resonance. For example, the Z decaying in dimuons, in this case, the changes were: 2xVoigtian (\"2x Voigtian\" as in the code) which means fitting with two voigtians. The other sources are the upper and under limits of invariant mass distribution and so \"Mass Up\" means making the mass window bigger, and \"Mass Down\" means making the mass window smaller. Last source you can modify the bin size of the same distribution. \"Bin up\" means making the fit with more bins and \"Bin down\" means making the fit with fewer bins.</p> <p>In order to do the next step you will have to run the \"plot_sys_efficiency_Z.cpp\" for the Pt of both global and tracker Muon. To get the Pt for the tracker Muons the code should look like this.</p> <pre><code>//Which Muon Id do you want to study?\nstring MuonId   = \"trackerMuon\";\n//string MuonId   = \"standaloneMuon\";\n//string MuonId   = \"globalMuon\";\n\n//Which quantity do you want to use?\n\n//for Z\nstring quantity = \"Pt\";     double bins[] = {15.0,20.0,25.0,30.0,40.0,120.0};\n//string quantity = \"Eta\";    double bins[] = {-2.4, -1.8, -1.4, -1.2, -1.0, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1.0, 1.2, 1.4, 1.8, 2.4};\n//string quantity = \"Phi\";    double bins[] = {-3.0, -1.8, -1.6, -1.2, -1.0, -0.7, -0.4, -0.2, 0, 0.2, 0.4, 0.7, 1.0, 1.2, 1.6, 1.8, 3.0};\n</code></pre> <p>and like this to the global Muons.</p> <pre><code>//Which Muon Id do you want to study?\n//string MuonId   = \"trackerMuon\";\n//string MuonId   = \"standaloneMuon\";\nstring MuonId   = \"globalMuon\";\n\n//Which quantity do you want to use?\n\n//for Z\nstring quantity = \"Pt\";     double bins[] = {15.0,20.0,25.0,30.0,40.0,120.0};\n//string quantity = \"Eta\";    double bins[] = {-2.4, -1.8, -1.4, -1.2, -1.0, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1.0, 1.2, 1.4, 1.8, 2.4};\n//string quantity = \"Phi\";    double bins[] = {-3.0, -1.8, -1.6, -1.2, -1.0, -0.7, -0.4, -0.2, 0, 0.2, 0.4, 0.7, 1.0, 1.2, 1.6, 1.8, 3.0};\n</code></pre> <p>If you want to use the MC data, you need to change which fit you will use at the beginning of the code. If this is the case, your code should look like this</p> <pre><code>//Change if you need\n\n//#include \"src/dofits/DoFit_Jpsi_Run.h\"\n//#include \"src/dofits/DoFit_Jpsi_Run_2xGaus.h\"\n\n//#include \"src/dofits/DoFit_Z_Run_2xVoigtian.h\"\n//#include \"src/dofits/DoFit_Z.h\"\n\n#include \"src/dofits/DoFit_Z_MC.h\"\n#include \"src/dofits/DoFit_Z_MC_2xVoigtian.h\"\n</code></pre> <p>For Jpsi use</p> <pre><code>root -l -b -q plot_sys_efficiency_Jpsi.cpp\n</code></pre> <p>and make the same adjustments but make sure you are changing the fits you use and the MuonId of the correct function.</p>"},{"location":"analysis/selection/idefficiencystudy/tutorial/05-systematic/#systematic-efficiency-overplot","title":"Systematic efficiency overplot","text":"<p>To better understand the results of the last part, this code will put all the different plots created previously in an image.</p> <pre><code>root overplot_efficiencies.cpp\n</code></pre> <p>You should get a result like this:</p> <p></p> <p>If you're using the Jpsi particle you need to uncomment line 4 and comment line 3. If that's the case, your code should look like this.</p> <pre><code>#include \"src/create_folder.h\"\n//choose the particle\n//string particle =\"Z\";\nstring particle =\"Jpsi\";\nTEfficiency* read_TEfficiency(const char* folder_path, const char* file_name, const char* TEfficiency_path)\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/tutorial/05-systematic/#2d-efficiency-map","title":"2D Efficiency Map","text":"<p>This code generates a 2D systematic efficiency overplot, it outputs a .root that contains the efficiency histograms that can be visualised by the root TBrowser.</p> <pre><code>root -l -b -q plot_sys_efficiency_2d.cpp\n</code></pre> <p>This is one of the graphs that will be generated.</p> <p></p> <p>It is noteworthy that the uncertainties presented above in the 2d map are already the quadrature sum of systematics and statistical uncertainties.</p> <p>If you're using the Jpsi particle you need to choose which fit function you will use and the particle. Your code should look like this.</p> <pre><code>//Change if you need\n#include \"src/dofits/DoFit_Jpsi_Run.h\"\n#include \"src/dofits/DoFit_Jpsi_Run_2xGaus.h\"\n#include \"src/dofits/DoFit_Z_Run_2xVoigtian.h\"\n//#include \"src/dofits/DoFit_Z.h\"\n//#include \"src/dofits/DoFit_Z_MC.h\"\n//#include \"src/dofits/DoFit_Z_MC_2xVoigtian.h\"\n//#include \"src/dofits/DoFit_Jpsi_MC.h\"\n//#include \"src/dofits/DoFit_Jpsi_MC_2xGaus.h\"\n#include \"src/create_folder.h\"\n#include \"src/create_TH2D.h\"\n#include \"src/get_efficiency_TH2D.h\"\n#include \"src/get_efficiency_TH2DJpsi.h\"\n#include \"src/yields_n_errs_to_TH2Ds_bin.h\"\n\n//particle\n//string particle = \"Z\";\nstring particle = \"Jpsi\";\n</code></pre>"},{"location":"analysis/selection/idefficiencystudy/tutorial/05-systematic/#scale-factor","title":"Scale Factor","text":"<p>For this one, you will have to run the same commands as before but with MC (Monte Carlo) data, follow the same steps as before but with the new file as your source. This 2d map consists of dividing the real data efficiency value by the MC efficiency value.</p> <pre><code>root -l -b -q scale_factor_Z.cpp\n</code></pre> <p>This is one of the graphs that will be generated.</p> <p></p> <p>If you are using the Jpsi particle use this instead</p> <pre><code>root -l -b -q scale_factor_Jpsi.cpp\n</code></pre>"},{"location":"analysis/selection/objects/electrons/","title":"Electrons","text":""},{"location":"analysis/selection/objects/electrons/#introduction","title":"Introduction","text":"<p>Electrons are measured in the CMS experiment combining the information from the inner tracker and the electromagnetic calorimeter as summarized on an introductory page on Finding electrons and photons. The signals from these systems are processed with CMSSW through subsequent steps to form electron candidates which are then available in the electron collection of the data files.</p>"},{"location":"analysis/selection/objects/electrons/#electron-4-vector-and-track-information","title":"Electron 4-vector and track information","text":"Run 1 DataRun 2 Data <p>An example of an EDAnalyzer accessing electron information is available in the ElectronAnalyzer of the Physics Object Extractor Tool (POET). The following header files needed for accessing electron information are included:</p> <pre><code>//classes to extract electron information\n#include \"DataFormats/EgammaCandidates/interface/GsfElectron.h\"\n#include \"DataFormats/EgammaCandidates/interface/GsfElectronFwd.h\"\n#include \"DataFormats/GsfTrackReco/interface/GsfTrack.h\"\n#include \"RecoEgamma/EgammaTools/interface/ConversionTools.h\"\n#include \"DataFormats/VertexReco/interface/Vertex.h\"\n#include \"DataFormats/VertexReco/interface/VertexFwd.h\"\n</code></pre> <p>In ElectronAnalyzer.cc, the electron four-vector elements are accessed as shown below.</p> <pre><code>Handle&lt;reco::GsfElectronCollection&gt; myelectrons;\niEvent.getByLabel(electronInput, myelectrons);\n\n[...]\n\nfor (reco::GsfElectronCollection::const_iterator itElec=myelectrons-&gt;begin(); itElec!=myelectrons-&gt;end(); ++itElec){\n\n[...]\n\nelectron_e.push_back(itElec-&gt;energy());\nelectron_pt.push_back(itElec-&gt;pt());\nelectron_px.push_back(itElec-&gt;px());\nelectron_py.push_back(itElec-&gt;py());\nelectron_pz.push_back(itElec-&gt;pz());\nelectron_eta.push_back(itElec-&gt;eta());\nelectron_phi.push_back(itElec-&gt;phi());\n\n[...]\n}\n</code></pre> <p>Most charged physics objects are also connected to tracks from the CMS tracking detectors. The charge of the object can be queried directly:</p> <pre><code>  electron_ch.push_back(itElec-&gt;charge());\n</code></pre> <p>Information from tracks provides other kinematic quantities. Often, the most pertinent information about an object to access from its associated track is its impact parameter with respect to the primary interaction vertex. The access to the vertex collection is gained through the <code>getByLabel</code> method and the first elemement of the vertex collection gives the best estimate of the interaction point (\"primary vertex\" - <code>pv</code>):</p> <pre><code>  Handle&lt;reco::VertexCollection&gt; vertices;\niEvent.getByLabel(InputTag(\"offlinePrimaryVertices\"), vertices);\nmath::XYZPoint pv(vertices-&gt;begin()-&gt;position());\n</code></pre> <p>The access to the track is provided through</p> <pre><code>  auto trk = itElec-&gt;gsfTrack();\n</code></pre> <p>for each electron in the electron loop, and the impact parameter information is obtained with</p> <pre><code>  electron_dxy.push_back(trk-&gt;dxy(pv));\nelectron_dz.push_back(trk-&gt;dz(pv));\nelectron_dxyError.push_back(trk-&gt;d0Error());\nelectron_dzError.push_back(trk-&gt;dzError());\n</code></pre> <p>An example of an EDAnalyzer accessing electron information is available in the ElectronAnalyzer of the Physics Object Extractor Tool (POET). The following header files needed for accessing electron information are included:</p> <pre><code>//classes to extract electron information\n#include \"DataFormats/PatCandidates/interface/Electron.h\"\n#include \"DataFormats/VertexReco/interface/VertexFwd.h\"\n#include \"DataFormats/VertexReco/interface/Vertex.h\"\n</code></pre> <p>In ElectronAnalyzer.cc, the electron four-vector elements are accessed from the <code>pat::Electron</code> collection as shown below.</p> <pre><code>Handle&lt;pat::ElectronCollection&gt; electrons;\niEvent.getByToken(electronToken_, electrons);\n\n[...]\n\nfor (const pat::Electron &amp;el : *electrons)\n{\nelectron_e.push_back(el.energy());\nelectron_pt.push_back(el.pt());\nelectron_px.push_back(el.px());\nelectron_py.push_back(el.py());\nelectron_pz.push_back(el.pz());\nelectron_eta.push_back(el.eta());\nelectron_phi.push_back(el.phi());\n\n[...]\n}\n</code></pre> <p>with <code>electronToken_</code> defined as a member of the <code>ElectronAnalyzer</code> class and its value read from the configuration file.</p> <p>Most charged physics objects are also connected to tracks from the CMS tracking detectors. The charge of the object can be queried directly:</p> <pre><code>  electron_ch.push_back(el.charge());\n</code></pre> <p>Information from tracks provides other kinematic quantities. Often, the most pertinent information about an object to access from its associated track is its impact parameter with respect to the primary interaction vertex. The access to the vertex collection is gained through the <code>getByToken</code> method and the first elemement of the vertex collection gives the best estimate of the interaction point (\"primary vertex\" - <code>pv</code>):</p> <pre><code>  Handle&lt;reco::VertexCollection&gt; vertices;\niEvent.getByToken(vtxToken_, vertices);\nmath::XYZPoint pv(vertices-&gt;begin()-&gt;position());\n</code></pre> <p>again with <code>vtxToken_</code> defined as a member of the <code>ElectronAnalyzer</code> class and its value read from the configuration file.</p> <p>The access to the track is provided through member function <code>gsfTrack</code>, and for each electron in the electron loop, and the impact parameter information is obtained with</p> <pre><code>  electron_dxy.push_back(el.gsfTrack()-&gt;dxy(pv));\nelectron_dz.push_back(el.gsfTrack()-&gt;dz(pv));\nelectron_dxyError.push_back(el.gsfTrack()-&gt;d0Error());\nelectron_dzError.push_back(el.gsfTrack()-&gt;dzError());\n</code></pre>"},{"location":"analysis/selection/objects/electrons/#electron-identification","title":"Electron identification","text":"<p>As explained in the Physics Object page, a mandatory task in the physics analysis is to identify electrons, i.e. to separate \u201creal\u201d objects from \u201cfakes\u201d. The criteria depend on the type of analysis.</p> <p>The selection is based on cuts on a small number of variables. Different thresholds are used for electrons found in the ECAL barrel and the ECAL endcap. Selection variables may be categorized in three groups:</p> <ul> <li>electron ID variables (shower shape, track cluster matching etc)</li> <li>isolation variables</li> <li>conversion rejection variables.</li> </ul> Run 1 DataRun 2 Data <p>The standard identification and isolation algorithm results can be accessed from the electron object class and the recommended working points are documented in the the public data page for electron for 2010 and 2011. The values implemented in the example code ElectronAnalyzer.cc are those recommended for 2012.</p> <p>Three levels of identification criteria are defined</p> <pre><code>bool isLoose = false, isMedium = false, isTight = false;\n</code></pre> <p>For electrons in the electromagnetic calorimeter barrel area, they are determined as follows:</p> <pre><code>if ( abs(itElec-&gt;eta()) &lt;= 1.479 ) {   if ( abs(itElec-&gt;deltaEtaSuperClusterTrackAtVtx())&lt;.007 &amp;&amp; abs(itElec-&gt;deltaPhiSuperClusterTrackAtVtx())&lt;.15 &amp;&amp; itElec-&gt;sigmaIetaIeta()&lt;.01 &amp;&amp; itElec-&gt;hadronicOverEm()&lt;.12 &amp;&amp; abs(trk-&gt;dxy(pv))&lt;.02 &amp;&amp; abs(trk-&gt;dz(pv))&lt;.2 &amp;&amp; missing_hits&lt;=1 &amp;&amp; passelectronveto==true &amp;&amp;\nabs(1/itElec-&gt;ecalEnergy()-1/(itElec-&gt;ecalEnergy()/itElec-&gt;eSuperClusterOverP()))&lt;.05 &amp;&amp; el_pfIso&lt;.15){\n\nisLoose = true;\n\nif ( abs(itElec-&gt;deltaEtaSuperClusterTrackAtVtx())&lt;.004 &amp;&amp; abs(itElec-&gt;deltaPhiSuperClusterTrackAtVtx())&lt;.06 &amp;&amp; abs(trk-&gt;dz(pv))&lt;.1 ){\nisMedium = true;\n\nif (abs(itElec-&gt;deltaPhiSuperClusterTrackAtVtx())&lt;.03 &amp;&amp; missing_hits&lt;=0 &amp;&amp; el_pfIso&lt;.10 ){\nisTight = true;\n}\n}\n}\n}\n</code></pre> <p>where</p> <ul> <li><code>deltaEta...</code> and <code>deltaPhi...</code> indicate how the electron's trajectory varies between the track and the ECAL cluster, with smaller variations preferred for the \"tightest\" quality levels.</li> <li><code>sigmaIetaIeta</code> describes the variance of the ECAL cluster in psuedorapidity (\"ieta\" is an integer index for this angle).</li> <li><code>hadronicOverEm</code> describes the ratio of HCAL to ECAL energy deposits, which should be small for good quality electrons.</li> <li>The impact parameters <code>dxy</code> and <code>dz</code> should also be small for good quality electrons produced in the initial collision.</li> <li>Missing hits are gaps in the trajectory through the inner tracker (shouldn't be any!)</li> <li>The conversion veto is an algorithm that rejects electrons coming from photon conversions in the tracker, which should instead be reconstructed as part of the photon.</li> <li>The criterion using <code>ecalEnergy</code> and <code>eSuperClusterOverP</code> compares the differences between the electron's energy and momentum measurements, which should be very similar to each other for good electrons.</li> <li><code>el_pfIso</code> represents how much energy, relative to the electron's, within a cone around the electron comes from other particle-flow candidates. If this value is small the electron is likely \"isolated\" in the local region.</li> </ul> <p>The isolation variable <code>el_pfIso</code>, based on a cone size of 0.3 around the electron, is defined with</p> <pre><code>if (itElec-&gt;passingPflowPreselection()) {\ndouble rho = 0;\nif(rhoHandle.isValid()) rho = *(rhoHandle.product());\ndouble Aeff = effectiveArea0p3cone(itElec-&gt;eta());\nauto iso03 = itElec-&gt;pfIsolationVariables();\nel_pfIso = (iso03.chargedHadronIso + std::max(0.0,iso03.neutralHadronIso + iso03.photonIso - rho*Aeff))/itElec-&gt;pt();\n} </code></pre> <p>In the endcap part of the electromagnetic calorimeter, the procedure is similar with different values.</p> <p>To do</p> <ul> <li>The isolation snippet needs more explanation</li> <li>Check if the PR fixing the problem with missing hits affects the identification code snippet.</li> </ul> <p>The standard identification and isolation algorithm results can be accessed from the pat electron object class. Three levels of identification criteria are defined: loose, medium, and tight. An example selection is implemented in ElectronAnalyzer.cc:</p> <pre><code>  electron_isLoose.push_back(el.electronID(\"cutBasedElectronID-PHYS14-PU20bx25-V2-standalone-loose\"));\nelectron_isMedium.push_back(el.electronID(\"cutBasedElectronID-PHYS14-PU20bx25-V2-standalone-medium\"));\nelectron_isTight.push_back(el.electronID(\"cutBasedElectronID-PHYS14-PU20bx25-V2-standalone-tight\"));\n</code></pre> <p>Warning</p> <p>The choice of recommended electron ID criteria for 2015 data needs to be verified. In addition to <code>PHYS14_PU20bx25_V2</code> other sets, for example <code>Spring15_25ns_V1</code>, are available.</p> <p>Isolation computed from PF Clusters, is available through the methods <code>ecalPFClusterIso</code> and <code>hcalPFClusterIso</code>:</p> <pre><code>  electron_iso.push_back(el.ecalPFClusterIso());\n</code></pre> <p>To do</p> <ul> <li>More information is needed for particle flow (PF) clusters</li> <li>More information is needed for the values returned by <code>ecalPFClusterIso</code> and if the description in the class header still holds.</li> </ul>"},{"location":"analysis/selection/objects/jets/","title":"Jets","text":"<p>To do</p> <p>Only basic 4-vector access information has been added for Run 2 data</p>"},{"location":"analysis/selection/objects/jets/#what-are-jets","title":"What are jets?","text":"<p>Jets are spatially-grouped collections of long-lived particles that are produced when a quark or gluon hadronizes. The kinematic properties of jets resemble that of the initial partons that produced them. In the CMS language, jets are made up of many particles, with the following predictable energy composition:</p> <ul> <li>~65% charged hadrons</li> <li>~25% photons (from neutral pions)</li> <li>~10% neutral hadrons</li> </ul> <p>Jets are very messy! Hadronization and the subsequent decays of unstable hadrons can produce 100s of particles near each other in the CMS detector. Hence these particles are rarely analyzed individually. How can we determine which particle candidates should be included in each jet?</p>"},{"location":"analysis/selection/objects/jets/#clustering","title":"Clustering","text":"<p>Jets can be clustered using a variety of different inputs from the CMS detector. \"CaloJets\" use only calorimeter energy deposits. \"GenJets\" use generated particles from a simulation. But by far the most common are \"PFJets\", from particle flow candidates.</p> <p>The result of the CMS Particle Flow algorithm is a list of particle candidates that account for all inner-tracker and muon tracks and all above-threshold energy deposits in the calorimeters. These particles are formed into jets using a \"clustering algorithm\". The most common algorithm used by CMS is the \"anti-kt\" algorithm, which is abbreviated \"AK\". It iterates over particle pairs and finds the two (i and j) that are the closest in some distance measure and determines whether to combine them:</p> <p></p> <p></p> <p>The momentum power (-2) used by the anti-kt algorithm means that higher-momentum particles are clustered first. This leads to jets with a round shape that tend to be centered on the hardest particle. In CMS software this clustering is implemented using the fastjet package.</p> <p></p>"},{"location":"analysis/selection/objects/jets/#pileup","title":"Pileup","text":"<p>Inevitably, the list of particle flow candidates contains particles that did not originate from the primary interaction point. CMS experiences multiple simultaneous collisions, called \"pileup\", during each \"bunch crossing\" of the LHC, so particles from multiple collisions coexist in the detector. There are various methods to remove their contributions from jets:</p> <ul> <li>Charged hadron subtraction CHS: all charged hadron candidates are associated with a track. If the track is not associated with the primary vertex, that charged hadron can be removed from the list. CHS is limited to the region of the detector covered by the inner tracker. The pileup contribution to neutral hadrons has to be removed mathematically which will be discussed later.</li> <li>PileUp Per Particle Identification (PUPPI, available in Run 2): CHS is applied, and then all remaining particles are weighted based on their likelihood of arising from pileup. This method is more stable and performant in high pileup scenarios such as the upcoming HL-LHC era.</li> </ul>"},{"location":"analysis/selection/objects/jets/#accessing-jets-in-cms-software","title":"Accessing Jets in CMS Software","text":"Run 1 DataRun 2 Data <p>Two examples of EDAnalyzers accessing jet information are available in the Physics Object Extractor Tool (POET):</p> <ul> <li>JetAnalyzer accessing jets from the <code>PFJetCollection</code></li> <li>PatJetAnalyzer accessing jets from <code>std::vector&lt;pat::Jet&gt;</code> collection using \"Physics Analysis Toolkit\" (PAT) format in which jets are easier to work with.</li> </ul> <p>The following header files needed for accessing jet information are included:</p> <pre><code>//classes to extract PFJet information\n#include \"DataFormats/JetReco/interface/PFJet.h\"\n#include \"DataFormats/JetReco/interface/PFJetCollection.h\"\n#include \"DataFormats/BTauReco/interface/JetTag.h\"\n#include \"CondFormats/JetMETObjects/interface/JetCorrectionUncertainty.h\"\n#include \"CondFormats/JetMETObjects/interface/FactorizedJetCorrector.h\"\n#include \"CondFormats/JetMETObjects/interface/JetCorrectorParameters.h\"\n#include \"CondFormats/JetMETObjects/interface/SimpleJetCorrector.h\"\n#include \"CondFormats/JetMETObjects/interface/SimpleJetCorrectionUncertainty.h\"\n#include \"DataFormats/VertexReco/interface/Vertex.h\"\n#include \"DataFormats/VertexReco/interface/VertexFwd.h\"\n\n#include \"DataFormats/JetReco/interface/Jet.h\"\n#include \"SimDataFormats/JetMatching/interface/JetFlavourInfo.h\"\n#include \"SimDataFormats/JetMatching/interface/JetFlavourInfoMatching.h\"\n</code></pre> <p>Jets software classes have the same basic 4-vector methods as the objects discussed in the Common Tools page. In JetAnalyzer.cc, the jet four-vector elements are accessed (with <code>jetInput</code> passed as <code>\"ak5PFJets\"</code> in the configuration file) :</p> <pre><code>Handle&lt;reco::PFJetCollection&gt; myjets;\niEvent.getByLabel(jetInput, myjets);\n\n[...]\n\nfor (reco::PFJetCollection::const_iterator itjet=myjets-&gt;begin(); itjet!=myjets-&gt;end(); ++itjet){\n\n[...]\n\njet_e.push_back(itjet-&gt;energy());\njet_pt.push_back(itjet-&gt;pt());\njet_px.push_back(itjet-&gt;px());\njet_py.push_back(itjet-&gt;py());\njet_pz.push_back(itjet-&gt;pz());\njet_eta.push_back(itjet-&gt;eta());\njet_phi.push_back(itjet-&gt;phi());\n\n[...]\n\n}\n</code></pre> <p>An example of an EDAnalyzer accessing jet information is available in the JetAnalyzer of the Physics Object Extractor Tool (POET). The following header file needed for accessing MET information is included:</p> <pre><code>//class to extract jet information\n#include \"DataFormats/PatCandidates/interface/Jet.h\"\n</code></pre> <p>Jets software classes have the same basic 4-vector methods as the objects discussed in the Common Tools page. In JetAnalyzer.cc, the jet four-vector elements are accessed from the <code>pat::Jet</code> collection as shown below</p> <pre><code>Handle&lt;pat::JetCollection&gt; jets;\niEvent.getByToken(jetToken_, jets);\n\n[...]\n\nfor (const pat::Jet &amp;jet : *jets)\n{\njet_e.push_back(jet.energy());\njet_pt.push_back(jet.pt());\njet_px.push_back(jet.px());\njet_py.push_back(jet.py());\njet_pz.push_back(jet.pz());\njet_eta.push_back(jet.eta());\njet_phi.push_back(jet.phi());\n\n[...]\n\n}\n</code></pre> <p><code>jetToken_</code> is defined as a member of the <code>MetAnalyzer</code> class and its value read from the configuration file.</p>"},{"location":"analysis/selection/objects/jets/#jet-id","title":"Jet ID","text":"Run 1 DataRun 2 Data <p>Particle-flow jets are not immune to noise in the detector, and jets used in analyses should be filtered to remove noise jets. CMS has defined a Jet ID with criteria for good jets:</p> <p>The PFlow jets are required to have charged hadron fraction CHF &gt; 0.0 if within tracking fiducial region of |eta| &lt; 2.4, neutral hadron fraction NHF &lt; 1.0, charged electromagnetic (electron) fraction CEF &lt; 1.0, and neutral electromagnetic (photon) fraction NEF &lt; 1.0. These requirements remove fake jets arising from spurious energy depositions in a single sub-detector.</p> <p>These criteria demonstrate how particle-flow jets combine information across subdetectors. Jets will typically have energy from electrons and photons, but those fractions of the total energy should be less than one. Similarly, jets should have some energy from charged hadrons if they overlap the inner tracker, and all the energy should not come from neutral hadrons. A mixture of energy sources is expected for genuine jets. All of these energy fractions (and more) can be accessed from the jet objects.</p> <p>You can use the PFJet class definition of the CMSSW DataFormats package to see what methods are available for PFJets. It is rendered for maybe easier readability in the CMSSW software documentation. We can implement a jet ID to reject jets that do not pass so that these jets are not stored in any of the tree branches. This code show an implementation of Jet ID cuts while also applying a minimum momentum threshold.</p> <pre><code>for (reco::PFJetCollection::const_iterator itjet=jets-&gt;begin(); itjet!=jets-&gt;end(); ++itjet){\nif (itjet-&gt;pt &gt; jet_min_pt &amp;&amp; itjet-&gt;chargedHadronEnergyFraction() &gt; 0 &amp;&amp; itjet-&gt;neutralHadronEnergyFraction() &lt; 1.0 &amp;&amp;\nitjet-&gt;electronEnergyFraction() &lt; 1.0 &amp;&amp; itjet-&gt;photonEnergyFraction() &lt; 1.0){\n\n// jet calculations\n</code></pre> <p>Note</p> <ul> <li>The code snippet above is not part of the example analyzer.</li> </ul> <p>Work in progress</p>"},{"location":"analysis/selection/objects/jets/#jet-corrections","title":"Jet corrections","text":"Run 1 DataRun 2 Data <p>To do</p> <p>Description of jet corrections needs to be added, see the workshop tutorial</p> <p>Work in progress</p>"},{"location":"analysis/selection/objects/jets/#b-tagging-algorithms","title":"B Tagging Algorithms","text":"<p>Jet reconstruction and identification is an important part of the analyses at the LHC. A jet may contain the hadronization products of any quark or gluon, or possibly the decay products of more massive particles such as W or Higgs bosons. Several b tagging\u201d algorithms exist to identify jets from the hadronization of b quarks, which have unique properties that distinguish them from light quark or gluon jets.</p> <p>Tagging algorithms first connect the jets with good quality tracks that are either associated with one of the jet\u2019s particle flow candidates or within a nearby cone. Both tracks and \u201csecondary vertices\u201d (track vertices from the decays of b hadrons) can be used in track-based, vertex-based, or \u201ccombined\u201d tagging algorithms. The specific details depend upon the algorithm use. However, they all exploit properties of b hadrons such as:</p> <ul> <li>long lifetime</li> <li>large mass</li> <li>high track multiplicity</li> <li>large semiloptonic branching fraction</li> <li>hard fragmentation function.</li> </ul> <p>Tagging algorithms are Algorithms that are used for b-tagging:</p> <ul> <li>Track Counting: identifies a b jet if it contains at least N tracks with significantly non-zero impact parameters.</li> <li>Jet Probability: combines information from all selected tracks in the jet and uses probability density functions to assign a probability to each track.</li> <li>Soft Muon and Soft Electron: identifies b jets by searching for a lepton from a semi-leptonic b decay.</li> <li>Simple Secondary Vertex: reconstructs the b decay vertex and calculates a discriminator using related kinematic variables.</li> <li>Combined Secondary Vertex: exploits all known kinematic variables of the jets, information about track impact parameter significance and the secondary vertices to distinguish b jets. This tagger became the default CMS algorithm.</li> </ul> <p>These algorithms produce a single, real number (often the output of an MVA) called a b tagging \u201cdiscriminator\u201d for each jet. The more positive the discriminator value, the more likely it is that this jet contained b hadrons.</p>"},{"location":"analysis/selection/objects/jets/#accessing-tagging-information","title":"Accessing Tagging Information","text":"Run 1 DataRun 2 Data <p>In PatJetAnalyzer.cc we access the information from the Combined Secondary Vertex (CSV) b tagging algorithm and associate discriminator values with the jets. The CSV values are stored in a separate collection in the POET files called a JetTagCollection, which is effectively a vector of associations between jet references and float values (such as a b-tagging discriminator).</p> <pre><code>#include \"DataFormats/BTauReco/interface/JetTag.h\"\n#include \"DataFormats/PatCandidates/interface/Jet.h\"\n\n[...]\n\nHandle&lt;std::vector&lt;pat::Jet&gt;&gt; myjets;\niEvent.getByLabel(jetInput, myjets);\n\n[...]\n\n//define b-tag discriminators handle and get the discriminators\n\nfor (std::vector&lt;pat::Jet&gt;::const_iterator itjet=myjets-&gt;begin(); itjet!=myjets-&gt;end(); ++itjet){\n\n[...]\n// from the btag collection get the float (second) from the association to this jet.\njet_btag.push_back(itjet-&gt;bDiscriminator(\"combinedSecondaryVertexBJetTags\"));\n}\n</code></pre> <p>You can use the command edmDumpEventContent to investiate other b tagging algorithms available as edm::AssociationVector types. This is an example opening the collections for two alternate taggers--the MVA version of CSV and the high purity track counting tagger, which was the most common tagger in 2011:</p> <pre><code>// inside the jet loop\njet_btagheb.push_back(itjet-&gt;bDiscriminator(\"simpleSecondaryVertexHighEffBJetTags\"));\njet_btagtc.push_back(itjet-&gt;bDiscriminator(\"trackCountingHighEffBJetTags\"));\n</code></pre> <p>The distributions in ttbar events (excluding events with values of -9 where the tagger was not evaluated) are shown below. The track counting discriminant is quite different and ranges 0-30 or so.</p> <p></p> <p>Work in progress</p>"},{"location":"analysis/selection/objects/jets/#working-points","title":"Working Points","text":"Run 1 DataRun 2 Data <p>A jet is considered \"b tagged\" if the discriminator value exceeds some threshold. Different thresholds will have different efficiencies for identifying true b quark jets and for mis-tagging light quark jets. As we saw for muons and other objects, a \"loose\" working point will allow the highest mis-tagging rate, while a \"tight\" working point will sacrifice some correct-tag efficiency to reduce mis-tagging. The CSV algorithm has working points defined based on mis-tagging rate:</p> <ul> <li>Loose = ~10% mis-tagging = discriminator &gt; 0.244</li> <li>Medium = ~1% mis-tagging = discriminator &gt; 0.679</li> <li>Tight = ~0.1% mis-tagging = discriminator &gt; 0.898</li> </ul> <p>We can count the number  of \"Medium CSV\" b-tagged jets by summing up the number of jets with discriminant values greater than 0.679. After adding a variable declaration and branch we can sum up the counter:</p> <pre><code>value_jet_nCSVM = 0;\nfor (std::vector&lt;pat::Jet&gt;::const_iterator itjet=myjets-&gt;begin(); itjet!=myjets-&gt;end(); ++itjet){\n// skipping bits\njet_btag.push_back(itjet-&gt;bDiscriminator(\"combinedSecondaryVertexBJetTags\"));\nif (jet_btag.at(value_jet_n) &gt; 0.679) value_jet_nCSVM++;\n}\n</code></pre> <p>We show distributions of the number CSV b jets at the medium working point in Drell-Yan events and top pair events. As expected there are significantly more b jets in the top pair sample.</p> <p></p> <p>Work in progress</p>"},{"location":"analysis/selection/objects/jets/#data-and-simulation-differences","title":"Data and Simulation Differences","text":"<p>When training a tagging algorithm, it is highly probable that the efficiencies for tagging different quark flavors as b jets will vary between simulation and data. These differences must be measured and corrected for using \"scale factors\" constructed from ratios of the efficiencies from different sources.</p> Run 1 DataRun 2 Data <p>The figures below show examples of the b and light quark efficiencies and scale factors as a function of jet momentum read more. Corrections must be applied to make the b-tagging performance match between data and simulation. Read more about these corrections and their uncertainties on this page.</p> <p>When training a tagging algorithm, it is highly probable that the efficiencies for tagging different quark flavors as b jets will vary between simulation and data. These differences must be measured and corrected for using \"scale factors\" constructed from ratios of the efficiencies from different sources. The figures below show examples of the b and light quark efficiencies and scale factors as a function of jet momentum read more.</p> <p>To do</p> <p>\"The figures below\" for both paragraphs are missing. Decide whether this part is valid for Run 2 as well.</p> <p>Work in progress</p>"},{"location":"analysis/selection/objects/met/","title":"MET","text":""},{"location":"analysis/selection/objects/met/#what-is-met","title":"What is MET?","text":"<p>Missing transverse momentum is the negative vector sum of the transverse momenta of all particle flow candidates in an event. The magnitude of the missing transverse momentum vector is called missing transverse energy and referred to with the acronym \u201cMET\u201d. Since energy corrections are made to the particle flow jets, those corrections are propagated to MET by adding back the momentum vectors of the original jets and then subtracting the momentum vectors of the corrected jets. This correction is called \u201cType 1\u201d and is standard for all CMS analyses. The jet energy corrections are discussed more deeply in the Jets page.</p>"},{"location":"analysis/selection/objects/met/#accessing-met-in-cms-software","title":"Accessing MET in CMS Software","text":"Run 1 DataRun 2 Data <p>An example of an EDAnalyzer accessing MET information is available in the MetAnalyzer of the Physics Object Extractor Tool (POET). The following header files needed for accessing MET information are included:</p> <pre><code>//classes to extract PFMET information\n#include \"DataFormats/METReco/interface/PFMET.h\"\n#include \"DataFormats/METReco/interface/PFMETFwd.h\"\n#include \"DataFormats/PatCandidates/interface/MET.h\"\n</code></pre> <p>In MetAnalyzer.cc we open the particle flow MET module (with <code>metInput</code> passed as <code>\"pfMet\"</code> in the configuration file) and extract the magnitude and angle of the MET, the sum of all energy in the detector, and variables related to the \u201csignificance\u201d of the MET. Note that MET quantities have a single value for the entire event, unlike the objects studied previously.</p> <pre><code>Handle&lt;reco::PFMETCollection&gt; mymets;\niEvent.getByLabel(metInput, mymets);\n\n[...]\n\nmet_e = mymets-&gt;begin()-&gt;sumEt();\nmet_pt = mymets-&gt;begin()-&gt;pt();\nmet_px = mymets-&gt;begin()-&gt;px();\nmet_py = mymets-&gt;begin()-&gt;py();\nmet_phi = mymets-&gt;begin()-&gt;phi();\nmet_significance = mymets-&gt;begin()-&gt;significance();\n</code></pre> <p>The MET significance matrix could be accessed with:</p> <pre><code>auto cov = mymets-&gt;begin()-&gt;getSignificanceMatrix();\nvalue_met_covxx = cov[0][0];\nvalue_met_covxy = cov[0][1];\nvalue_met_covyy = cov[1][1];\n</code></pre> <p>MET significance can be a useful tool: it describes the likelihood that the MET arose from noise or mismeasurement in the detector as opposed to a neutrino or similar non-interacting particle. The four-vectors of the other physics objects along with their uncertainties are required to compute the significance of the MET signature. MET that is directed nearly (anti)colinnear with a physics object is likely to arise from mismeasurement and should not have a large significance.</p> <p>The difference between the Drell-Yan events with primarily fake MET and the top pair events with primarily genuine MET can be seen by drawing <code>met_pt</code> or by drawing <code>met_significance</code>. In both distributions the Drell-Yan events have smaller values than the top pair events.</p> <p>An example of an EDAnalyzer accessing MET information is available in the MetAnalyzer of the Physics Object Extractor Tool (POET). The following header file needed for accessing MET information is included:</p> <pre><code>//class to extract MET information\n#include \"DataFormats/PatCandidates/interface/MET.h\"\n</code></pre> <p>In MetAnalyzer.cc we open the particle flow MET module (with <code>metToken_</code> defined as a member of the <code>MetAnalyzer</code> class and its value read from the configuration file) and extract the magnitude and angle of the MET, the sum of all energy in the detector, and variables related to the \u201csignificance\u201d of the MET. Note that MET quantities have a single value for the entire event, unlike the objects studied previously.</p> <pre><code>Handle&lt;pat::METCollection&gt; mets;\niEvent.getByToken(metToken_, mets);\n\n[...]\n\nmet_e = met.sumEt();\nmet_pt = met.pt();\nmet_px = met.px();\nmet_py = met.py();\nmet_phi = met.phi();\nmet_significance = met.significance();\n</code></pre> <p>The MET significance matrix could be accessed with:</p> <pre><code>auto cov = met_significance-&gt;getSignificanceMatrix();\nvalue_met_covxx = cov[0][0];\nvalue_met_covxy = cov[0][1];\nvalue_met_covyy = cov[1][1];\n</code></pre> <p>To do</p> <p>Verify this snippet for matrix, it is taken (and minimally modified) from the Run 1 Met tutorial</p> <p>MET significance can be a useful tool: it describes the likelihood that the MET arose from noise or mismeasurement in the detector as opposed to a neutrino or similar non-interacting particle. The four-vectors of the other physics objects along with their uncertainties are required to compute the significance of the MET signature. MET that is directed nearly (anti)colinnear with a physics object is likely to arise from mismeasurement and should not have a large significance.</p> <p>The difference between the Drell-Yan events with primarily fake MET and the top pair events with primarily genuine MET can be seen by drawing <code>met_pt</code> or by drawing <code>met_significance</code>. In both distributions the Drell-Yan events have smaller values than the top pair events.</p> <p>Warning</p> <p>This page is under construction</p>"},{"location":"analysis/selection/objects/muons/","title":"Muons","text":""},{"location":"analysis/selection/objects/muons/#introduction","title":"Introduction","text":"<p>Muons are measured in the CMS experiment combining the information from the inner tracker and the muon system. The signals from these systems are processed with CMSSW through subsequent steps to form muon candidates which are then available in the muon collection of the data files.</p>"},{"location":"analysis/selection/objects/muons/#access-to-muon-information","title":"Access to muon information","text":"<p>The Physics Objects page shows how to access muon collections, and which header files should be included in the C++ code in order to access all of their class information. The Common Tools page gives instructions to access all the basic kinematic information about any physics object.</p>"},{"location":"analysis/selection/objects/muons/#muon-identification","title":"Muon identification","text":"Run 1 DataRun 2 Data <p>As explained in the Physics Object page, a mandatory task in the physics analysis is to identify muons, i.e. to separate \u201creal\u201d objects from \u201cfakes\u201d. The criteria depend on the type of analysis. The muon object has member functions available which can directly be used to select muon with \"loose\" or \"tight\" selection criteria. These are the corresponding lines in MuonAnalyzer:</p> <pre><code>    muon_tightid.push_back(muon::isTightMuon(*itmuon, *vertices-&gt;begin()));\nmuon_softid.push_back(muon::isSoftMuon(*itmuon, *vertices-&gt;begin()));\n</code></pre> <p>These functions need the interaction vertex as an input (in addition to the muon properties) and this is provided through the first elemement of the vertex collection <code>vertices</code> which gives the best estimate of the interaction point. The vertex collection is accessed with:</p> <pre><code>  Handle&lt;reco::VertexCollection&gt; vertices;\niEvent.getByLabel(InputTag(\"offlinePrimaryVertices\"), vertices);\n</code></pre> <p>In the physics analysis, hard processes that produce large angles between the final state objects are of interest. The final object will be separated from the other objects in the event or be \u201cisolated\u201d. For instance, an isolated muon might be produced in the decay of a W boson. In contrast, a non-isolated muon can come from a weak decay inside a jet.</p> <p>Muon isolation is calculated from a combination of factors: energy from charged hadrons, energy from neutral hadrons, and energy from photons, all in a cone of radius dR &lt; 0.3 or 0.4 around the muon. It is done as shown in this code snippet from MuonAnalyzer:</p> <pre><code>    if (itmuon-&gt;isPFMuon() &amp;&amp; itmuon-&gt;isPFIsolationValid()) {\nauto iso03 = itmuon-&gt;pfIsolationR03();\nmuon_pfreliso03all.push_back((iso03.sumChargedHadronPt + iso03.sumNeutralHadronEt + iso03.sumPhotonEt)/itmuon-&gt;pt());\nauto iso04 = itmuon-&gt;pfIsolationR04();\nmuon_pfreliso04all.push_back((iso04.sumChargedHadronPt + iso04.sumNeutralHadronEt + iso04.sumPhotonEt)/itmuon-&gt;pt());\n}\n</code></pre> <p>More details on the muon identification can be found in the CMS SWGuide MuonID page. The full list of member function can be found in documentation of the muon object class.</p> <p>As explained in the Physics Object page, a mandatory task in the physics analysis is to identify muons, i.e. to separate \u201creal\u201d objects from \u201cfakes\u201d. The criteria depend on the type of analysis. The muon object has member functions available which can directly be used to select muon with \"loose\" or \"tight\" selection criteria. These are the corresponding lines in MuonAnalyzer:</p> <pre><code>  muon_isSoft.push_back(mu.isSoftMuon(PV));\nmuon_isTight.push_back(mu.isTightMuon(PV));\n</code></pre> <p>These functions need the interaction vertex as an input (in addition to the muon properties) and this is provided through the first elemement of the vertex collection <code>vertices</code> which gives the best estimate of the interaction point. The vertex collection is accessed with:</p> <pre><code>  Handle&lt;reco::VertexCollection&gt; vertices;\niEvent.getByToken(vtxToken_, vertices);\nconst reco::Vertex &amp;PV = vertices-&gt;front();\n</code></pre> <p>with <code>vtxToken_</code> defined as a member of the <code>MuonAnalyzer</code> class and its value read from the configuration file in a similar way as for the muon collection.</p> <p>In the physics analysis, hard processes that produce large angles between the final state objects are of interest. The final object will be separated from the other objects in the event or be \u201cisolated\u201d. For instance, an isolated muon might be produced in the decay of a W boson. In contrast, a non-isolated muon can come from a weak decay inside a jet.</p> <p>Muon isolation is calculated from a combination of factors: energy from charged hadrons, energy from neutral hadrons, and energy from photons, all in a cone of radius dR &lt; 0.3 or 0.4 around the muon. It is done as shown in this code snippet from MuonAnalyzer:</p> <pre><code>  auto iso03 = mu.pfIsolationR03();\nmuon_pfreliso03all.push_back((iso03.sumChargedHadronPt + iso03.sumNeutralHadronEt + iso03.sumPhotonEt)/mu.pt());\nauto iso04 = mu.pfIsolationR04();\nmuon_pfreliso04all.push_back((iso04.sumChargedHadronPt + iso04.sumNeutralHadronEt + iso04.sumPhotonEt)/mu.pt());\n</code></pre> <p>More details on the muon identification can be found in the CMS SWGuide MuonID page. The full list of member function can be found in documentation of the PAT muon object class.</p>"},{"location":"analysis/selection/objects/muons/#further-muon-corrections","title":"Further muon corrections","text":"<p>There are misalignments in the CMS detector that make the reconstruction of muon momentum biased. The CMS reconstruction software does not fully correct these misalignments and additional corrections are needed to remove the bias. Correcting the misalignments is important when precision measurements are done using the muon momentum, because the bias in muon momentum will affect the results. However, if the measurement is not sensitive to the exact muon momentum, applying these corrections is not necessary.</p>"},{"location":"analysis/selection/objects/muons/#the-muon-momentum-scale-corrections","title":"The Muon Momentum Scale Corrections","text":"<p>The Muon Momentum Scale Corrections, also known as the Rochester Corrections, are available in the MuonCorrectionsTool. The correction parameters have been extracted in a two step method. In the first step, initial corrections are obtained in bins of the charge of the muon and the \u03b7 and \u03d5 coordinates of the muon track. The reconstruction bias in muon momentum depends on these variables. In the second step, the corrections are fine tuned using the mass of the Z boson.</p> <p>The corrections for data and Monte Carlo (MC) are different since the MC events start with no biases but they can be induced during the reconstruction. Corrections have been extracted for both data and MC events.</p> <p>In the MuonCorrectionsTool, the Run1 Rochester Corrections are added to two datasets as an example: a 2012 dataset and a MC dataset. A plot is created to check that the corrections were applied correctly. Creating the plot requires selections and the produced dataset contains only a part of the initial dataset. These selections can be skipped when the plot is not needed and a corrected version of the whole dataset is wanted as a result. Below you can find instructions on how to run the example code, how to apply the corrections to a different dataset and how to apply the corrections when you don't want to create the plot/make the selections. The official code for the Rochester Corrections can be found in the <code>RochesterCorrections</code> directory. The example code for applying the corrections is in the <code>Test</code> directory.</p> <p>Warning</p> <p>The following example does not need the CMSSW environment but it requires ROOT. This code was written using the ROOT version 6.22.08. If you are using an older version, you might get errors running the code. In this case, try using <code>rochcor2012wasym_old.h</code> instead of <code>rochcor2012wasym.h</code>. You can do this by changing the first line of <code>rochcor2012wasym.cc</code> to <code>#include \"rochcor2012wasym_old.h\"</code>.</p>"},{"location":"analysis/selection/objects/muons/#applying-the-corrections-to-data-and-mc","title":"Applying the corrections to data and MC","text":"<p>In the <code>Test</code> directory you can find <code>Analysis.C</code>, which is the example code for adding the corrections. The main function of <code>Analysis.C</code> is simply used for calling the <code>applyCorrections</code> function which takes as a parameter the name of the ROOT-file (without the .root-part), path to the ROOT-file, the name of the TTree, a boolean value of whether the file contains data (<code>true</code>) or MC (<code>false</code>) and a boolean variable of whether you want to correct the whole dataset (<code>true</code>) or make the selections needed for the plot (<code>false</code>).</p> <pre><code>void Analysis::main()\n{\n// Data\napplyCorrections(\"Run2012BC_DoubleMuParked_Muons\", \"root://eospublic.cern.ch//eos/opendata/cms/derived-data/AOD2NanoAODOutreachTool/Run2012BC_DoubleMuParked_Muons.root\", \"Events\", true, false);\n\n// MC\napplyCorrections(\"ZZTo2e2mu\", \"root://eospublic.cern.ch//eos/opendata/cms/upload/stefan/HiggsToFourLeptonsNanoAODOutreachAnalysis/ZZTo2e2mu.root\", \"Events\", false, false);\n}\n</code></pre> <p>The first thing <code>applyCorrections</code> does is create a TTree from the ROOT-file. Then variables for holding the values read from the tree are created and branch addresses are set so that the variables are populated when looping over events. An output file, new branches for the corrected values and a few variables needed for the corrections are also created.</p> <pre><code>int applyCorrections(string filename, string pathToFile, string treeName, bool isData, bool correctAll) {\n// Create TTree from ROOT file\nTFile *f1 = TFile::Open((pathToFile).c_str());\nTTree *DataTree = (TTree*)f1-&gt;Get(\"Events\");\n\n//Variables to hold values read from the tree\nint maxmuon=1000;\nUInt_t nMuon = 0;\nFloat_t Muon_pt[maxmuon];\nFloat_t Muon_eta[maxmuon];\nFloat_t Muon_phi[maxmuon];\nFloat_t Muon_mass[maxmuon];\nInt_t Muon_charge[maxmuon];\n\n//Set addresses to make the tree populate the variables when reading an entry\nDataTree-&gt;SetBranchAddress(\"nMuon\", &amp;nMuon);\nDataTree-&gt;SetBranchAddress(\"Muon_pt\", &amp;Muon_pt);\nDataTree-&gt;SetBranchAddress(\"Muon_eta\", &amp;Muon_eta);\nDataTree-&gt;SetBranchAddress(\"Muon_phi\", &amp;Muon_phi);\nDataTree-&gt;SetBranchAddress(\"Muon_mass\", &amp;Muon_mass);\nDataTree-&gt;SetBranchAddress(\"Muon_charge\", &amp;Muon_charge);\n</code></pre> <p>Next, the events in the TTree are looped over and the corrections are applied to the muons. The boolean variable <code>correctAll</code> is used here to determine whether to correct all muons in the dataset or to make the selections required for the plot. The invariant mass of \u03bc+\u03bc- is used in the plot, which is why the events are filtered to muon pairs with opposite charges.</p> <pre><code>  // Loop over events\nInt_t nEntries = (Int_t)DataTree-&gt;GetEntries();\n\nfor (Int_t k=0; k&lt;nEntries; k++) {\nDataTree-&gt;GetEntry(k);\n\nif (correctAll) { // Correct all muons in dataset\nif (nMuon &gt; 0) {\n...\n}\n} else { // Correct muons that pass the selections\n// Select events with exactly two muons\nif (nMuon == 2 ) {\n// Select events with two muons of opposite charge\nif (Muon_charge[0] != Muon_charge[1]) {\n...\n}\n}\n}\n</code></pre> <p>Whether all muons or only selected ones are being corrected, it is done in the loop below that loops over all the muons in an event and applies the corrections. The functions for applying the Rochester Corrections take as a parameter a TLorentzVector, which is a four-vector that describes the muons momentum and energy. A TLorentzVector is created for each muon using the muon's pt, eta, phi and mass. As mentioned earlier, the muon momentum scale corrections are different for data and MC and therefore there are separate functions for both: <code>momcor_data</code> and <code>momcor_mc</code>. These functions can be found in <code>rochcor2012wasym.cc</code> if you want to take a closer look at them.</p> <p>The corrected values are stored in the same TLorentzVectors after calling the correction functions. The values are then extracted from the TLorentzVecotrs and saved to the new variables. The new TTree is then filled with the new values.</p> <pre><code>        // Loop over muons in event\nfor (UInt_t i=0; i&lt;nMuon; i++) {\n// Create TLorentzVector\nTLorentzVector mu;\nmu.SetPtEtaPhiM(Muon_pt[i], Muon_eta[i], Muon_phi[i], Muon_mass[i]);\n\n// Apply the corrections\nif (isData) {\nrmcor.momcor_data(mu, Muon_charge[i], runopt, qter);\n} else {\nrmcor.momcor_mc(mu, Muon_charge[i], ntrk, qter);\n}\n\n// Save corrected values\nMuon_pt_cor[i] = mu.Pt();\nMuon_eta_cor[i] = mu.Eta();\nMuon_phi_cor[i] = mu.Phi();\nMuon_mass_cor[i] = mu.M();\n}\n\nDataTreeCor-&gt;Fill();\n</code></pre> <p>When only the selected muons are being corrected, the code does more than just apply the corrections. Both the uncorrected and corrected invariant mass of \u03bc+\u03bc- is computed and saved to a branch. The MuonCorrectionsTool plot is made in bins of eta of \u03bc+ and eta of \u03bc- and new branches are filled for those variables.</p> <pre><code>          // Compute invariant mass of the dimuon system\nDimuon_mass = computeInvariantMass(Muon_pt[0], Muon_pt[1], Muon_eta[0], Muon_eta[1], Muon_phi[0], Muon_phi[1], Muon_mass[0], Muon_mass[1]);\n\n// Choose positive and negative muons' etas\nif (Muon_charge[0] &gt; 0) {\nMuon_eta_pos = Muon_eta[0];\nMuon_eta_neg = Muon_eta[1];\n} else {\nMuon_eta_pos = Muon_eta[1];\nMuon_eta_neg = Muon_eta[0];\n}\n\n// Compute invariant mass of the corrected dimuon system\nDimuon_mass_cor = computeInvariantMass(Muon_pt_cor[0], Muon_pt_cor[1], Muon_eta_cor[0], Muon_eta_cor[1], Muon_phi_cor[0], Muon_phi_cor[1], Muon_mass_cor[0], Muon_mass_cor[1]);\n</code></pre> <p>Finally, the new TTree is written to the output file.</p> <pre><code>  std::cout &lt;&lt; \"Writing tree to ouput file\" &lt;&lt; std::endl;\n\n//Save the new tree\nDataTreeCor-&gt;Write();\n</code></pre>"},{"location":"analysis/selection/objects/muons/#running-the-code","title":"Running the code","text":"<ol> <li>Open ROOT in terminal</li> </ol> <pre><code>root\n</code></pre> <ol> <li>Compile <code>muresolution.cc</code>, <code>rochcor2012wasym.cc</code> and <code>Analysis.C</code></li> </ol> <pre><code>.L RochesterCorrections/muresolution.cc++\n.L RochesterCorrections/rochcor2012wasym.cc++\n.L RochesterCorrections/Test/Analysis.C+\n</code></pre> <ol> <li>Run the main function</li> </ol> <pre><code>Analysis pf\npf.main()\n</code></pre> <ol> <li>To create the plot, compile <code>Plot.C</code> and run the main function</li> </ol> <pre><code>.L RochesterCorrections/Test/Plot.C+\nmain()\n</code></pre>"},{"location":"analysis/selection/objects/muons/#applying-the-corrections-to-a-different-dataset","title":"Applying the corrections to a different dataset","text":"<p>You can use the example code to apply the corrections to different datasets. However, a few changes needs to be made for the code to work correctly. The first thing that needs to be changed is of course the function call in the main function. Call <code>applyCorrections</code> using the parameters that correspond your dataset. Remember that for the first boolean parameter <code>true</code> means your ROOT file contains data and <code>false</code> means it contains MC. For the last parameter, <code>true</code> means you want to correct all muons without making selections and <code>false</code> means you want to make the selections needed for the MuonCorrectionsTool plot.</p> <pre><code>void Analysis::main()\n{\n// Your dataset\napplyCorrections(\"nameOfFile\", \"pathToFile\", \"treeName\", isData, correctAll);\n}\n</code></pre> <p>The second thing you need to do is check the names and data types of the branches in your dataset. For example, instead of the name <code>nMuon</code> you might have <code>numberOfMuons</code> and instead of data type <code>Muon_pt[nMuon]</code> you might have <code>vector&lt;float&gt; Muon_pt</code>. The correct name needs to be changed to the branch address and the data type needs to be corrected. If you have vector, you might need to change <code>Muon_pt[i]</code> to <code>Muon_pt-&gt;at(i)</code> or something similiar later in the code. Example:</p> <pre><code>  //Variables to hold values read from the tree\nint maxmuon=1000;\nUInt_t nMuon = 0;\nvector&lt;float&gt;* Muon_pt;\n\n//Set addresses to make the tree populate the variables when reading an entry\nDataTree-&gt;SetBranchAddress(\"numberOfMuons\", &amp;nMuon);\nDataTree-&gt;SetBranchAddress(\"Muon_pt\", &amp;Muon_pt);\n</code></pre>"},{"location":"analysis/selection/objects/muons/#correcting-the-dataset-without-making-selections","title":"Correcting the dataset without making selections","text":"<p>If you want to correct all muons without making the selections needed for the MuonCorrectionsTool plot, simply give <code>true</code> as the last parameter when calling <code>applyCorrections</code>. The code will then loop through the events, select events with muons and correct the muons.</p>"},{"location":"analysis/selection/objects/objects/","title":"Physics Objects","text":""},{"location":"analysis/selection/objects/objects/#description","title":"Description","text":"<p>The CMS is a giant detector that acts like a camera that \"photographs\" particle collisions, allowing us to interpret their nature.</p> <p>Certainly we cannot directly observe all the particles created in the collisions because some of them decay very quickly or simply do not interact with our detector.  However, we can infer their presence.  If they decay to other stable particles and interact with the apparatus, they leave signals in the CMS subdetectors. These signals are used to reconstruct the decay products or infer their presence; we call these physics objects. These objects could be electrons, muons, jets, missing energy, etc., but also lower level objects like tracks.  For the current releases of open data, we store them in ROOT files following the EDM data model in AOD format.</p> <p>In the CERN Open Portal site one can find a description of these physical objects and a list of them corresponding to 2010 and 2011/2012 releases of open data. For Run 1 data, this guide has the most uptodate content with the corresponding code snippets. For Run 2 data from 2015, a detailed listing is available in the CMS WorkBook.</p> <p>The code examples to which this guide mainly refers to are:</p> Run 1 DataRun 2 Data <ul> <li>Physics Objects Extractor Tool (POET): shows how to extract physics (objects) information and gives examples of methods or tools needed for processing them. For the sake of clarity, EDAnalyzer modules are provided separately for each object.</li> <li>AOD2NanoAODOutreachTool: reads events from CMS AOD files and convert them to a reduced data format. This example provides a single EdAnalyzer module handling all types of physics objects.</li> </ul> <ul> <li>Physics Objects Extractor Tool (POET): shows how to extract physics (objects) information and gives examples of methods or tools needed for processing them. For the sake of clarity, EDAnalyzer modules are provided separately for each object.</li> </ul>"},{"location":"analysis/selection/objects/objects/#dataformats","title":"DataFormats","text":"Run 1 DataRun 2 Data <p>The physical objects are usually stored in specific collections.  For instance, muons are most commonly obtained from the <code>reco::Muon</code> collection.  The AOD Data Format Table gives a good description of the different collections (or data formats) for the AOD tier.  Unfortunately, the links for the containers column got broken after CMSSW was moved to Github.  Those links would have pointed us to the corresponding CMSSW C++ classes associated with those containers.  This is important because one needs to know which CMSSW class matches a given collection of objects to include the headers of those classes in the header of your analyzer code.  But let that not let us down.</p> <p></p> <p>Fortunately, the names of the collections containers actually match the name of its associated CMSSW classes.  These classes (data format classes) live under the DataFormats directory in CMSSW.  If we browse through, we find the <code>MuonReco</code> package.   In its <code>interface</code> area we find the DataFormats/MuonReco/interface/Muon.h class header, which is the one we would need to incorporate in our analyzer.  This is corroborated by this Muon Analysis Twiki section.</p> <p>Remember</p> <p>When accessing a specific piece of code in the CMSSW github repository, and want to explore its methods, variables, etc., make sure you select the right git branch.  E.g., CMSSW_5_3_X for 2011/2012 open data.</p> <p>In addition to this base class, sometimes it is necessary to invoke other auxiliary classes.  For instance, DataFormats/MuonReco/interface/MuonFwd.h, which can be found in the same interface area.</p> <p>So, in the context of this example, in order to support muons information, at the top of your EDAnalyzer you should include the following lines:</p> <pre><code>//classes to extract Muon information\n#include \"DataFormats/MuonReco/interface/Muon.h\"\n#include \"DataFormats/MuonReco/interface/MuonFwd.h\"\n</code></pre> <p>See an example of an EDAnalyzer accessing muon information in the MuonAnalyzer of the Physics Object Extractor Tool (POET).</p> <p>The physical objects are usually stored in specific collections.  For instance,  muons are obtained from the C++ class <code>std::vector&lt;pat::Muon&gt;</code> where <code>pat</code> stands for \"Physics analysis tools\" and is a namespace for miniAOD object classes. The collection is often called <code>pat::MuonCollection</code> using its definition in the muon class). The MINIAOD physics objects table gives a good description of the different collections (or data formats) for the MINIAOD tier. This is the muon entry in that table:</p> <p></p> <p>The MINIAOD data format classes live under the DataFormats/PatCandidates/interface directory in CMSSW. Here we find the DataFormats/PatCandidates/interface/Muon.h class header, which is the one we would need to incorporate in our analyzer. The other possible collection</p> <p>So, in the context of this example, in order to support muons information, at the top of your EDAnalyzer you should include the following line:</p> <pre><code>//classes to extract Muon information\n#include \"DataFormats/MuonReco/interface/Muon.h\"\n</code></pre> <p>See an example of an EDAnalyzer accessing muon information in the MuonAnalyzer of the Physics Object Extractor Tool (POET). The definitions of object classes used in miniAOD data format can be found in the list of all classes of the <code>pat</code> namespace.</p>"},{"location":"analysis/selection/objects/objects/#access-methods","title":"Access methods","text":"<p>In the Event methods for data access section of the Getting Data From an Event Twiki page, one can find a complete description of the different methods available for Event data access.</p> <p>Remember</p> <p>When accessing the CMS twiki pages we will usually point you to the most recent page.  However, historical Twiki documentation, i.e., earlier revision of the pages, may provide more accurate information for open data that is already a few years old. One can access this historical archive by going to the bottom of any Twiki page, clicking on History and exploring the revisions closer to the open data release year.</p> Run 1 DataRun 2 Data <p>As indicated in that page, all Event data access methods use the <code>edm::Handle&lt;T&gt;</code>, where <code>T</code> is the C++ type of the requested object, to hold the result of an access.  As an example, during Run 1, the recommended method was the <code>getByLabel</code> one.  This method needed an <code>InputTag</code>.  This can also be extracted from the AOD Data Format Table.  The first column indicate the InputTag:</p> <p></p> <p>Therefore, when accessing muon information, in the <code>analyze</code> method of the EDAnalyzer would include the following lines:</p> <pre><code>Handle&lt;reco::MuonCollection&gt; mymuons;\niEvent.getByLabel(\"muons\", mymuons);\n</code></pre> <p>If you required cosmic muons, you would need instead:</p> <pre><code>Handle&lt;reco::MuonCollection&gt; mymuons;\niEvent.getByLabel(\"muonsFromCosmics\", mymuons);\n</code></pre> <p>Alternatively, as done in the POET MuonAnalyzer, it is also possible to retrieve the InputTag name from configuration.  In that case, the configuration file has:</p> <pre><code>process.demo = cms.EDAnalyzer('MuonAnalyzer',\n                InputCollection = cms.InputTag(\"muons\")\n)\n</code></pre> <p>In this case, the appropriate input tag needs to be defined in the EDAnalyzer class as done in the MuonAnalyzer example class:</p> <pre><code>//declare the input tag for MuonCollection\nedm::InputTag muonInput;\n</code></pre> <p>It is extracted from the ParameterSet in the constructor</p> <pre><code>MuonAnalyzer::MuonAnalyzer(const edm::ParameterSet&amp; iConfig)\n{\n//now do what ever initialization is needed\nmuonInput = iConfig.getParameter&lt;edm::InputTag&gt;(\"InputCollection\");\n}\n</code></pre> <p>and used in the <code>analyze</code> method:</p> <pre><code>Handle&lt;reco::MuonCollection&gt; mymuons;\niEvent.getByLabel(muonInput, mymuons);\n</code></pre> <p>As indicated in that page, all Event data access methods use the <code>edm::Handle&lt;T&gt;</code>, where <code>T</code> is the C++ type of the requested object, to hold the result of an access.  For Run 2 data, the method is <code>getByToken</code>.  This method needs a \"token\" and an <code>InputTag</code>, which will pass the name of the collection to the analyzed. This name is indicated as \"Label\" the MINIAOD table, <code>slimmedMuons</code> for muons.</p> <p>The InputTag name is defined in the configuration. In that case, the configuration file has:</p> <pre><code>process.mymuons = cms.EDAnalyzer('MuonAnalyzer', \n                muons = cms.InputTag(\"slimmedMuons\")\n)\n</code></pre> <p>The appropriate token in the EDAnalyzer class as done in the MuonAnalyzer example class:</p> <pre><code>//declare the token for MuonCollection\nedm::EDGetTokenT&lt;pat::MuonCollection&gt; muonToken_;\n</code></pre> <p>which is then passed with the Input Tag to the constructor of the EDAnalyzer class</p> <pre><code>MuonAnalyzer::MuonAnalyzer(const edm::ParameterSet&amp; iConfig): muonToken_(consumes&lt;pat::MuonCollection&gt;(iConfig.getParameter&lt;edm::InputTag&gt;(\"muons\")))\n{\n//now do what ever initialization is needed\n</code></pre> <p>and used in the <code>analyze</code> method:</p> <pre><code>Handle&lt;pat::MuonCollection&gt; muons;\niEvent.getByToken(muonToken_, muons);\n</code></pre>"},{"location":"analysis/selection/objects/objects/#detector-information-for-identification","title":"Detector information for identification","text":"<p>The most signicant difference between a list of certain particles from a Monte Carlo generator and a list of the corresponding physics objects from CMS is likely the inherent uncertainty in the reconstruction. Selection of \u201ca muon\u201d or \u201can electron\u201d for analysis requires algorithms designed to separate \u201creal\u201d objects from \u201cfakes\u201d. These are called identification algorithms, often abbreviated as ID.</p> <p>Isolation algorithms are designed to measure the amount of energy deposited near the object, to determine if it was likely produced near the primary interaction (typically little nearby energy), or from the decay of a longer-lived particle (typically a lot of nearby energy). Many types of isolation algorithms exist to deal with unique physics cases.</p> <p>Isolation is computed in similar ways for all physics objects: search for particles in a cone around the object of interest and sum up their energies, subtracting off the energy deposited by pileup particles. This sum divided by the object of interest\u2019s transverse momentum is called relative isolation and is the most common way to determine whether an object was produced \u201cpromptly\u201d in or following the proton-proton collision (ex: electrons from a Z boson decay, or photons from a Higgs boson decay). Relative isolation values will tend to be large for particles that emerged from weak decays of hadrons within jets, or other similar \u201cnonprompt\u201d processes.</p> <p>Both types of algorithms function using working points that are described on a spectrum from \u201cloose\u201d to \u201ctight\u201d. Working points that are \u201clooser\u201d tend to have a high efficiency for accepting real objects, but perhaps a poor rejection rate for fake objects. Working points that are \u201ctighter\u201d tend to have lower efficiencies for accepting real objects, but much better rejection rates for fake objects. The choice of working point is highly analysis dependent. Some analyses value efficiency over background rejection, and some analyses are the opposite.</p> <p>The standard identification and isolation algorithm results can be accessed from the physics object classes.</p>"},{"location":"analysis/selection/objects/objects/#additional-information","title":"Additional information","text":"<p>The next pages in this guide provide further details on how to access and identify CMS physics objects.</p> <p>In addition, in Chapter 7 of the CMS Workbook one can find Analysis pages that provide additional information, which can be useful to check on top of the general strategy for accessing objects that was discussed above.</p>"},{"location":"analysis/selection/objects/photons/","title":"Photons","text":""},{"location":"analysis/selection/objects/photons/#introduction","title":"Introduction","text":"<p>Photons are measured in the CMS experiment in the electromagnetic calorimeter and, in case they convert to electron-positron pairs, also in the inner tracker as summarized on  introductory page on Finding electrons and photons. The signals from these systems are processed with CMSSW through subsequent steps to form photon candidates which are then available in the photon collection of the data files.</p>"},{"location":"analysis/selection/objects/photons/#photon-4-vector-information","title":"Photon 4-vector information","text":"Run 1 DataRun 2 Data <p>An example of an EDAnalyzer accessing photon information is available in the PhotonAnalyzer of the Physics Object Extractor Tool (POET). The following header files needed for accessing electron information are included:</p> <pre><code>//classes to extract Photon information\n#include \"DataFormats/EgammaCandidates/interface/Photon.h\"\n#include \"DataFormats/EgammaCandidates/interface/PhotonFwd.h\"\n#include \"DataFormats/GsfTrackReco/interface/GsfTrack.h\"\n#include \"DataFormats/EgammaCandidates/interface/GsfElectron.h\"\n#include \"RecoEgamma/EgammaTools/interface/ConversionTools.h\"\n#include \"EgammaAnalysis/ElectronTools/interface/PFIsolationEstimator.h\"\n</code></pre> <p>In PhotonAnalyzer.cc, the photon four-vector elements are accessed as shown below.</p> <pre><code>Handle&lt;reco::PhotonCollection&gt; myphotons;\niEvent.getByLabel(photonInput, myphotons);\n\n[...]\n\nfor (reco::PhotonCollection::const_iterator itphoton=myphotons-&gt;begin(); itphoton!=myphotons-&gt;end(); ++itphoton){\n\n[...]\n\nphoton_e.push_back(itphoton-&gt;energy());\nphoton_pt.push_back(itphoton-&gt;pt());\nphoton_px.push_back(itphoton-&gt;px());\nphoton_py.push_back(itphoton-&gt;py());\nphoton_pz.push_back(itphoton-&gt;pz());\nphoton_eta.push_back(itphoton-&gt;eta());\nphoton_phi.push_back(itphoton-&gt;phi());\n\n[...]\n}\n</code></pre> <p>An example of an EDAnalyzer accessing electron information is available in the PhotonAnalyzer of the Physics Object Extractor Tool (POET). The following header file needed for accessing photon information is included:</p> <pre><code>//class to extract photon information\n#include \"DataFormats/PatCandidates/interface/Photon.h\"\n</code></pre> <p>In PhotonAnalyzer.cc, the electron four-vector elements are accessed from the <code>pat::photon</code> collection as shown below.</p> <pre><code>Handle&lt;pat::PhotonCollection&gt; photons;\niEvent.getByToken(photonToken_, photons);\n\n[...]\n\nfor (const pat::Photon &amp;pho : *photons)\n{\nphoton_e.push_back(pho.energy());\nphoton_pt.push_back(pho.pt());\nphoton_px.push_back(pho.px());\nphoton_py.push_back(pho.py());\nphoton_pz.push_back(pho.pz());\nphoton_eta.push_back(pho.eta());\nphoton_phi.push_back(pho.phi());\n\n[...]\n}\n</code></pre> <p>with <code>photonToken_</code> defined as a member of the <code>PhotonAnalyzer</code> class and its value read from the configuration file.</p>"},{"location":"analysis/selection/objects/photons/#photon-identification","title":"Photon identification","text":"<p>As explained in the Physics Object page, a mandatory task in the physics analysis is to identify photons, i.e. to separate \u201creal\u201d objects from \u201cfakes\u201d. A large fraction of the energy deposited in the detector by all proton-proton interactions arises from photons originating in the decay of neutral mesons, and these electromagnetic showers provide a substantial background to signal photons. The identification criteria depend on the type of analysis.</p> Run 1 DataRun 2 Data <p>The standard identification and isolation algorithm results can be accessed from the photon object class and the recommended working points for 2012 are implemented in the example code PhotonAnalyzer.cc.</p> <p>Three levels of identification criteria are defined</p> <pre><code>bool isLoose = false, isMedium = false, isTight = false;\n</code></pre> <p>For photons in the electromagnetic calorimeter barrel area, they are determined as follows:</p> <pre><code>if ( itphoton-&gt;eta() &lt;= 1.479 ){\nif ( ph_hOverEm&lt;.05 &amp;&amp; ph_sigIetaIeta&lt;.012 &amp;&amp; corrPFCHIso&lt;2.6 &amp;&amp; corrPFNHIso&lt;(3.5+.04*itphoton-&gt;pt()) &amp;&amp; corrPFPhIso&lt;(1.3+.005*itphoton-&gt;pt()) &amp;&amp; passelectronveto==true) {\nisLoose = true;\n\nif ( ph_sigIetaIeta&lt;.011 &amp;&amp; corrPFCHIso&lt;1.5 &amp;&amp; corrPFNHIso&lt;(1.0+.04*itphoton-&gt;pt()) &amp;&amp; corrPFPhIso&lt;(.7+.005*itphoton-&gt;pt())){\nisMedium = true;\n\nif ( corrPFCHIso&lt;.7 &amp;&amp; corrPFNHIso&lt;(.4+.04*itphoton-&gt;pt()) &amp;&amp; corrPFPhIso&lt;(.5+0.005*itphoton-&gt;pt()) ){\nisTight = true;\n}\n}\n}\n}\n</code></pre> <p>where</p> <ul> <li><code>ph_sigIetaIeta</code> describes the variance of the ECAL cluster in psuedorapidity (\"ieta\" is an integer index for this angle).</li> <li><code>ph_hOverEm</code> describes the ratio of HCAL to ECAL energy deposits, which should be small for good quality photons.</li> <li>The electron veto <code>passelectronveto</code> is obtained from an algorithm that indicates if photons have been identified also as electrons.</li> <li><code>corr...Iso</code> variables represent different isolation properties of the photon.</li> </ul> <p>The isolation variables are defined with the <code>PFIsolationEstimator</code> class in the default cone size of 0.3 with</p> <pre><code>PFIsolationEstimator isolator;\nisolator.initializePhotonIsolation(kTRUE);\nisolator. setConeSize(0.3);\nconst reco::VertexRef vertex(vertices, 0);\nconst reco::Photon &amp;thephoton = *itphoton;\nisolator.fGetIsolation(&amp;thephoton, pfCands.product(), vertex, vertices);\ndouble corrPFCHIso = std::max(isolator.getIsolationCharged() - rhoIso * aEff.CH_AEff, 0.)/itphoton-&gt;pt();\ndouble corrPFNHIso = std::max(isolator.getIsolationNeutral() - rhoIso * aEff.NH_AEff, 0.)/itphoton-&gt;pt();\ndouble corrPFPhIso = std::max(isolator.getIsolationPhoton() - rhoIso * aEff.Ph_AEff, 0.)/itphoton-&gt;pt();\n</code></pre> <p>In the endcap part of the electromagnetic calorimeter, the procedure is similar with different values.</p> <p>To do</p> <ul> <li>The isolation snippet needs more explanation</li> </ul> <p>The standard identification and isolation algorithm results can be accessed from the pat photon object class. Three levels of identification criteria are defined: loose, medium, and tight. An example selection is implemented in PhotonAnalyzer.cc:</p> <pre><code>  photon_isLoose.push_back(pho.photonID(\"cutBasedPhotonID-PHYS14-PU20bx25-V2p1-standalone-loose\"));\nphoton_isMedium.push_back(pho.photonID(\"cutBasedPhotonID-PHYS14-PU20bx25-V2p1-standalone-medium\"));\nphoton_isTight.push_back(pho.photonID(\"cutBasedPhotonID-PHYS14-PU20bx25-V2p1-standalone-tight\"));\n</code></pre> <p>Warning</p> <p>The choice of recommended photon ID criteria for 2015 data needs to be verified. In addition to <code>PHYS14_PU20bx25_V2</code> other sets, for example <code>Spring15_25ns_V1</code>, are available.</p> <p>Several isolation methods are available through the class member methods, for example:</p> <pre><code>  photon_chIso.push_back(pho.chargedHadronIso());\nphoton_nhIso.push_back(pho.neutralHadronIso());\nphoton_phIso.push_back(pho.photonIso());\n</code></pre> <p>To do</p> <ul> <li>Verify the recommended isolation, many other are available in the photon class</li> </ul> <p>To do</p> <p>Add a mention on photon conversions both for Run 1 and Run 2</p>"},{"location":"analysis/selection/objects/taus/","title":"Taus","text":""},{"location":"analysis/selection/objects/taus/#introduction","title":"Introduction","text":"<p>Tau lepton having a mass of 1.777 GeV, is the only lepton heavy enough to decay into hadrons. As depicted in the pi-chart, in about one third of the cases \u03c4\u2019s decay leptonically to a muon (\u03c4\u03bc) or an electron (\u03c4e) with two neutrinos, and are reconstructed and identified with the usual techniques for muons and electrons. In the remaining cases, \u03c4 leptons decay hadronically, to a combination of charged and neutral mesons with a \u03c4\u03bd.</p> <p></p> <p>Hadronically decaying \u03c4\u2019s, denoted by \u03c4h, are reconstructed and identified with the hadrons-plus-strips (HPS) algorithm, which was developed for use in the LHC Run-1. The key challenge that this algorithm has to face is the distinction between genuine \u03c4h, and quark and gluon jets, which are copiously produced in QCD multijet process and can be misidentified as \u03c4h. The main handle for reducing these jet\u2192\u03c4h misidentification backgrounds is to utilize the fact that the particles produced in \u03c4h decays are of lower multiplicity, deposit energy in a narrow region compared to a quark or gluon jet, and are typically isolated with respect to other particles in the event.</p> <p>In some physics analyses, the misidentification of electrons or muons as \u03c4h candidates may constitute a sizeable background as well. Therefore, HPS algorithm has got various discriminators like isolation, against electrons and muons etc. to identify genuine hadronically decaying taus.</p>"},{"location":"analysis/selection/objects/taus/#tau-4-vector-information","title":"Tau 4-vector information","text":"Run 1 DataRun 2 Data <p>An example of an EDAnalyzer tau information is available in the TauAnalyzer of the Physics Object Extractor Tool (POET). The following header files needed for accessing tau information are included:</p> <pre><code>//classes to extract tau information\n#include \"DataFormats/TauReco/interface/PFTau.h\"\n#include \"DataFormats/TauReco/interface/PFTauFwd.h\"\n#include \"DataFormats/TauReco/interface/PFTauDiscriminator.h\"\n</code></pre> <p>In TauAnalyzer.cc, the tau four-vector elements are accessed as shown below.</p> <pre><code>Handle&lt;reco::PFTauCollection&gt; mytaus;\niEvent.getByLabel(tauInput, mytaus);\n\n[...]\n\nfor (reco::PFTauCollection::const_iterator itTau=mytaus-&gt;begin(); itTau!=mytaus-&gt;end(); ++itTau){\nif (itTau-&gt;pt() &gt; tau_min_pt) {\ntau_e.push_back(itTau-&gt;energy());\ntau_pt.push_back(itTau-&gt;pt());\ntau_px.push_back(itTau-&gt;px());\ntau_py.push_back(itTau-&gt;py());\ntau_pz.push_back(itTau-&gt;pz());\ntau_eta.push_back(itTau-&gt;eta());\ntau_phi.push_back(itTau-&gt;phi());\n\n[...]\n}\n</code></pre> <p>An example of an EDAnalyzer tau information is available in the TauAnalyzer of the Physics Object Extractor Tool (POET). The following header file needed for accessing tau information is included:</p> <pre><code>//class to extract tau information\n#include \"DataFormats/PatCandidates/interface/Tau.h\"\n</code></pre> <p>In TauAnalyzer.cc, the tau four-vector elements are accessed from the <code>pat::tau</code> collection as shown below.</p> <pre><code>Handle&lt;pat::TauCollection&gt; taus;\niEvent.getByToken(tauToken_, taus);\n[...]\n\nfor (const pat::Tau &amp;tau : *taus)\n{\ntau_e.push_back(tau.energy());\ntau_pt.push_back(tau.pt());\ntau_px.push_back(tau.px());\ntau_py.push_back(tau.py());\ntau_pz.push_back(tau.pz());\ntau_eta.push_back(tau.eta());\ntau_phi.push_back(tau.phi());\n\n[...]\n}\n</code></pre> <p>with <code>tauToken_</code> defined as a member of the <code>TauAnalyzer</code> class and its value read from the configuration file.</p>"},{"location":"analysis/selection/objects/taus/#tau-identification","title":"Tau identification","text":"Run 1 DataRun 2 Data <p>The identification of taus relies almost entirely on pre-computed algorithms to determine the quality of the tau reconstruction and the decay type. Since this object is not stable and has several decay modes, different combinations of identification and isolation algorithms are used across different analyses. The Tau tagging CMS WorkBook page provides a large table of available algorithms.</p> <p>In contrast to the muon object, tau algorithm results are typically saved in the AOD files as their own PFTauDisciminator collections, rather than as part of the tau object class. As shown in TauAnalyzer.cc, these collections can be accessed with the following impressively long label names:</p> <pre><code>Handle&lt;reco::PFTauDiscriminator&gt; tausLooseIso, tausVLooseIso, tausMediumIso, tausTightIso, tausTightEleRej, tausTightMuonRej, tausDecayMode, tausRawIso;\n\niEvent.getByLabel(InputTag(\"hpsPFTauDiscriminationByDecayModeFinding\"),tausDecayMode);\niEvent.getByLabel(InputTag(\"hpsPFTauDiscriminationByRawCombinedIsolationDBSumPtCorr\"), tausRawIso);\niEvent.getByLabel(InputTag(\"hpsPFTauDiscriminationByVLooseCombinedIsolationDBSumPtCorr\"), tausVLooseIso);\niEvent.getByLabel(InputTag(\"hpsPFTauDiscriminationByLooseCombinedIsolationDBSumPtCorr\"), tausLooseIso);\niEvent.getByLabel(InputTag(\"hpsPFTauDiscriminationByMediumCombinedIsolationDBSumPtCorr\"), tausMediumIso);\niEvent.getByLabel(InputTag(\"hpsPFTauDiscriminationByTightCombinedIsolationDBSumPtCorr\"), tausTightIso);\niEvent.getByLabel(InputTag(\"hpsPFTauDiscriminationByTightElectronRejection\"), tausTightEleRej);\niEvent.getByLabel(InputTag(\"hpsPFTauDiscriminationByTightMuonRejection\"), tausTightMuonRej);\n</code></pre> <p>The tau discriminator collections act as pairs, containing the index of the tau and the value of the discriminant for that tau. Note that the arrays are filled by calls to the individual discriminant objects, but referencing the vector index of the tau in the main tau collection.</p> <pre><code>const auto idx = itTau - mytaus-&gt;begin();\ntau_iddecaymode.push_back(tausDecayMode-&gt;operator[](idx).second);\ntau_idisoraw.push_back(tausRawIso-&gt;operator[](idx).second);\ntau_idisovloose.push_back(tausVLooseIso-&gt;operator[](idx).second);\ntau_idisoloose.push_back(tausLooseIso-&gt;operator[](idx).second);\ntau_idisomedium.push_back(tausMediumIso-&gt;operator[](idx).second);\ntau_idisotight.push_back(tausTightIso-&gt;operator[](idx).second);\ntau_idantieletight.push_back(tausTightEleRej-&gt;operator[](idx).second);\ntau_idantimutight.push_back(tausTightMuonRej-&gt;operator[](idx).second);\n</code></pre> <p>The recommendations for applying these discriminators are summarizes in the Tau identification nutshell recipe.</p> <p>To do</p> <ul> <li>to complete once POET 2015MiniAOD branch TauAnalyzer has been validated</li> </ul>"},{"location":"analysis/selection/objects/tools/","title":"Common tools for physics objects","text":"<p>Many of the most important kinematic quantities defining a physics object are accessed in a common way across all the objects. All objects have associated energy-momentum vectors, typically constructed using transverse momentum, pseudorapdity, azimuthal angle, and mass or energy.</p>"},{"location":"analysis/selection/objects/tools/#4-vector-access-functions","title":"4-vector access functions","text":"Run 1 DataRun 2 Data <p>In MuonAnalyzer.cc, the muon four-vector elements are accessed as shown below. In this example, the values for each muon are stored into an array, which will become a branch in a ROOT TTree.</p> <pre><code>Handle&lt;reco::MuonCollection&gt; mymuons;\niEvent.getByLabel(muonInput, mymuons);\n\n[...]\n\nfor (reco::MuonCollection::const_iterator itmuon=mymuons-&gt;begin(); itmuon!=mymuons-&gt;end(); ++itmuon){\nif (itmuon-&gt;pt() &gt; mu_min_pt) {\n\nmuon_e.push_back(itmuon-&gt;energy());\nmuon_pt.push_back(itmuon-&gt;pt());\nmuon_eta.push_back(itmuon-&gt;eta());\nmuon_phi.push_back(itmuon-&gt;phi());\n\nmuon_px.push_back(itmuon-&gt;px());\nmuon_py.push_back(itmuon-&gt;py());\nmuon_pz.push_back(itmuon-&gt;pz());\n\nmuon_mass.push_back(itmuon-&gt;mass());\n\n}\n</code></pre> <p>The same type of kinematic member functions are used in all the different analyzers in the src/ directory of the POET example code. These and other basic kinematic methods are defined in the LeafCandidate class of the CMSSW DataFormats package (rendered for maybe easier readability in the CMSSW software documentation).</p> <p>In MuonAnalyzer.cc, the muon four-vector elements are accessed as shown below. In this example, the values for each muon are stored into an array, which will become a branch in a ROOT TTree.</p> <pre><code>Handle&lt;pat::MuonCollection&gt; muons;\niEvent.getByToken(muonToken_, muons);\n\n[...]\n\nfor (const pat::Muon &amp;mu : *muons)\n{\nmuon_e.push_back(mu.energy());\nmuon_pt.push_back(mu.pt());\nmuon_px.push_back(mu.px());\nmuon_py.push_back(mu.py());\nmuon_pz.push_back(mu.pz());\nmuon_eta.push_back(mu.eta());\nmuon_phi.push_back(mu.phi());\n\n[...]\n\n}\n</code></pre> <p>The same type of kinematic member functions are used in all the different analyzers in the src/ directory of the POET example code. These and other basic kinematic methods are defined in the LeafCandidate class of the CMSSW DataFormats package (rendered for maybe easier readability in the CMSSW software documentation).</p>"},{"location":"analysis/selection/objects/tools/#track-access-functions","title":"Track access functions","text":"<p>Many objects are also connected to tracks from the CMS tracking detectors. Information from tracks provides other kinematic quantities that are common to multiple types of objects.</p> Run 1 DataRun 2 Data <p>From a muon object, we can access the associated track while looping over muons via the <code>globalTrack</code> method:</p> <pre><code>auto trk = mu-&gt;globalTrack(); // muon track\n</code></pre> <p>Often, the most pertinent information about an object (such as a muon) to access from its associated track is its impact parameter with respect to the primary interaction vertex. Since muons can also be tracked through the muon detectors, we first check if the track is well-defined, and then access impact parameters in the xy-plane (<code>dxy</code> or <code>d0</code>) and along the beam axis (<code>dz</code>), as well as their respective uncertainties. They can be accessed as shown in this code snippet from MuonAnalyzer:</p> <pre><code>    auto trk = itmuon-&gt;globalTrack();\nif (trk.isNonnull()) {\nmuon_dxy.push_back(trk-&gt;dxy(pv));\nmuon_dz.push_back(trk-&gt;dz(pv));\nmuon_dxyErr.push_back(trk-&gt;d0Error());\nmuon_dzErr.push_back(trk-&gt;dzError());\n}\n</code></pre> <p>These functions need the position of the interaction vertex (<code>pv</code>) as an input and this is provided through the first elemement of the vertex collection <code>vertices</code> which gives the best estimate of the interaction point. The vertex collection is accessed with:</p> <pre><code>  Handle&lt;reco::VertexCollection&gt; vertices;\niEvent.getByLabel(InputTag(\"offlinePrimaryVertices\"), vertices);\n\n[...]\n\nmath::XYZPoint pv(vertices-&gt;begin()-&gt;position());\n</code></pre> <p>Note that the tracking method depends on the object, the electron tracks are found using the Gaussian-sum filter method <code>gsfTrack</code>:</p> <pre><code>auto trk = it-&gt;gsfTrack(); // electron track\n</code></pre> <p>From a muon object, we can access the associated track while looping over muons via the <code>muonBestTrack</code> method.</p> <p>Often, the most pertinent information about an object (such as a muon) to access from its associated track is its impact parameter with respect to the primary interaction vertex. Since muons can also be tracked through the muon detectors, we first check if the track is well-defined, and then access impact parameters in the xy-plane (<code>dxy</code> or <code>d0</code>) and along the beam axis (<code>dz</code>), as well as their respective uncertainties. They can be accessed as shown in this code snippet from MuonAnalyzer:</p> <pre><code>  muon_dxy.push_back(mu.muonBestTrack()-&gt;dxy(PV.position()));\nmuon_dz.push_back(mu.muonBestTrack()-&gt;dz(PV.position()));\nmuon_dxyError.push_back(mu.muonBestTrack()-&gt;d0Error());\nmuon_dzError.push_back(mu.muonBestTrack()-&gt;dzError());\n</code></pre> <p>These functions need the position of the interaction vertex (<code>PV</code>) as an input and this is provided through the first elemement of the vertex collection <code>vertices</code> which gives the best estimate of the interaction point. The vertex collection is accessed with:</p> <pre><code>  Handle&lt;reco::VertexCollection&gt; vertices;\niEvent.getByToken(vtxToken_, vertices);\nconst reco::Vertex &amp;PV = vertices-&gt;front();\n</code></pre> <p>Note that the tracking method depends on the object, the electron tracks are found using the Gaussian-sum filter method <code>gsfTrack</code>.</p>"},{"location":"analysis/selection/objects/tools/#matching-to-generated-particles","title":"Matching to generated particles","text":"<p>Simulated files also contain information about the generator-level particles that were propagated into the showering and detector simulations. Physics objects can be matched to these generated particles spatially.</p> Run 1 DataRun 2 Data <p>The AOD2NanoAOD tool is an example code extracting objects from AOD file and storing them in an output file. In its analyzer code, it sets up several utility functions for matching: <code>findBestMatch</code>, <code>findBestVisibleMatch</code>, and <code>subtractInvisible</code>. The <code>findBestMatch</code> function takes generated particles (with an automated type <code>T</code>) and the 4-vector of a physics object. It uses angular separation to find the closest generated particle to the reconstructed particle:</p> <pre><code>template &lt;typename T&gt;\nint findBestMatch(T&amp; gens, reco::Candidate::LorentzVector&amp; p4) {\n\n# initial definition of \"closest\" is really bad\nfloat minDeltaR = 999.0;\nint idx = -1;\n\n# loop over the generated particles\nfor (auto g = gens.begin(); g != gens.end(); g++) {\nconst auto tmp = deltaR(g-&gt;p4(), p4);\n\n# if it is closer, overwrite the definition of closest\nif (tmp &lt; minDeltaR) {\nminDeltaR = tmp;\nidx = g - gens.begin();\n}\n}\nreturn idx; # return the index of the match\n}\n</code></pre> <p>The other utility functions are similar, but correct for generated particles that decay to neutrinos, which would affect the \"visible\" 4-vector.</p> <p>In the AOD2NanoAOD tool, muons are matched only to \"interesting\" generated particles, which are all the leptons and photons (PDG ID 11, 13, 15, 22). Their generator status must be 1, indicating a final-state particle after any radiation chain.</p> <pre><code>if (!isData){\nvalue_gen_n = 0;\n\nfor (auto p = selectedMuons.begin(); p != selectedMuons.end(); p++) {\n\n// get the muon's 4-vector\nauto p4 = p-&gt;p4();\n\n// perform the matching with a utility function\nauto idx = findBestVisibleMatch(interestingGenParticles, p4);\n\n// if a match was found, save the generated particle's information\nif (idx != -1) {\nauto g = interestingGenParticles.begin() + idx;\n\n// another example of common 4-vector access functions!\nvalue_gen_pt[value_gen_n] = g-&gt;pt();\nvalue_gen_eta[value_gen_n] = g-&gt;eta();\nvalue_gen_phi[value_gen_n] = g-&gt;phi();\nvalue_gen_mass[value_gen_n] = g-&gt;mass();\n\n// gen particles also have ID and status from the generator\nvalue_gen_pdgid[value_gen_n] = g-&gt;pdgId();\nvalue_gen_status[value_gen_n] = g-&gt;status();\n\n// save the index of the matched gen particle\nvalue_mu_genpartidx[p - selectedMuons.begin()] = value_gen_n;\nvalue_gen_n++;\n}\n}\n}\n</code></pre> <p>To do</p> <p>This will refer to Run 2 MiniAOD branch of AOD2NanoAODOutreachTool once available</p>"},{"location":"analysis/systematics/lumiuncertain/","title":"Luminosity Uncertainty","text":"<p>Warning</p> <p>This page is under construction</p>"},{"location":"analysis/systematics/mcuncertain/","title":"MC Uncertainty","text":"<p>Warning</p> <p>This page is under construction</p>"},{"location":"analysis/systematics/pileupuncertain/","title":"Pileup Uncertainty","text":"<p>Warning</p> <p>This page is under construction</p>"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/","title":"B Tag Uncertainty","text":""},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#scale-factors","title":"Scale Factors","text":"<p>In simulation,</p> <ul> <li>Efficiency for tagging b quarks as b jets: the number of \"real b jets\" (jets spatially matched to generator-level b hadrons) tagged as b jets divided by the number of real b jets.</li> <li>Efficiency for mis-tagging c or light quarks as b jets: real c/light jets tagged as b jets divided by real c/light jets.</li> </ul> <p>These values are typically computed as functions of the momentum or pseudorapidity of the jet. The \"real\" flavor of the jet is accessed most simply by creating pat::Jet objects instead of reco::Jet objects.</p> <p>Scale factors to increase or decrease the number of b-tagged jets in simulation can be applied in a number of ways, but typically involve weighting simulation events based on the efficiencies and scale factors relevant to each jet in the event. Scale factors for the CSV algorithm are available for Open Data and involve extracting functions from a comma-separated-values file. The main documentation for b tagging and scale factors can be found in the b tagging recommendation twiki.</p>"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#applying-scale-factors","title":"Applying Scale Factors","text":""},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#calculating-efficiencies","title":"Calculating Efficiencies","text":"<p>The BTagging folder of PhysObjectExtractorTool (POET) is used for calculating the efficiency for tagging each flavor of jet as a b quark, as a function of the jet momentum with the file WeightAnalyzer.cc. The purpose of this file is to set up jet momentum histograms for numerators and denominators of efficiency histograms as defined above. The code loops through the jets, checks their flavor, checks their btagging discriminator to see if it passes tight, medium and or loose cut, and then fills the histograms according to that information.</p> <pre><code>    double disc = it-&gt;bDiscriminator(discriminatorStr);\nint hadronFlavor = it-&gt;partonFlavour();\n\nif( abs(hadronFlavor)==5 ){\nBEff_Dptbins_b-&gt;Fill(pt,weight);      if( disc &gt;= discriminatorValueT) BEffTight_Nptbins_b-&gt;Fill(pt,weight);\nif( disc &gt;= discriminatorValueM) BEffMed_Nptbins_b-&gt;Fill(pt,weight);\nif( disc &gt;= discriminatorValueL) BEffLoose_Nptbins_b-&gt;Fill(pt,weight);\n} else if( abs(hadronFlavor)==4 ){\n...\n</code></pre> <p>These historgrams are then stored in an output file.</p> <p>Input, output, and other parameters can be changed in the config file.</p> <p>After this, you can save, exit, and compile, and then move onto the config file. You will put the file(s) which you wish to run efficiencies on here:</p> <pre><code>##### ------- This is a test file\nprocess.source = cms.Source(\"PoolSource\",\nfileNames = cms.untracked.vstring(\n'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12_DR53X/TTbar_8TeV-Madspin_aMCatNLO-herwig/AODSIM/PU_S10_START53_V19-v2/00000/04FCA1D5-E74C-E311-92CE-002590A887F0.root'))\n</code></pre> <p>Once this is complete, you can run the config file for your efficiencies.</p>"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#run-complete","title":"Run Complete","text":"<p>Once your run is complete, in the 'BTagging' folder there should be a file called plotBeff.C. This file is set up to do the numerator and denomenator divisions (as defined earlier), show you a histogram of your efficiencies from those calculations, and write the same efficiencies that you calculated in a numerical form. To run this code open this file in root like such:</p> <pre><code>root plotBeff.c\n</code></pre> <p>The histogram and output should appear through root. An example of what the histogram should look like is this:</p> <p></p>"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#if-needed-updating-momentum-bin-code","title":"If Needed: Updating Momentum Bin Code","text":"<p> In WeightAnalyzer.cc, there is a spot to input custom jet momentum bins that looks like this: </p> <pre><code>double ptbinsB[10] = {0, 15, 30, 50, 70, 100, 150, 200, 500, 1000};\n</code></pre> <p> where a bin's momentums span from 0 to 15, 15 to 30, etc. </p> <p> After your jet momentum bin update, you need to update the actual code that produces the histogram. Continuing this example, there are a total of 9 momentum bins from the numbers given in, ptbinsB. In the histogram producing code, there is a 9 indicating the number of bins: </p> <pre><code>  BEff_Dptbins_b    = fs-&gt;make&lt;TH1D&gt;(\"BEff_Dptbins_b   \",\"\",9,ptbinsB); BEff_Dptbins_b-&gt;Sumw2();\n</code></pre> <p> Where the number 9 is now, this number will need to be updated to your number of bins. </p>"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#access-efficiencies","title":"Access Efficiencies","text":"<p>Once you have your efficiencies, you can then put them in to the 3 look up functions that have been implemented in PatJetAnalyzer for storing efficiencies. Here, for example, is the b tag efficiencies function which returns efficiency given a jet momentum:</p> <pre><code>double\nPatJetAnalyzer::getBtagEfficiency(double pt){\nif(pt &lt; 25) return 0.263407;\nelse if(pt &lt; 50) return 0.548796;\nelse if(pt &lt; 75) return 0.656801;\nelse if(pt &lt; 100) return 0.689167;\nelse if(pt &lt; 125) return 0.697911;\nelse if(pt &lt; 150) return 0.700187;\nelse if(pt &lt; 200) return 0.679236;\nelse if(pt &lt; 400) return 0.625296;\nelse return 0.394916;\n}\n</code></pre>"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#access-scale-factors","title":"Access Scale Factors","text":"<p>The data file provided by the CMS b tagging group contains the scale factor functions for all types of jets. Some important titles to give more context to are as follows:</p> <ul> <li>OperatingPoint - This is the light (0), medium (1), or tight (2) cut of the flavored jet.</li> <li>formula - This is the equation for calculating the scale factor, where x is the momentum of the jet.</li> <li>jetFlavor - b = 0, c = 1, udsg = 2.</li> </ul> <p>Sorting Columns and creating filters with the .csv file can make accessing and finding sepcific scale factor equations easier. For example, filtering the OperatingPoint column to only show the number 1 will give you only medium cut jet information. Other useful information about the .csv file can be found here.</p> <p>The scale factor equations from the folumla column have been implemented in POET! In PatJetAnalyzer there are 2 functions, one for b and c flavored jets and one for light flavored jets, that return the scale factor of the jet depending on the momentum of the jet. Below is the b and c tag function.</p> <pre><code>double\nPatJetAnalyzer::getBorCtagSF(double pt, double eta){\nif (pt &gt; 670.) pt = 670;\nif(fabs(eta) &gt; 2.4 or pt&lt;20.) return 1.0;\n\nreturn 0.92955*((1.+(0.0589629*pt))/(1.+(0.0568063*pt)));\n}\n</code></pre> <p>Look at this twiki for additional information about scale factors.</p>"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#calculating-weights","title":"Calculating Weights","text":"<p>Once these functions are updated to their desired states, weight calculating can happen! The first thing to check for when event weight calculating is this: <code>if (jet_btag.at(value_jet_n) &gt; 0.679)</code>. This check is to see whether or not the jet distminator makes the cut we want our jets to make. In this case, we want our jets to make the medium cut (.679). If a jet makes the cut, there are then a couple more checks to be made:</p> <pre><code>  if(abs(hadronFlavour) == 5){\neff = getBtagEfficiency(corrpt);\nSF = getBorCtagSF(corrpt, jet_eta.at(value_jet_n));\nSFu = SF + uncertaintyForBTagSF(corrpt, jet_eta.at(value_jet_n));\nSFd = SF - uncertaintyForBTagSF(corrpt, jet_eta.at(value_jet_n));\n} else if(abs(hadronFlavour) == 4){\neff = getCtagEfficiency(corrpt);\nSF = getBorCtagSF(corrpt, jet_eta.at(value_jet_n));\nSFu = SF + (2 * uncertaintyForBTagSF(corrpt, jet_eta.at(value_jet_n)));\nSFd = SF - (2 * uncertaintyForBTagSF(corrpt, jet_eta.at(value_jet_n)));\n} else {\neff = getLFtagEfficiency(corrpt);\nSF = getLFtagSF(corrpt, jet_eta.at(value_jet_n));\nSFu = SF + ( uncertaintyForLFTagSF(corrpt, jet_eta.at(value_jet_n)));\nSFd = SF - ( uncertaintyForLFTagSF(corrpt, jet_eta.at(value_jet_n)));\n}\n</code></pre> <p>This section first finds which flavor of jet it is (b = 5, c = 4, and light = anything else) and then gets the efficiency for the respected jet, as well as calculates its scale factor. It also calculates its up and down quarked scale factors of the jet. Once these checks and calculations are complete, the following calulations can occur:</p> <pre><code> MC *= eff;\nbtagWeight *= SF * eff;\nbtagWeightUp *= SFu * eff;\nbtagWeightDn *= SFd * eff;\n</code></pre> <p>These calculations are the probability of a given configuration of jets in MC simulation (<code>MC</code>) and data (<code>btagWeight</code>, <code>btagWeightUp</code>, and <code>btagWeightDn</code>). The same process with a little bit different probability calculating is done if the jet did not meet the desired cut.</p> <p>Once these checks have finished for every jet you are looking at, a final calculation for the event weights is done.</p> <pre><code>btagWeight = (btagWeight/MC);\nbtagWeightUp = (btagWeightUp/MC);\nbtagWeightDn = (btagWeightDn/MC);\n</code></pre> <p>NOTE: There are many ways to go about calculating event weights. This link shows a couple of the different ways. In POET, method 1a is the method used.</p>"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#uncertainties","title":"Uncertainties","text":"<p>As we just saw in the \"Calculating Weights\" section above, there are uncertainties that need to be considered. These uncertainties are actually already taken into account in the .csv file. When looking at the scale factor equation, there should be a main equation followed by either an addition or subtraction of a number, which is the uncertainty.</p>"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#uncertainties-for-each-flavor","title":"Uncertainties for Each Flavor","text":"<p>In POET, there are 2 functions for the uncertainty, one for the b tag uncertainty and one for the light flavor tag uncertainty. The reason that there is not one specifically for c tagged jets is because c tagged jet's uncertainty is two times that of the b tagged jet's uncertainty, so you can simply multiply the b tag uncertainty call by two, as seen here: <code>SFu = SF + (2 * uncertaintyForBTagSF(corrpt, jet_eta.at(value_jet_n)));</code></p> <p>Here is what the b tag uncertainty function looks like, which returns the uncertainty given a jet momentum:</p> <pre><code>double\nPatJetAnalyzer::uncertaintyForBTagSF( double pt, double eta){\nif(fabs(eta) &gt; 2.4 or pt&lt;20.) return 0;\nif(pt &lt; 30) return 0.0466655;\nelse if(pt &lt; 40) return 0.0203547;\nelse if(pt &lt; 50) return 0.0187707;\nelse if(pt &lt; 60) return 0.0250719;\nelse if(pt &lt; 70) return 0.023081;\nelse if(pt &lt; 80) return 0.0183273;\nelse if(pt &lt; 100) return 0.0256502;\nelse if(pt &lt; 120) return 0.0189555;\nelse if(pt &lt; 160) return 0.0236561;\nelse if(pt &lt; 210) return 0.0307624;\nelse if(pt &lt; 260) return 0.0387889;\nelse if(pt &lt; 320) return 0.0443912;\nelse if(pt &lt; 400) return 0.0693573;\nelse if(pt &lt; 500) return 0.0650147;\nelse return 0.066886;\n}\n</code></pre>"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#storing-final-weights","title":"Storing Final Weights","text":"<p>Also from the \"Calculating Weights\" section, there are 3 final variables that are used to store the final event weights that were calculated: <code>btagWeight</code>, <code>btagWeightUp</code> , and <code>btagWeightDn</code>. When the file has completed running, you can run root with your output file and look up these 3 names to access the data calculated from your run. Here is an example of these variables accessed in root (Normal - Black, Up - Red, Down - Blue):</p> <p></p> <p>Warning</p> <p>This page is under construction</p>"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/","title":"Jet Uncertainty","text":"<p>Unsurprisingly, the CMS detector does not measure jet energies perfectly, nor do simulation and data agree perfectly! The measured energy of jet must be corrected so that it can be related to the true energy of its parent particle. These corrections account for several effects and are factorized so that each effect can be studied independently.</p>"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#jet-energy-corrections-jec","title":"Jet Energy Corrections (JEC)","text":"<p>What is JEC?</p> <p>JEC is the first set of corrections applied on jets that adjust the mean of the response distribution in a series of correction levels.</p> <p>Correction Levels</p> <p></p> <p>Particles from additional interactions in nearby bunch crossings of the LHC contribute energy in the calorimeters that must somehow be distinguished from the energy deposits of the main interaction. Extra energy in a jet's cone can make its measured momentum larger than the momentum of the parent particle. The first layer (\"L1\") of jet energy corrections accounts for pileup by subtracting the average transverse momentum contribution of the pileup interactions to the jet's cone area. This average pileup contribution varies by pseudorapidity and, of course, by the number of interactions in the event.</p> <p>The second and third layers of corrections (\"L2L3\") correct the measured momentum to the true momentum as functions of momentum and pseudorapidity, bringing the reconstructed jet in line with the generated jet. These corrections are derived using momentum balancing and missing energy techniques in dijet and Z boson events. One well-measured object (ex: a jet near the center of the detector, a Z boson reconstructed from leptons) is balanced against a jet for which corrections are derived.</p> <p>All of these corrections are applied to both data and simulation. Data events are then given \"residual\" corrections to bring data into line with the corrected simulation. A final set of flavor-based corrections are used in certain analyses that are especially sensitive to flavor effects. All of the corrections are described in this paper. The figure below shows the result of the L1+L2+L3 corrections on the jet response.</p> <p></p>"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#implementing-jec-in-cms-software","title":"Implementing JEC in CMS Software","text":"<p>JEC From Text Files</p> <p>There are several methods available for applying jet energy corrections to reconstructed jets. We have demonstrated a method to read in the corrections from text files and extract the corrections manually for each jet. In order to produce these text files, we have to run jec_cfg.py.</p> <pre><code>isData = False\n#if len(sys.argv) &gt; 1: isData = bool(eval(sys.argv[1]))\n#print 'Writing JEC text files. isData = ',isData\n\n# CMS process initialization\nprocess = cms.Process('jecprocess')\nprocess.load('Configuration.StandardSequences.Services_cff')\nprocess.load('Configuration.StandardSequences.FrontierConditions_GlobalTag_cff')\n\n# connect to global tag\nif isData:\n#    process.GlobalTag.connect = cms.string('sqlite_file:/cvmfs/cms-opendata-conddb.cern.ch/FT53_V21A_AN6_FULL.db')\n    process.GlobalTag.globaltag = 'FT53_V21A_AN6::All'\nelse:\n#    process.GlobalTag.connect = cms.string('sqlite_file:/cvmfs/cms-opendata-conddb.cern.ch/START53_V27.db')\n    process.GlobalTag.globaltag = 'START53_V27::All'\n\n\n# setup JetCorrectorDBReader \nprocess.maxEvents = cms.untracked.PSet(input=cms.untracked.int32(1))\nprocess.source = cms.Source('EmptySource')\nprocess.ak5 = cms.EDAnalyzer('JetCorrectorDBReader', \n                             payloadName=cms.untracked.string('AK5PF'),\n                             printScreen=cms.untracked.bool(False),\n                             createTextFile=cms.untracked.bool(True))\n\nif isData:\n    process.ak5.globalTag = cms.untracked.string('FT53_V21A_AN6')\nelse:\n    process.ak5.globalTag = cms.untracked.string('START53_V27')\n\nprocess.p = cms.Path(process.ak5)\n</code></pre> <p>Note that this analyzer will need to be run with both <code>isData = True</code> and <code>isData = False</code> to produce text files for both.</p> <pre><code>$ cd JEC\n$ cmsRun jec_cfg.py\n$ #edit the file and flip isData\n$ cmsRun jec_cfg.py\n</code></pre>"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#applying-jec-correction","title":"Applying JEC Correction","text":"<p>JEC begins in poet_cfg.py, where we apply jet energy corrections and Type-1 MET corrections on PAT jets, which are a popular object format in CMS that stands for \"Physics Analysis Toolkit\". To do this we will load the global tag and databases directly in the configuration file and use the \u2018addJetCollection\u2019 process to create a collection of pat::jets.</p> <p>Note: The JEC Uncertainty text file is needed for the manually created correction uncertainties created inside of the analyzer. Uncertainty will be covered later.</p> <pre><code>if doPat:\n ...\n # Choose which jet correction levels to apply\n jetcorrlabels = ['L1FastJet','L2Relative','L3Absolute']\n if isData:\n  # For data we need to remove generator-level matching processes\n  runOnData(process, ['Jets','METs'], \"\", None, [])\n  jetcorrlabels.append('L2L3Residual')\n\n # Set up the new jet collection\n process.ak5PFJets.doAreaFastjet = True\n addPfMET(process, 'PF')\n\n addJetCollection(process,cms.InputTag('ak5PFJets'),\n    'AK5', 'PFCorr',\n   doJTA        = True,\n   doBTagging   = True,\n   jetCorrLabel = ('AK5PF', cms.vstring(jetcorrlabels)),\n   doType1MET   = True,\n   doL1Cleaning = True,\n   doL1Counters = False,\n   doJetID      = True,\n   jetIdLabel   = \"ak5\",\n   )\n process.myjets= cms.EDAnalyzer('PatJetAnalyzer',\n       InputCollection = cms.InputTag(\"selectedPatJetsAK5PFCorr\"),\n                                   isData = cms.bool(isData),\n                                   jecUncName = cms.FileInPath('PhysObjectExtractorTool/PhysObjectExtractor/JEC/'+JecString+'Uncertainty_AK5PF.txt'), \n                                   jerResName = cms.FileInPath('PhysObjectExtractorTool/PhysObjectExtractor/JEC/JetResolutionInputAK5PF.txt')         \n                               )\n ...\n</code></pre> <p>Now we can go into PatJetAnalyzer.cc, where in the jet loop of <code>analyzeJets</code>, the correction has already automatically been corrected for each jet. We then save a uncorrected version of the jet as <code>uncorrJet</code>.</p> <pre><code>for (std::vector&lt;pat::Jet&gt;::const_iterator itjet=myjets-&gt;begin(); itjet!=myjets-&gt;end(); ++itjet){\npat::Jet uncorrJet = itjet-&gt;correctedJet(0);     ...\n</code></pre>"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#jet-energy-resolution-jer","title":"Jet Energy Resolution (JER)","text":"<p>What is JER?</p> <p>Jet Energy Resolution (JER) corrections are applied after JEC on strictly MC simulations. Unlike JEC, which adjusts for the mean of the response distribution, JER adjusts the width of the distribution. This is because MC simulations tend to be more sharply peaked and less broad than the same distribution in data, therefore we have to increase the resolution based on the effects of pileup, jet size and jet flavor.</p> <p>Accesing JER in CMS Software</p> <p>Unlike JEC, the majority of JER is done inside of <code>PatJetAnalyzer.cc</code>, but we do have to import the file path to the text file containing a jet resolution factor table from the JEC directory in <code>poet_cfg.py</code>.</p> <pre><code>process.myjets= cms.EDAnalyzer('PatJetAnalyzer',\n           ... \n           jerResName = cms.FileInPath('PhysObjectExtractorTool/PhysObjectExtractor/JEC/JetResolutionInputAK5PF.txt')         \n           )\n</code></pre> <p>Back inside the jet loop, we define <code>ptscale</code>, the eventual scale factor multiplied onto the jet momentum.</p> <p>Note: As mentioned previously, if we are running <code>PatJetAnalyzer.cc</code> on data, we do not want to affect to the resolution, so we initialize it as <code>1</code>.</p> <p>Next we calculate <code>ptscale</code> using one of two methods:</p> <ol> <li> <p>A stochastic smearing method, which is used on generator-level jets (<code>genJet</code>), described by equation 4.11 in this dissertation.</p> </li> <li> <p>A hybrid smearing method, which is used otherwise, described in section 8 of the 2017 CMS jet algorithm paper, which also includes more information about JEC in general.</p> </li> </ol> <p>Note: Also mentioned previously was the fact that JER is applied after JEC, meaning the pT that is used various times in the evaluations (e.g <code>PTNPU.push_back( itjet-&gt;pt() );</code>) is the JEC corrected momentum, rather than the uncorrected one.</p> <pre><code>void\nJetAnalyzer::analyze(const edm::Event&amp; iEvent, const edm::EventSetup&amp; iSetup)\n{\n...\nfor (std::vector&lt;pat::Jet&gt;::const_iterator itjet=myjets-&gt;begin(); itjet!=myjets-&gt;end(); ++itjet){     ...\nptscale = 1;\nres = 1;\nif(!isData) {\nstd::vector&lt;float&gt; factors = factorLookup(fabs(itjet-&gt;eta())); // returns in order {factor, factor_down, factor_up}\nstd::vector&lt;float&gt; feta;\nstd::vector&lt;float&gt; PTNPU;\nfeta.push_back( fabs(itjet-&gt;eta()) );\nPTNPU.push_back( itjet-&gt;pt() );\nPTNPU.push_back( vertices-&gt;size() );\n\nres = jer_-&gt;correction(feta, PTNPU);\nfloat pt = itjet-&gt;pt();\n\nconst reco::GenJet *genJet = itjet-&gt;genJet();\nbool smeared = false;\nif(genJet){\ndouble deltaPt = fabs(genJet-&gt;pt() - pt);\ndouble deltaR = reco::deltaR(genJet-&gt;p4(),itjet-&gt;p4());\nif ((deltaR &lt; 0.2) &amp;&amp; deltaPt &lt;= 3*pt*res){\ndouble gen_pt = genJet-&gt;pt();\ndouble reco_pt = pt;\ndouble deltapt = (reco_pt - gen_pt) * factors[0];\ndouble deltapt_down = (reco_pt - gen_pt) * factors[1];\ndouble deltapt_up = (reco_pt - gen_pt) * factors[2];\nptscale = max(0.0, (reco_pt + deltapt) / reco_pt);\n...\nsmeared = true;\n}\n} if (!smeared &amp;&amp; factors[0]&gt;0) {\nTRandom3 JERrand;\n\nJERrand.SetSeed(abs(static_cast&lt;int&gt;(itjet-&gt;phi()*1e4)));\nptscale = max(0.0, JERrand.Gaus(pt,sqrt(factors[0]*(factors[0]+2))*res*pt)/pt);\n...\n}\n</code></pre>"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#jet-correction-uncertainty","title":"Jet Correction Uncertainty","text":"<p>An important factor we have to keep in mind when applying both JEC and JER are the statistical uncertainities. These uncertainties have several sources, shown in the figure below. The L1 (pileup) uncertainty dominates at low momentum, while the L3 (absolute scale) uncertainty takes over for higher momentum jets. All corrections are quite precise for jets located near the center of the CMS barrel region, and the precision drops as pseudorapidity increases and different subdetectors lose coverage.</p> <p></p> <p>These uncertainties are accounted for by including an \"up\" and \"down\" version of our correction factor.</p> <p>JEC Uncertainty</p> <p>While the JEC corrected momentum can be accessed automatically through the jet object (e.g. <code>itjet-&gt;pt()</code>), the \"up\" and \"down\" versions must be calculated manually.</p> <p>Here in the jet loop, the <code>corrUp</code> and <code>corrDown</code> variables are created in part using <code>jetUnc_-&gt;getUncertainty()</code> (This object is created from the a text file which was briefly mentioned during the JEC initialization in <code>poet_cfg.py</code> of the Implementing JEC in CMS Software section). In order to access the <code>getUncertainty</code> function, we use a JEC uncertainty object, in this case called <code>jecUnc_</code>, where we input information about the jet, like its psuedorapidity and momentum.</p> <pre><code>for (std::vector&lt;pat::Jet&gt;::const_iterator itjet=myjets-&gt;begin(); itjet!=myjets-&gt;end(); ++itjet){\n...\ndouble corrUp = 1.0;\ndouble corrDown = 1.0;\njecUnc_-&gt;setJetEta( itjet-&gt;eta() );\njecUnc_-&gt;setJetPt( itjet-&gt;pt() );\ncorrUp = (1 + fabs(jecUnc_-&gt;getUncertainty(1)));\njecUnc_-&gt;setJetEta( itjet-&gt;eta() );\njecUnc_-&gt;setJetPt( itjet-&gt;pt() );\ncorrDown = (1 - fabs(jecUnc_-&gt;getUncertainty(-1)));\n...\n</code></pre> <p>JER Uncertainty</p> <p>Just how <code>ptscale</code> was manually calculated on genJets using this line:</p> <pre><code>ptscale = max(0.0, (reco_pt + deltapt) / reco_pt);\n</code></pre> <p>We calculate the JER uncertainty like so:</p> <pre><code>ptscale_up = max(0.0, (reco_pt + deltapt_up) / reco_pt);\nptscale_down = max(0.0, (reco_pt + deltapt_down) / reco_pt);\n</code></pre> <p>Otherwise for non-genJets,</p> <pre><code>JERrand.SetSeed(abs(static_cast&lt;int&gt;(itjet-&gt;phi()*1e4)));\nptscale_down = max(0.0, JERrand.Gaus(pt,sqrt(factors[1]*(factors[1]+2))*res*pt)/pt);\n\nJERrand.SetSeed(abs(static_cast&lt;int&gt;(itjet-&gt;phi()*1e4)));\nptscale_up = max(0.0, JERrand.Gaus(pt,sqrt(factors[2]*(factors[2]+2))*res*pt)/pt);\n</code></pre>"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#storing-the-corrections","title":"Storing the corrections","text":"<p>The final step in actualizing the jet corrections occurs after the JEC/JER calculations, where we fill the five momentum vectors for each jet.</p> <ul> <li><code>corr_jet_pt</code> is the JEC + JER corrected pT</li> <li><code>corr_jet_ptUp</code> and <code>corr_jet_ptDown</code> are the (\"up\" and \"down\" versions of the JEC) + JER corrected pT</li> <li><code>corr_jet_ptSmearUp</code> and <code>corr_jet_ptSmearDown</code> are the JEC + (smeared \"up\" and \"down\" versions of the JER) corrected pT</li> </ul> <pre><code>corr_jet_pt.push_back(ptscale*itjet-&gt;pt());\ncorr_jet_ptUp.push_back(ptscale*corrUp*itjet-&gt;pt());\ncorr_jet_ptDown.push_back(ptscale*corrDown*itjet-&gt;pt());\ncorr_jet_ptSmearUp.push_back(ptscale_up*itjet-&gt;pt());\ncorr_jet_ptSmearDown.push_back(ptscale_down*itjet-&gt;pt()); </code></pre>"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#putting-it-all-together","title":"Putting it all together","text":"<p>Inside of the dropdown is the full jet loop, comprised of the storing of the uncorrected jet object, creation of JEC uncertainty, JER corrections + uncertainty, and storing of the corrected momentum.</p> Full Jet Loop <pre><code>for (std::vector&lt;pat::Jet&gt;::const_iterator itjet=myjets-&gt;begin(); itjet!=myjets-&gt;end(); ++itjet){\npat::Jet uncorrJet = itjet-&gt;correctedJet(0);     double corrUp = 1.0;\ndouble corrDown = 1.0;\njecUnc_-&gt;setJetEta( itjet-&gt;eta() );\njecUnc_-&gt;setJetPt( itjet-&gt;pt() );\ncorrUp = (1 + fabs(jecUnc_-&gt;getUncertainty(1)));\njecUnc_-&gt;setJetEta( itjet-&gt;eta() );\njecUnc_-&gt;setJetPt( itjet-&gt;pt() );\ncorrDown = (1 - fabs(jecUnc_-&gt;getUncertainty(-1)));\n\nptscale = 1;\nptscale_down = 1;\nptscale_up = 1;\nres = 1;\nif(!isData) {\nstd::vector&lt;float&gt; factors = factorLookup(fabs(itjet-&gt;eta())); // returns in order {factor, factor_down, factor_up}\nstd::vector&lt;float&gt; feta;\nstd::vector&lt;float&gt; PTNPU;\nfeta.push_back( fabs(itjet-&gt;eta()) );\nPTNPU.push_back( itjet-&gt;pt() );\nPTNPU.push_back( vertices-&gt;size() );\n\nres = jer_-&gt;correction(feta, PTNPU);\nfloat pt = itjet-&gt;pt();\n\nconst reco::GenJet *genJet = itjet-&gt;genJet();\nbool smeared = false;\nif(genJet){\ndouble deltaPt = fabs(genJet-&gt;pt() - pt);\ndouble deltaR = reco::deltaR(genJet-&gt;p4(),itjet-&gt;p4());\nif ((deltaR &lt; 0.2) &amp;&amp; deltaPt &lt;= 3*pt*res){\ndouble gen_pt = genJet-&gt;pt();\ndouble reco_pt = pt;\ndouble deltapt = (reco_pt - gen_pt) * factors[0];\ndouble deltapt_down = (reco_pt - gen_pt) * factors[1];\ndouble deltapt_up = (reco_pt - gen_pt) * factors[2];\nptscale = max(0.0, (reco_pt + deltapt) / reco_pt);\nptscale_up = max(0.0, (reco_pt + deltapt_up) / reco_pt);\nptscale_down = max(0.0, (reco_pt + deltapt_down) / reco_pt);\nsmeared = true;\n}\n} if (!smeared &amp;&amp; factors[0]&gt;0) {\nTRandom3 JERrand;\n\nJERrand.SetSeed(abs(static_cast&lt;int&gt;(itjet-&gt;phi()*1e4)));\nptscale = max(0.0, JERrand.Gaus(pt,sqrt(factors[0]*(factors[0]+2))*res*pt)/pt);\n\nJERrand.SetSeed(abs(static_cast&lt;int&gt;(itjet-&gt;phi()*1e4)));\nptscale_down = max(0.0, JERrand.Gaus(pt,sqrt(factors[1]*(factors[1]+2))*res*pt)/pt);\n\nJERrand.SetSeed(abs(static_cast&lt;int&gt;(itjet-&gt;phi()*1e4)));\nptscale_up = max(0.0, JERrand.Gaus(pt,sqrt(factors[2]*(factors[2]+2))*res*pt)/pt);\n}\n}\n\nif( ptscale*itjet-&gt;pt() &lt;= min_pt) continue;\n\njet_pt.push_back(uncorrJet.pt());\njet_eta.push_back(itjet-&gt;eta());\njet_phi.push_back(itjet-&gt;phi());\njet_ch.push_back(itjet-&gt;charge());\njet_mass.push_back(uncorrJet.mass());\njet_btag.push_back(itjet-&gt;bDiscriminator(\"combinedSecondaryVertexBJetTags\"));\ncorr_jet_pt.push_back(ptscale*itjet-&gt;pt());\ncorr_jet_ptUp.push_back(ptscale*corrUp*itjet-&gt;pt());\ncorr_jet_ptDown.push_back(ptscale*corrDown*itjet-&gt;pt());\ncorr_jet_ptSmearUp.push_back(ptscale_up*itjet-&gt;pt());\ncorr_jet_ptSmearDown.push_back(ptscale_down*itjet-&gt;pt()); corr_jet_mass.push_back(itjet-&gt;mass());\ncorr_jet_e.push_back(itjet-&gt;energy());\ncorr_jet_px.push_back(itjet-&gt;px());\ncorr_jet_py.push_back(itjet-&gt;py());\ncorr_jet_pz.push_back(itjet-&gt;pz());\n...\n}\n</code></pre>"},{"location":"analysis/systematics/objectsuncertain/leptonuncertain/","title":"Object Uncertainty","text":"<p>Warning</p> <p>This page is under construction</p>"},{"location":"cmsOpenData/cernportal/","title":"The CERN Open Data Portal","text":"<p>All CMS open data is available through the CERN Open Data portal. The portal hosts data from many experiments and offers search options, such as experiment, type or energy of collisions, type of data (from collisions or simulated), and many more. A brief description of the portal and those of each experiment are available from the \"About\" dropdown menu top right.</p> <p>The CERN Open Data portal contains the data records, environment, software and supplementary material to enable research-level use of open data. It also includes some basic documentation and topical guides. For CMS, this Open data guide complements the information available on the portal.</p> <p>The data records are accessed either using XRootD, which allows the data to be streamed, or through direct http download. A command-line tool cernopendata-client is also available for data download and inspection.</p>"},{"location":"cmsOpenData/cmsopendata/","title":"CMS Open Data","text":"<p>The CMS experiment at CERN has released research-quality data from particle collisions at the LHC since 2014. Almost all data from the first LHC run in 2010\u20132012 (\"Run1\") with the corresponding simulated samples are in the public domain, and several scientific studies have been performed using these data. First data from the second LHC run in 2015-2018 (\"Run2\") have been released in 2021.</p> <p>Open data are released after an embargo period of six years, which allows the collaboration to understand the detector performance and to exploit the scientific potential of these data. This is also necessary for the time needed to reprocess the data with the best available knowledge before the release.</p> <p>The first release of each year\u2019s data consists of 50% of the integrated luminosity recorded by the experiment, and the remaining data will be released within ten years, unless active analysis is still ongoing. However, the amount of open data will be limited to 20% of data with the similar centre-of-mass energy and collision type while such data are still planned to be taken. This approach allows for a fairly prompt release of the data after a major reprocessing once the reconstruction has been optimised, but still guarantees that the collaboration will have the opportunity to complete the planned studies with the complete dataset first.</p> <p>The open data releases are regulated in the CMS data preservation, re-use and open access policy.</p> <p>CERN open data portal includes a brief description about CMS open data and different tools available to analyze them. The main points are:</p> <ul> <li> <p>the released data are as those used by the CMS collaboration, with all their complexicity</p> </li> <li> <p>some CMS-specific software is needed and available to get started with these data</p> </li> <li> <p>a computing environment compatible with the data and software needed for their analysis is provided.</p> </li> </ul> <p>The experimental particle physics data are complex and studying them requires a solid understanding of the underlying physics, knowledge of different detector systems involved in data taking, and some mastering of the data handling. Some of these challenges have been addressed in this note, and this guide is part of the measures taken to improve the usability of CMS open data.</p>"},{"location":"cmsOpenData/findingData/","title":"Finding Data","text":"<p>All CMS open data are available through the CERN Open Data portal. See the CMS Open data Workshop tutorial lesson for advice on how to explore available CMS datasets on the CERN Open Data portal.</p>"},{"location":"cmsOpenData/workshops/","title":"CMS Open Data workshops","text":"<p>The CMS Open Data group organizes regular workshops to help users get started with the CMS Open Data.</p> <p>The tutorials of these workshops are available in these links and free to use:</p> <ul> <li>2020 workshop</li> <li>2021 workshop</li> <li>2022 workshop</li> <li>2023 workshop</li> </ul> <p>Keep in mind that there may have been updates to the CMS Open Data environments and not all older tutorials work out of the box.</p>"},{"location":"cmssw/cmsswanalyzers/","title":"Analyzers","text":"<p>First, a few general words about analysis in the CMSSW framework. Physics analysis proceeds via a series of subsequent steps. Building blocks are identified and more complex objects are built on top of them. How to write a Framework Module and run the job with the <code>cmsRun</code> can be found here.</p> <p>When setting up code for the new EDM (such as creating a new EDProducer) there is a fair amount of 'boiler plate' code that you must write. To make writing such code easier CMS provides a series of scripts that will generate the necessary directory structure and files needed so that all you need to do is write your actual algorithms.</p> <p>CMSSW distiguishes the following module types:</p> <ul> <li>EDAnalyzer: takes input from the event and processes the input without writing information back to the event</li> <li>EDProducer: takes input from the event and produces new output which is saved in the event</li> <li>EDFilter: decides if processing the event can be stopped and continued</li> <li>EventSetup: external service not bound to the event structure which provides information useable by all modules (e.g. Geometry, Magnetic Field, etc.)</li> </ul> <p>In order to generate above modules:</p> <ul> <li>mkedanlzr : makes a skeleton of a package containing an EDAnalyzer</li> <li>mkedprod : makes a skeleton of a package containing an EDProducer</li> <li>mkedfltr : makes a skeleton of a package containing an EDFilter</li> <li>mkrecord : makes a complete implementation of a Record used by the EventSetup</li> </ul> <p>More generators are available and you can find them here</p> <p>The code examples provided with the CMS open data are mostly EDAnalyzers. A hands-on tutorial to learn more on CMSSW and EDAnalyzers is available in the CMS open data workshop material. For examples, this guide mainly refers to:</p> Run 1 DataRun 2 Data <ul> <li>Physics Objects Extractor Tool (POET): shows how to extract physics (objects) information and gives examples of methods or tools needed for processing them. For the sake of clarity, EDAnalyzer modules are provided separately for each object.</li> <li>AOD2NanoAODOutreachTool: reads events from CMS AOD files and convert them to a reduced data format. This example provides a single EdAnalyzer module handling all types of physics objects.</li> </ul> <ul> <li>Physics Objects Extractor Tool (POET): shows how to extract physics (objects) information and gives examples of methods or tools needed for processing them. For the sake of clarity, EDAnalyzer modules are provided separately for each object.</li> </ul>"},{"location":"cmssw/cmsswconditions/","title":"Conditions","text":"<p>This page explains the use of global tags and the condition database with the CMS Open Data. All information was taken from here.</p> <p>A Global Tag is a coherent collection of records of additional data needed by the reconstruction and analysis software. The Global Tag is defined for each data-taking period, separately for collision and simulated data.</p> <p>These records are stored in the condition database. Condition data include non-event-related information (Alignment, Calibration, Temperature, etc.) and parameters for the simulation/reconstruction/analysis software. For CMS Open Data, the condition data are provided as sqlite files in the <code>/cvmfs/cms-opendata-conddb.cern.ch/</code> directory, which is accessible through the CMS Open Data VM.</p> <p>Most physics objects such as <code>electrons</code>, <code>muons</code>, <code>photons</code> in the CMS Open Data are already calibrated and ready-to-use, and no additional corrections are needed other than selection and identification criteria, which will be applied in the analysis code. Therefore, simple analyses do not need to access the condition database. For example you can check the Higgs analysis example.</p> <p>However, access to the condition database is necessary, for example, for jet energy corrections and trigger configuration information. Examples of such analyses are for the PAT object production or the top quark pair production.</p> <p>Note that when you need to access the condition database, the first time you run the job on the CMS Open Data VM, it will download the condition data from the <code>/cvmfs</code> area. It will take time (an example run of a 10 Mbps line took 45 mins), but it will only happen once as the files will be cached on your VM. The job will not produce any output during this time, but you can check the ongoing processes with the command 'top' and you can monitor the progress of reading the condition data to the local cache with the command 'df'.</p> <p>Collision data and Monte Carlo data sets can be found at http://opendata.cern.ch/docs/cms-guide-for-condition-database for years 2010, 2011 and 2012.</p> <p>Warning</p> <p>This page is under construction</p>"},{"location":"cmssw/cmsswconfigure/","title":"Configuration","text":"<p>A configuration document, written using the Python language, is used to configure the cmsRun executable. A Python configuration program specifies which modules, inputs, outputs and services are to be loaded during execution, how to configure these modules and services, and in what order to execute them. More information can be found at the CMS software guide.</p> <p>The hands-on tutorial on CMSSW available in the CMS open data workshop material includes a detailed lesson on CMSSW configuration files.</p> <p>The configuration files for the examples that this guide mainly refers to can be found in:</p> Run 1 DataRun 2 Data <ul> <li>Physics Objects Extractor Tool (POET) configuration file: a single configuration file where options to process data or MC (or other processing algorithm choices) is done through input arguments.</li> <li>AOD2NanoAODOutreachTool configuration files for data and MC.</li> </ul> <ul> <li>Physics Objects Extractor Tool (POET) configuration file: a single configuration file where options to process data or MC (or other processing algorithm choices) is done through input arguments.</li> </ul>"},{"location":"cmssw/cmsswdatamodel/","title":"Data Model","text":"<p>The CMS Event Data Model (EDM) is centered around the concept of an Event. Physically, an event is the result of a single readout of the detector electronics and the signals that will (in general) have been generated by particles, tracks, energy deposits, present in a number of bunch crossings.</p> <p>In software terms, an Event starts as a collection of the RAW data from a detector or MC event, stored as a single entity in memory, a C++ type-safe container called <code>edm::Event</code>. An Event is a C++ object container for all RAW and reconstructed data related to a particular collision. During processing, data are passed from one module to the next via the Event, and are accessed only through the Event. All objects in the Event may be individually or collectively stored in ROOT files, and are thus directly browsable in ROOT.</p> <p>More and detailed information can be found here.</p>"},{"location":"cmssw/cmsswdatamodel/#the-cms-data-hierarchy","title":"The CMS Data Hierarchy","text":"<p>CMS Data is arranged into a hierarchy of data tiers. Each physics event is written into each data tier, where the tiers each contain different levels of information about the event. The different tiers each have different uses. The three main data tiers written in CMS are:</p> <ol> <li>RAW: full event information from the Tier-0 (i.e. from CERN), containing 'raw' detector information (detector element hits, etc)<ul> <li>RAW is not used directly for analysis</li> </ul> </li> <li>RECO (\"RECOnstructed data\"): the output from first-pass processing by the Tier-0. This layer contains reconstructed physics objects, but it's still very detailed.<ul> <li>RECO can be used for analysis, but is too big for frequent or heavy use when CMS has collected a substantial data sample.</li> <li>RECO Data Format Table</li> </ul> </li> <li>AOD (\"Analysis Object Data\"): this is a \"distilled\" version of the RECO event information, and was used for most analyses on Run 1 data.<ul> <li>AOD provides a trade-off between event size and complexity of the available information to optimize flexibility and speed for analyses.</li> <li>AOD Data Format Table</li> </ul> </li> <li>MINIAOD: slimmer version of AOD, used for analyses on Run 2 data.<ul> <li>MINIAOD is approximately one tenth of the size of AOD.</li> <li>The reduction is obtained defining light-weight physics-object candidate representations, increasing transverse momentum thresholds for storing physics-object candidates, and reduced numerical precision when it is not required at the analysis level.</li> <li>MINIAOD physics objects table</li> </ul> </li> </ol> <p>The data tiers are described in more detail in a dedicated WorkBook chapter on Data Formats and Tiers.</p>"},{"location":"cmssw/cmsswoverview/","title":"Overview","text":"<p>The overall collection of software, referred to as CMS Software (CMSSW), is built around a Framework, an Event Data Model (EDM), and Services needed by the simulation, calibration and alignment, and reconstruction modules that process event data so that physicists can perform analysis. The primary goal of the Framework and EDM is to facilitate the development and deployment of reconstruction and analysis software.</p> <p>The CMSSW event processing model consists of one executable, called <code>cmsRun</code>, and many plug-in modules which are managed by the Framework. All the code needed in the event processing (calibration, reconstruction algorithms, etc.) is contained in the modules. The same executable is used for both detector and Monte Carlo data. More and detailed information can be found here.</p>"},{"location":"tools/cppandpython/","title":"C++ and python","text":"<p>CMS primarily uses C++ and python to analyze data. Here are some computing tutorials.</p>"},{"location":"tools/cppandpython/#c","title":"C++","text":"<ul> <li> <p>Basic Modern C++</p> </li> <li> <p>cplusplus.com</p> </li> </ul>"},{"location":"tools/cppandpython/#python","title":"Python","text":"<ul> <li> <p>Programming with Python</p> </li> <li> <p>Plotting and Programming in Python</p> </li> </ul>"},{"location":"tools/docker/","title":"Docker","text":"<p>Warning</p> <p>This page is under construction</p> <p>Docker is a commercial implementation of a container, a way to package up a snapshot of everything needed to run some particular version of software (OS, libraries, compilers, etc.). It is a very effective way of interfacing with the CMS open data as it gives you the proper environment you need to analyze these data.</p> <p>To learn more about Docker in general, from a HEP perspective, you may want to check out this Introduction to Docker, from Matthew Feickert.</p> <p>To account for the different running conditions in Run 1 vs Run 2, click the appropriate tab below for Run 1 vs Run 2 data.</p> Run 1 DataRun 2 Data <ul> <li>You can find the list of Docker container images available for CMS open data in the guide page for CMS open data containers</li> <li>You can also jump right in with the most recent tutorial on the CMS open data containers. If the tutorial is for more recent data, make sure to change the container image to correspond to the Run 1 data you intend to analyse.</li> </ul> <ul> <li>You can find the list of Docker container images available for CMS open data in the guide page for CMS open data containers</li> <li>You can also jump right in with the most recent tutorial on the CMS open data containers.</li> </ul>"},{"location":"tools/git/","title":"Git","text":"<p>Git is an open-source distributed version control system.</p> <p>Here are some helpful links to learn how to use git.</p> <ul> <li> <p>Version Control with Git</p> </li> <li> <p>Pro Git Book</p> </li> <li> <p>git reference</p> </li> </ul>"},{"location":"tools/root/","title":"ROOT","text":"<p>Warning</p> <p>This page is under construction</p> <p>From ROOT's webpage</p> <p>A modular scientific software toolkit. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualisation and storage. It is mainly written in C++ but integrated with other languages such as Python and R.</p> <p>It is the primary toolkit for many experimental analysis and while you are free to analyze these datasets however you like, some familiarity with ROOT will serve you well when accessing the data.</p> <p>To get started analyzing data with ROOT and C++, start with C++ and ROOT. A ROOT installation comes with the virtual machine and/or docker installation pointed to in this guide.</p> <p>To learn more about ROOT, see the ROOT Manual.</p> <ul> <li> <p>Many ROOT examples can be found here. If you don't know where to start, we would recommend</p> <ul> <li>fillrandom.C - fill in a 1D histogram from a parametric function</li> <li>basic.C - read in data and create a root file</li> <li>h1ReadAndDraw.c - read in a 1D histogram from a ROOT file, and then draw the histogram</li> <li>draw2dopt.C - explore 2D drawing options</li> </ul> </li> <li> <p>Python has become the language of choice for many analysts and most of the examples   you'll see make use of the PyROOT module, callable from python. For more on pyROOT, see Python interface: PyROOT. You can go through a number of examples here.   If you don't know where to start, we would recommend</p> <ul> <li>hsimple.py - create and draw histograms</li> <li>fillrandom.py - fill in a 1D histogram from a parametric function, and save your output as a root file</li> <li>fit1.py - open the root file created from fillrandom.py, and do a fit</li> </ul> </li> </ul>"},{"location":"tools/unix/","title":"Unix","text":"<p>The unix shell provides a command-line interpreter. The shell provides an interface between the user and the kernel. The shell will pass your commands to the operating system. Mastering basic shell commands will help you speed up and automate a variety of tasks.</p> <p>You can get started with unix by working through the exercises in The Unix Shell.</p> <p>More advanced material is available in Extra Unix Shell Material.</p> <p>A linux commmand line tutorial focused on ubuntu is available at The Linux command line for beginners.</p>"},{"location":"tools/virtualmachines/","title":"Virtual machines","text":"<p>CMS open data and legacy data, even though still exciting and full of potential, are already a few years old.  Because of the rapidly evolving technolgies, the computing environments that were used to analyze these data are already ancient compared to the current, bleeding edge ones.</p> <p>Therefore, in order to mantain our ability to study these data, we have to rely on technologies that help us preserve adequate computer environments.  One way of doing this is by using virtual machines.</p> <p>In simple words, a virtual machine is an emulation of a computer system that can run within another system.  The latter is usually known as the host.</p>"},{"location":"tools/virtualmachines/#open-data-releases-cmssw-versions-and-operating-systems","title":"Open data releases, CMSSW versions and operating systems","text":"<p>CMS open data from our 2010 release can be studied using CMSSW_4_2_8, a version of the CMSSW software that used to run under Scientific Linux CERN 5 (slc5) operating system.  Likewise, open data from our 2011/2012 release used CMSSW_5_3_32 and those from 2015 release CMSSW_7_6_7 under Scientific Linux CERN 6 (slc6).</p> <p>The virtual machines that are used to analyze these data, therefore, need to consider all these compatibility subtleties.</p>"},{"location":"tools/virtualmachines/#virtual-machine-images","title":"Virtual machine images","text":"<p>In practical terms, a virtual machine image is a computer file that has all the right ingredients to create a virtual computer inside a given host.  This file, however, needs to be decoded by a virtual machine interpreter, usually known as hypervisor, which runs on the host machine.  One of the most famous hypervisors is Oracle's VirtualBox.</p>"},{"location":"tools/virtualmachines/#cms-virtual-images","title":"CMS virtual images","text":"<p>The most current images for CMS open data usage are described separately in the CERN Open Portal site for 2010 and 2011/2012/2015.  They come equiped with the ROOT framework, CMSSW and CVMFS access.</p> <p>Remember</p> <p>When installing a CMS virtual machine (following the instructions below), always use the latest image file available for 2010 or 2011/2012/2015 data.</p>"},{"location":"tools/virtualmachines/#installation","title":"Installation","text":"<p>Detailed instructions on how to install the CERN virtual machines can be found in the 2010, 2011/2012 and 2015 virtual machine installation guides from the CERN Open Portal.  Choose the one to follow depending on the data release you will be working on.</p> <p>In summary, the basic steps are as follows:</p> <ul> <li>Download and install the latest (or even better, the latest tested) version of VirtualBox.  Note that it is available for an ample range of platforms.</li> <li> <p>Download the latest CMS virtual image file.  Choose between 2010 or 2011/2012, depending on the data release of interest. Once downloaded, import the image file into VirtualBox.</p> <p>Remember</p> <p>Always use the latest image file available for 2010 or 2011/2012/2015. Older ones are usually deprecated.</p> </li> <li> <p>Test the environment; again, 2010, 2011/2012 and 2015, depending on the release.</p> </li> <li>Finally, check for any known issues or limitations (2010, 2011/2012, 2015.)</li> </ul>"}]}